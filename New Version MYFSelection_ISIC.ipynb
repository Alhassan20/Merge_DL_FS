{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5678,"status":"ok","timestamp":1683456241108,"user":{"displayName":"Alhassan Fusion","userId":"13770454643682145418"},"user_tz":-120},"id":"gUBcJ9XxrfDO","outputId":"a3a0a525-841a-4a9c-ae9d-f2e0dfd1ebf2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19511,"status":"ok","timestamp":1683456260615,"user":{"displayName":"Alhassan Fusion","userId":"13770454643682145418"},"user_tz":-120},"id":"2UOyQykuj0dY","outputId":"e09a0783-4431-4973-ec55-7979e1a80f2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (0.4.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pycm in /usr/local/lib/python3.10/dist-packages (3.9)\n","Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from pycm) (1.22.4)\n","Requirement already satisfied: art>=1.8 in /usr/local/lib/python3.10/dist-packages (from pycm) (5.9)\n"]}],"source":["!pip install colorama\n","!pip install pycm"]},{"cell_type":"markdown","metadata":{"id":"ubK_pvvYk8_k"},"source":["# Objective Function"]},{"cell_type":"markdown","metadata":{"id":"wQN7DRg8hViz"},"source":["### FitnessFunction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kkidzk7DhYHy"},"outputs":[],"source":["\n","import numpy as np\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score\n","from sklearn.svm import SVC\n","from sklearn import svm\n","\n","from sklearn.preprocessing import Binarizer\n","from sklearn import metrics\n","\n","from numpy.linalg import norm\n","from xgboost import XGBClassifier\n","\n","class fitnessFUNs:\n","\t\t#____________________________________________________________________________________       \n","\t\tdef FN1(I,trainInput,trainOutput,dim):            \n","\t\t\tdata_train_internal, data_test_internal, target_train_internal, target_test_internal = train_test_split(trainInput, trainOutput, test_size=0.34, random_state=1)\n","\t\t\treducedfeatures=[]\n","\t\t\tfor index in range(0,dim):\n","\t\t\t\tif (I[index]==1):\n","\t\t\t\t\t\treducedfeatures.append(index)\n","\n","\t\t\treduced_data_train_internal=data_train_internal[:,reducedfeatures]\n","\t\t\treduced_data_test_internal=data_test_internal[:,reducedfeatures]\n","\t\t\t\n","\t\t\t\n","\t\t\t# clf = svm.SVC()\n","\t\t\tclf = XGBClassifier()\n","\t\t\t# clf = svm.SVC(kernel='linear')\n","\t\t\t# clf = KNeighborsClassifier(n_neighbors=5)\n","\t\t\tclf.fit(reduced_data_train_internal, target_train_internal)\n","\t\t\ttarget_pred_internal = clf.predict(reduced_data_test_internal)\n","\t\t\tacc_train = float(accuracy_score(target_test_internal, target_pred_internal))\n","\t\t\t# print(\"baseline: \")\n","\t\t\t# print(acc_train)\n","\n","\t\t\tfitness=0.99*(1-acc_train)+0.01*sum(I)/(dim)\n","\n","\t\t\treturn fitness\n","\t\t#_____________________________________________________________________       \n","\t\tdef getFunctionDetails(a):\n","\t\t\t\t\n","\t\t\t\t# [name, lb, ub, dim]\n","\t\t\t\tparam = {  0:[\"FN1\",-1,1]\n","\n","\t\t\t\t\t\t\t\t}\n","\t\t\t\treturn param.get(a, \"nothing\")"]},{"cell_type":"markdown","metadata":{"id":"YhlON76Cjv5V"},"source":["# Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oimdxsg1jyrD"},"outputs":[],"source":["class solution:\n","    def __init__(self):\n","        self.best = 0\n","        self.bestIndividual=[]\n","        self.convergence1 = []\n","        self.convergence2 = []\n","        self.optimizer=\"\"\n","        self.objfname=\"\"\n","        self.startTime=0\n","        self.endTime=0\n","        self.executionTime=0\n","        self.lb=0\n","        self.ub=0\n","        self.dim=0\n","        self.popnum=0\n","        self.maxiers=0\n","        self.trainAcc=None\n","        self.testAcc=None\n","        \n"]},{"cell_type":"markdown","metadata":{"id":"S3smX8xqj-zD"},"source":["# transfer_functions_benchmark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-iXyDutPkPsE"},"outputs":[],"source":["#import skfuzzy\n","import numpy as np\n","from math import pi\n","from scipy.special import erf\n","import matplotlib.pylab as plt\n","\n","\n","class transfer_functions_benchmark:\n","    #________________________V-shaped transfer functions______________________\n","    def v1(x):\n","      v1=abs(erf((np.sqrt(pi)/2)*x))\n","      return v1\n","\n","      \n","    def v2(x):\n","      v2=abs(np.tanh(x))\n","      return v2\n","      \n","      \n","    def v3(x):\n","      v3= abs(x/np.sqrt(1+np.square(x)))\n","      return v3  \n","      \n","      \n","    def v4(x):\n","      v4= abs((2/pi)*np.arctan((pi/2)*x))\n","      return v4  \n","    ##______________________S-shaped transfer functions_______________________\n","\n","    def s1(x):\n","        \n","        s1=1 / (1 + np.exp(-2*x))\n","        \n","        return s1\n","\n","    def s2(x):\n","        s2 = 1 / (1 + np.exp(-x))  \n","        return s2\n","    # s2 is called logistic function and can be imported using scipy.special.expit(x) library\n","\n","    def s3(x):\n","        s3=1 / (1 + np.exp(-x/3))\n","        return s3\n","\n","\n","    def s4(x):\n","        s4=1 / (1 + np.exp(-x/2))\n","        return s4\n","\n","\n","    ##________________________the sigmoid functions_________________________\n","\n","    # A customized function for SIGMOID \n","\n","    def sigmf1(x,b,c):\n","        b=10\n","        c=.5\n","        y = 1 / (1. + np.exp(- c * (x - b)))\n","      \n","        return y\n","\n","\n","\n","    ## Built-in function for SIGMOID using skfuzzy.membership library\n","\n","    def sigmf2(x,b,c):\n","        b=10\n","        c=.5\n","        y=skfuzzy.membership.sigmf(x,b,c)\n","\n","        return y\n","\n","\n","\n","\n","\n","    x = np.arange(-8, 8, 0.1) \n","    # x is used inside this script and will be replaced by a binary individual (1-d binary vector)\n","    #when the transfer function is called or imported in the optimizers scripts\n","\n","\n","    #__________________Calling for transfer functions inside this script____________________\n","\n","    #T1=v1(x)\n","    #T2=v2(x)\n","    #T3=v3(x)\n","    #T4=v4(x)\n","    #\n","    #T5=s1(x)\n","    #T6=s2(x)\n","    #T7=s3(x)\n","    #T8=s4(x)\n","    #\n","    #T9=sigmf1(x,.5,10)\n","    #T10=sigmf2(x,.5,10)\n","    #\n","    ##_______________Plotting of transfer functions inside this script________________________\n","    #plt.figure(1)\n","    #plt.subplot(211)\n","    #\n","    #plt.plot(x, T1) \n","    #plt.plot(x, T2) \n","    #plt.plot(x, T3) \n","    #plt.plot(x, T4)\n","    #plt.xlabel('x')\n","    #plt.ylabel('f(x)')\n","    ##plt.show()\n","    ##\n","    ##plt.figure(2)\n","    ##\n","    #plt.subplot(212)\n","    #plt.plot(x, T5) \n","    #plt.plot(x, T6) \n","    #plt.plot(x, T7) \n","    #plt.plot(x, T8) \n","    #\n","    #\n","    #plt.xlabel('x')\n","    #plt.ylabel('f(x)')\n","    ##plt.show()\n","    ##\n","    ##plt.figure(3)\n","    ##plt.subplot(222)\n","    ##plt.plot(x, T9) \n","    #\n","    ##plt.xlabel('x')\n","    ##plt.ylabel('f(x)')\n","    ##plt.show()\n","    ##\n","    ###plt.subplot(212)\n","    ##plt.plot(x, T10) \n","    ##\n","    ##\n","    ##plt.xlabel('x')\n","    ##plt.ylabel('f(x)')\n","    #plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Z68Zq0EejUdj"},"source":["# Optimizers"]},{"cell_type":"markdown","metadata":{"id":"rDyfDoDLjWrq"},"source":["### 1- PSO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CjhEr3gsjYQr"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\n","import random\n","import numpy\n","import math\n","from colorama import Fore, Back, Style\n","import time\n","from sklearn.preprocessing import Binarizer\n","\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","from random import seed\n","\n","\n","class pso:\n","\n","     def PSO(objf,lb,ub,dim,PopSize,iters,trainInput,trainOutput,LOGGER_FILE):\n","\n","          # PSO parameters\n","          \n","      #    dim=30\n","      #    iters=200\n","          Vmax=6\n","      #    PopSize=50     #population size\n","          wMax=0.9\n","          wMin=0.2\n","          c1=2\n","          c2=2\n","      #    lb=-10\n","      #    ub=10\n","      #    \n","          s=solution()\n","          \n","          print(lb)\n","          ######################## Initializations\n","          \n","          vel=numpy.zeros((PopSize,dim))\n","          \n","          pBestScore=numpy.zeros(PopSize) \n","          pBestScore.fill(float(\"inf\"))\n","          \n","          pBest=numpy.zeros((PopSize,dim))\n","          gBest=numpy.zeros(dim)\n","          \n","          \n","          gBestScore=float(\"inf\")\n","          \n","      #    pos=numpy.random.uniform(0,1,(PopSize,dim)) *(ub-lb)+lb #generating continuous individuals\n","      #    for i in range(0,PopSize):\n","      #         for j in range (0,dim):\n","      #             if (pos[i,j]<0.5): \n","      #                pos[i,j]=1;\n","      #             else:\n","      #                pos[i,j]=0;\n","          pos=numpy.random.randint(2, size=(PopSize,dim)) #generating binary individuals\n","          convergence_curve1=numpy.zeros(iters)\n","          convergence_curve2=numpy.zeros(iters)\n","\n","          ############################################\n","          print(\"PSO is optimizing  \\\"\"+objf.__name__+\"\\\"\")    \n","          LOGGER_FILE.info(\"PSO is optimizing  \\\"\"+objf.__name__+\"\\\"\") \n","\n","          timerStart=time.time() \n","          s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","          \n","          for l in range(0,iters):\n","              for i in range(0,PopSize):\n","                  #pos[i,:]=checkBounds(pos[i,:],lb,ub) --not needed with BPSO\n","                # pos[i,:]=numpy.clip(pos[i,:], lb, ub)--not needed with BPSO\n","                # transfer functions will do the boundary checking(individual values either 0 or 1)\n","        \n","        # the following statement insures that at least one feature is selected\n","        #(i.e the randomly generated individual has at least one value 1)\n","                  while numpy.sum(pos[i,:])==0:   \n","                      pos[i,:]=numpy.random.randint(2, size=(1,dim))\n","                      \n","        \n","                  #Calculate objective function for each particle\n","                  #fitness=objf(pos[i,:])\n","                  fitness=objf(pos[i,:],trainInput,trainOutput,dim)\n","\n","                  if(pBestScore[i]>fitness):\n","                      pBestScore[i]=fitness\n","                      pBest[i,:]=pos[i,:]\n","\n","                  if(gBestScore>fitness):\n","                      gBestScore=fitness #best fitness on training returned from F10\n","                      gBest=pos[i,:]\n","\n","            # print(gBest) \n","            # print(\"fitness \"+str(gBestScore))\n","              featurecount=sum(gBest)\n","              \n","              convergence_curve2[l]=featurecount# store the best number of features\n","              convergence_curve1[l]=gBestScore#store the best fitness on testing returened from F11\n","\n","              # if (l%1==0):\n","                    \n","              #   print(['At iteration'+ str(l+1)+' the best fitness on trainig is:'+ str(gBestScore)+', the best number of features: '+str(featurecount)]);\n","              #   LOGGER_FILE.info(['At iteration'+ str(l+1)+' the best fitness on trainig is:'+ str(gBestScore)+', the best number of features: '+str(featurecount)]);\n","      #      \n","              \n","              #Update the W of PSO\n","              w=wMax-l*((wMax-wMin)/iters);\n","              \n","              for i in range(0,PopSize):\n","                  for j in range (0,dim):\n","                      r1=random.random()\n","                      r2=random.random()\n","                      vel[i,j]=w*vel[i,j]+c1*r1*(pBest[i,j]-pos[i,j])+c2*r2*(gBest[j]-pos[i,j])\n","                      \n","                      if(vel[i,j]>Vmax):\n","                          vel[i,j]=Vmax\n","                      \n","                      if(vel[i,j]<-Vmax):\n","                          vel[i,j]=-Vmax\n","                                  \n","                      pos[i,j]=(pos[i,j]+vel[i,j])#update statement\n","                    # print(\"vel \"+str(vel[i,j]))\n","                      #print(\"pos \"+str( pos[i,j]))\n","                    #  time.sleep(2)                \n","                      ss= transfer_functions_benchmark.s1(pos[i,j])#transfer function\n","                      #print(transfer_functions_benchmark.s1(pos[i,j]))\n","                      #time.sleep(2)   \n","                      \n","                      if (random.random()<ss): \n","                          pos[i,j]=1;\n","                      else:\n","                          pos[i,j]=0;\n","                  \n","                    # print((\"jjjjj\"+str(pos[i,j])))\n","\n","                  \n","          timerEnd=time.time()  \n","          s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","          s.executionTime=timerEnd-timerStart\n","          s.bestIndividual=gBest\n","          s.convergence1=convergence_curve1\n","          s.convergence2=convergence_curve2\n","\n","          s.optimizer=\"PSO\"\n","          s.objfname=objf.__name__\n","\n","          return s"]},{"cell_type":"markdown","metadata":{"id":"MRfolq4mlewT"},"source":["### 2- BAT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CJOdUAGplgNh"},"outputs":[],"source":["import math\n","import numpy\n","import random\n","import time\n","from sklearn.preprocessing import Binarizer\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","\n","class bat:\n","\n","          \n","    def BAT(objf,lb,ub,dim,N,Max_iteration,trainInput,trainOutput,LOGGER_FILE):\n","        \n","        n=N;      # Population size\n","        #lb=-50\n","        #ub=50\n","        N_gen=Max_iteration  # Number of generations\n","        \n","        A=0.5;      # Loudness  (constant or decreasing)\n","        r=0.5;      # Pulse rate (constant or decreasing)\n","        \n","        Qmin=0         # Frequency minimum\n","        Qmax=2         # Frequency maximum\n","        \n","        \n","        d=dim           # Number of dimensions \n","        \n","        # Initializing arrays\n","        Q=numpy.zeros(n)  # Frequency\n","        v=numpy.zeros((n,d))  # Velocities\n","        Convergence_curve1=[];\n","        Convergence_curve2=[];\n","\n","        # Initialize the population/solutions\n","        \n","      # Sol=numpy.random.rand(n,d)*(ub-lb)+lb      #generating continuous individuals\n","        \n","        Sol=numpy.random.randint(2, size=(n,d))     #generating binary individuals\n","        # the following statement insures that at least one feature is selected\n","            #(i.e the randomly generated individual has at least one value 1)\n","        \n","        for i in range(0,n):\n","          while numpy.sum(Sol[i,:])==0: \n","              Sol[i,:]=numpy.random.randint(2, size=(1,d))\n","              \n","        \n","          \n","        S=numpy.zeros((n,d))  # Population size, dim\n","        S=numpy.copy(Sol)\n","        Fitness=numpy.zeros(n)\n","        \n","        # initialize solution for the final results   \n","        s=solution()\n","        print(\"BAT is optimizing  \\\"\"+objf.__name__+\"\\\"\")    \n","        LOGGER_FILE.info(\"BAT is optimizing  \\\"\"+objf.__name__+\"\\\"\")   \n","        # Initialize timer for the experiment\n","        timerStart=time.time() \n","        s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","        \n","        #Evaluate initial random solutions\n","        for i in range(0,n):    \n","          Fitness[i]=objf(S[i,:],trainInput,trainOutput,dim)\n","        \n","        # Find the initial best solution\n","        fmin = min(Fitness)\n","        I=numpy.argmin(Fitness)\n","        best=Sol[I,:]\n","          \n","        # Main loop\n","        for t in range (0,N_gen): \n","            \n","            # Loop over all bats(solutions)\n","            for i in range (0,n):\n","                \n","            # for i in range(0,n):\n","            # while numpy.sum(S[i,:])==0: \n","              # S[i,:]=numpy.random.randint(2, size=(1,d))   \n","              Q[i]=Qmin+(Qmin-Qmax)*random.random()\n","              v[i,:]=v[i,:]+(Sol[i,:]-best)*Q[i]\n","              S[i,:]=Sol[i,:]+v[i,:]\n","\n","\n","              \n","            # Check boundaries\n","            #  Sol=numpy.clip(Sol,lb,ub)\n","              \n","              \n","        \n","              # Pulse rate\n","              if random.random()>r:\n","                  S[i,:]=best+0.001*numpy.random.randn(d) #update statement\n","             \n","              \n","              for f in range(0,dim):\n","                  ss= transfer_functions_benchmark.s1(S[i,f])#transfer function\n","                  if (random.random()<ss): \n","                      S[i,f]=1;\n","                  else:\n","                      S[i,f]=0;\n","              \n","                \n","            \n","              for i in range(0,n):\n","                while numpy.sum(S[i,:])==0: \n","                    S[i,:]=numpy.random.randint(2, size=(1,d))\n","            \n","            \n","              # Evaluate new solutions\n","              Fnew=objf(S[i,:],trainInput,trainOutput,dim)\n","              \n","              # Update if the solution improves\n","              if ((Fnew<=Fitness[i]) and (random.random()<A) ):\n","                    Sol[i,:]=numpy.copy(S[i,:])\n","\n","                    Fitness[i]=Fnew;\n","              \n","        \n","              # Update the current best solution\n","              if Fnew<=fmin:\n","                    best=S[i,:]\n","                    fmin=Fnew\n","                    \n","              featurecount=0\n","              for f in range(0,dim):\n","                  if best[f]==1:\n","                    featurecount=featurecount+1   \n","                    \n","                                  \n","            #update convergence curve\n","            Convergence_curve1.append(fmin)  \n","            Convergence_curve2.append(featurecount)  \n","\n","\n","            # if (t%1==0):\n","            #   print(['At iteration'+ str(t+1)+' the best fitness on trainig is:'+ str(fmin)+', the best number of features: '+str(featurecount)]);\n","            #   LOGGER_FILE.info(['At iteration'+ str(t+1)+' the best fitness on trainig is:'+ str(fmin)+', the best number of features: '+str(featurecount)]);\n","              \n","        \n","        \n","        \n","        \n","        timerEnd=time.time()  \n","        s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","        s.executionTime=timerEnd-timerStart\n","        s.bestIndividual=best\n","        s.convergence1=Convergence_curve1\n","        s.convergence2=Convergence_curve2\n","\n","        s.optimizer=\"BAT\"\n","        s.objfname=objf.__name__\n","        \n","        \n","        \n","        return s"]},{"cell_type":"markdown","metadata":{"id":"nMljtBlelwme"},"source":["### 3- CS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wzeUe6pglxuU"},"outputs":[],"source":["import math\n","import numpy\n","import random\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","    \n","\n","def get_cuckoos(nest,best,lb,ub,n,dim):\n","    \n","    # perform Levy flights\n","    tempnest=numpy.zeros((n,dim))\n","    tempnest=numpy.array(nest)\n","    beta=3/2;\n","    sigma=(math.gamma(1+beta)*math.sin(math.pi*beta/2)/(math.gamma((1+beta)/2)*beta*2**((beta-1)/2)))**(1/beta);\n","\n","    s=numpy.zeros(dim)\n","    for j in range (0,n):\n","        s=nest[j,:]\n","        u=numpy.random.randn(len(s))*sigma\n","        v=numpy.random.randn(len(s))\n","        step=u/abs(v)**(1/beta)\n"," \n","        stepsize=0.01*(step*(s-best))\n","\n","        s=s+stepsize*numpy.random.randn(len(s))\n","        tempnest[j,:]=transfer_functions_benchmark.s1(s)\n","        for i in range (0,dim):\n","            ss= transfer_functions_benchmark.s1(tempnest[j,i])\n","            if (random.random()<ss): \n","               tempnest[j,i]=1;\n","            else:\n","               tempnest[j,i]=0;\n","        \n","        \n","        while numpy.sum(tempnest[j,:])==0: \n","         tempnest[j,:]=numpy.random.randint(2, size=(1,dim))\n","        #tempnest[j,:]=numpy.clip(s, lb, ub)\n","    return tempnest\n","\n","def get_best_nest(nest,newnest,fitness,n,dim,objf,trainInput,trainOutput):\n","# Evaluating all new solutions\n","    tempnest=numpy.zeros((n,dim))\n","    tempnest=numpy.copy(nest)\n","    for j in range(0,n):\n","    #for j=1:size(nest,1),\n","        fnew=objf(newnest[j,:],trainInput,trainOutput,dim);\n","        \n","        if fnew<=fitness[j]:\n","           fitness[j]=fnew\n","           tempnest[j,:]=newnest[j,:]\n","        \n","    # Find the current best\n","\n","    fmin = min(fitness)\n","    K=numpy.argmin(fitness)\n","    bestlocal=tempnest[K,:]\n","\n","    return fmin,bestlocal,tempnest,fitness\n","\n","# Replace some nests by constructing new solutions/nests\n","def empty_nests(nest,pa,n,dim):\n","    # Discovered or not \n","    tempnest=numpy.zeros((n,dim))\n","\n","    K=numpy.random.uniform(0,1,(n,dim))>pa\n","   # K=numpy.random.randint(2, size=(n,dim))>pa \n","    stepsize=random.random()*(nest[numpy.random.permutation(n),:]-nest[numpy.random.permutation(n),:])\n","\n","    \n","    tempnest=nest+stepsize*K\n","    for i in range(0,n):\n","        for j in range(0,dim):\n","          if tempnest[i,j] >=.5:\n","              tempnest[i,j]=1\n","          else:\n","              tempnest[i,j]=0\n","    \n","    for i in range(0,n):\n","      while numpy.sum(tempnest[i,:])==0: \n","          tempnest[i,:]=numpy.random.randint(2, size=(1,dim))\n","    \n","   #      print(tempnest[j,:])\n","    \n","    return tempnest\n","##########################################################################\n","\n","class cs:\n","      def CS(objf,lb,ub,dim,n,N_IterTotal,trainInput,trainOutput):\n","\n","\n","          #lb=-1\n","          #ub=1\n","          #n=50\n","          #N_IterTotal=1000\n","          #dim=30\n","          \n","          # Discovery rate of alien eggs/solutions\n","          pa=0.25\n","          \n","          \n","          nd=dim\n","          \n","          \n","      #    Lb=[lb]*nd\n","      #    Ub=[ub]*nd\n","          convergence1=[]\n","          convergence2=[]\n","\n","          # RInitialize nests randomely\n","          #nest=numpy.random.rand(n,dim)*(ub-lb)+lb\n","          nest=numpy.random.randint(2, size=(n,dim))  \n","\n","          for i in range(0,n):\n","            while numpy.sum(nest[i,:])==0: \n","                nest[i,:]=numpy.random.randint(2, size=(1,dim))\n","\n","          \n","          new_nest=numpy.zeros((n,dim))\n","          new_nest=numpy.copy(nest)\n","          \n","          bestnest=[0]*dim;\n","          \n","          fitness=numpy.zeros(n) \n","          fitness.fill(float(\"inf\"))\n","          \n","\n","          s=solution()\n","\n","          \n","          print(\"CS is optimizing  \\\"\"+objf.__name__+\"\\\"\")    \n","          \n","          timerStart=time.time() \n","          s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","          \n","          fmin,bestnest,nest,fitness =get_best_nest(nest,new_nest,fitness,n,dim,objf,trainInput,trainOutput)\n","          # Main loop counter\n","          for iter in range (0,N_IterTotal):\n","              # Generate new solutions (but keep the current best)\n","          \n","              new_nest=get_cuckoos(nest,bestnest,lb,ub,n,dim)\n","              # Evaluate new solutions and find best\n","              fnew,best,nest,fitness=get_best_nest(nest,new_nest,fitness,n,dim,objf,trainInput,trainOutput)\n","              \n","              new_nest=empty_nests(new_nest,pa,n,dim) ;\n","\n","            \n","              \n","              \n","              # Evaluate new solutions and find best\n","              fnew,best,nest,fitness=get_best_nest(nest,new_nest,fitness,n,dim,objf,trainInput,trainOutput)\n","          \n","              if fnew<fmin:\n","                  fmin=fnew\n","                  bestnest=best\n","                  \n","                  \n","              featurecount=0\n","              for f in range(0,dim):\n","                  if best[f]==1:\n","                      featurecount=featurecount+1\n","              convergence1.append(fmin)\n","              convergence2.append(featurecount)\n","\n","        \n","                  \n","                            \n","          \n","              if (iter%10==0):\n","                  print(['At iteration '+ str(iter)+ ' the best fitness on trainig is '+ str(fmin)+ ',the best number of features: '+str(featurecount) ]);\n","\n","          timerEnd=time.time()  \n","          s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","          s.executionTime=timerEnd-timerStart\n","          s.bestIndividual=best\n","          s.convergence1=convergence1\n","          s.convergence2=convergence2\n","\n","          s.optimizer=\"CS\"\n","          s.objfname=objf.__name__\n","          \n","          \n","          \n","          return s"]},{"cell_type":"markdown","metadata":{"id":"JCE4YoxlmqpT"},"source":["### 4- FFA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5J1z8oamuDy"},"outputs":[],"source":["#% ======================================================== % \n","#% Files of the Matlab programs included in the book:       %\n","#% Xin-She Yang, Nature-Inspired Metaheuristic Algorithms,  %\n","#% Second Edition, Luniver Press, (2010).   www.luniver.com %\n","#% ======================================================== %    \n","#\n","#% -------------------------------------------------------- %\n","#% Firefly Algorithm for constrained optimization using     %\n","#% for the design of a spring (benchmark)                   % \n","#% by Xin-She Yang (Cambridge University) Copyright @2009   %\n","#% -------------------------------------------------------- %\n","\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","import random\n","\n","\n","def alpha_new(alpha,NGen):\n","    #% alpha_n=alpha_0(1-delta)^NGen=10^(-4);\n","    #% alpha_0=0.9\n","    delta=1-(10**(-4)/0.9)**(1/NGen);\n","    alpha=(1-delta)*alpha\n","    return alpha\n","\n","\n","\n","class ffa:\n","\n","      def FFA(objf,lb,ub,dim,n,MaxGeneration,trainInput,trainOutput,LOGGER_FILE):\n","\n","          #General parameters\n","\n","          #n=50 #number of fireflies\n","          #dim=30 #dim  \n","          #lb=-50\n","          #ub=50\n","          #MaxGeneration=500\n","      \n","          #FFA parameters\n","          alpha=0.5  # Randomness 0--1 (highly random)\n","          betamin=0.20  # minimum value of beta\n","          gamma=1   # Absorption coefficient\n","          \n","          \n","          \n","          zn=numpy.ones(n)\n","          zn.fill(float(\"inf\")) \n","          \n","          \n","          #ns(i,:)=Lb+(Ub-Lb).*rand(1,d);\n","        # ns=numpy.random.uniform(0,1,(n,dim)) *(ub-lb)+lb #generating continuous individuals\n","          \n","          ns=numpy.random.randint(2, size=(n,dim))          #generating binary individuals\n","          \n","          \n","              \n","          Lightn=numpy.ones(n)\n","          Lightn.fill(float(\"inf\")) \n","          \n","          #[ns,Lightn]=init_ffa(n,d,Lb,Ub,u0)\n","          \n","          convergence1=[]\n","          convergence2=[]\n","\n","          s=solution()\n","\n","          \n","          print(\"FFA is optimizing  \\\"\"+objf.__name__+\"\\\"\")    \n","          LOGGER_FILE.info(\"FFA is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","\n","          timerStart=time.time() \n","          s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","          \n","          # Main loop\n","          for k in range (0,MaxGeneration):     # start iterations\n","          \n","              #% This line of reducing alpha is optional\n","              alpha=alpha_new(alpha,MaxGeneration);\n","              \n","              #% Evaluate new solutions (for all n fireflies)\n","              for i in range(0,n):\n","          # the following statement insures that at least one feature is selected\n","          #(i.e the randomly generated individual has at least one value 1)  \n","                  while numpy.sum(ns[i,:])==0:   \n","                      ns[i,:]=numpy.random.randint(2, size=(1,dim))\n","                  \n","                  zn[i]=objf(ns[i,:],trainInput,trainOutput,dim);\n","                  Lightn[i]=zn[i]\n","              \n","              \n","                      \n","              \n","              # Ranking fireflies by their light intensity/objectives\n","          \n","              \n","              Lightn=numpy.sort(zn)\n","              Index=numpy.argsort(zn)\n","              ns=ns[Index,:]\n","              \n","              \n","              #Find the current best\n","              nso=ns\n","              Lighto=Lightn\n","              nbest=ns[0,:] \n","              Lightbest=Lightn[0]\n","              \n","              #% For output only\n","              fbest=Lightbest;\n","              \n","              \n","              BestQuality=fbest\n","\n","              featurecount=0\n","              for f in range(0,dim):\n","                  if nbest[f]==1:\n","                      featurecount=featurecount+1\n","              \n","              convergence1.append(BestQuality)\n","              convergence2.append(featurecount)\n","                \n","              \n","              # if (k%1==0):\n","              #       print(['At iteration '+ str(k)+ ' the best fitness on trainig is '+ str(BestQuality)+', the best number of features: '+str(featurecount)]);\n","              #       LOGGER_FILE.info(['At iteration '+ str(k)+ ' the best fitness on trainig is '+ str(BestQuality)+', the best number of features: '+str(featurecount)]);\n","              \n","              \n","              \n","              \n","                  \n","            \n","              \n","              \n","                \n","                      \n","              \n","              #% Move all fireflies to the better locations\n","          #    [ns]=ffa_move(n,d,ns,Lightn,nso,Lighto,nbest,...\n","          #          Lightbest,alpha,betamin,gamma,Lb,Ub);\n","              scale=numpy.ones(dim)*abs(ub-lb)\n","              for i in range (0,n):\n","                  # The attractiveness parameter beta=exp(-gamma*r)\n","                  for j in range(0,n):\n","                      r=numpy.sqrt(numpy.sum((ns[i,:]-ns[j,:])**2));\n","                      #r=1\n","                      # Update moves\n","                      if Lightn[i]>Lighto[j]: # Brighter and more attractive\n","                        beta0=1\n","                        beta=(beta0-betamin)*math.exp(-gamma*r**2)+betamin\n","                        tmpf=alpha*(numpy.random.rand(dim)-0.5)*scale\n","                        ns[i,:]=ns[i,:]*(1-beta)+nso[j,:]*beta+tmpf #update statement\n","                        for j in range (0,dim):\n","                            ss= transfer_functions_benchmark.s1(ns[i,j])\n","                        \n","                          \n","                            if (random.random()<ss): \n","                              ns[i,j]=1;\n","                            else:\n","                              ns[i,j]=0;\n","              \n","              #ns=numpy.clip(ns, lb, ub)\n","              \n","            \n","          #    \n","            ####################### End main loop\n","          timerEnd=time.time()  \n","          s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","          s.executionTime=timerEnd-timerStart\n","          s.bestIndividual=nbest\n","          s.convergence1=convergence1\n","          s.convergence2=convergence2\n","\n","          s.optimizer=\"FFA\"\n","          s.objfname=objf.__name__\n","          \n","          return s\n","          \n","          \n","          \n","          \n","          "]},{"cell_type":"markdown","metadata":{"id":"EiA0nKKvm7hD"},"source":["### 5- GWO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gg0dEtOgm9xT"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","# from utils.solution import solution\n","import time\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","\n","    \n","\n","class gwo: \n","    def GWO(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","        \n","        #Max_iter=1000\n","        #lb=-100\n","        #ub=100\n","        #dim=30  \n","        #SearchAgents_no=5\n","        \n","        # initialize alpha, beta, and delta_pos\n","        Alpha_pos=numpy.zeros(dim)\n","        Alpha_score=float(\"inf\")\n","        \n","        Beta_pos=numpy.zeros(dim)\n","        Beta_score=float(\"inf\")\n","        \n","        Delta_pos=numpy.zeros(dim)\n","        Delta_score=float(\"inf\")\n","        \n","        #initialization stage of positions of the search agents(either continuous or discrete (binary) individual generation)\n","      # Positions=numpy.random.uniform(0,1,(SearchAgents_no,dim)) *(ub-lb)+lb #generating continuous individuals\n","      \n","        Positions=numpy.random.randint(2, size=(SearchAgents_no,dim)) #generating binary individuals\n","        \n","        Convergence_curve1=numpy.zeros(Max_iter)\n","        Convergence_curve2=numpy.zeros(Max_iter)\n","\n","        s=solution()\n","\n","        # Loop counter\n","        print(\"GWO is optimizing  \\\"\"+objf.__name__+\"\\\"\")    \n","        LOGGER_FILE.info(\"GWO is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","\n","        timerStart=time.time() \n","        s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","        # Main loop\n","        for l in range(0,Max_iter):\n","            for i in range(0,SearchAgents_no):\n","                \n","                # Return back the search agents that go beyond the boundaries of the search space\n","                Positions[i,:]=numpy.clip(Positions[i,:], lb, ub)\n","                \n","                # the following statement insures that at least one feature is selected\n","                #(i.e the randomly generated individual has at least one value 1)       \n","                while numpy.sum(Positions[i,:])==0:   \n","                    Positions[i,:]=numpy.random.randint(2, size=(1,dim))\n","\n","                # Calculate objective function for each search agent\n","                fitness=objf(Positions[i,:],trainInput,trainOutput,dim)\n","                \n","\n","                # Update Alpha, Beta, and Delta\n","                if fitness<Alpha_score :\n","                    Alpha_score=fitness; # Update alpha\n","                    Alpha_pos=Positions[i,:].copy()\n","                \n","                \n","                if (fitness>Alpha_score and fitness<Beta_score ):\n","                    Beta_score=fitness  # Update beta\n","                    Beta_pos=Positions[i,:].copy()\n","                \n","                \n","                if (fitness>Alpha_score and fitness>Beta_score and fitness<Delta_score): \n","                    Delta_score=fitness # Update delta\n","                    Delta_pos=Positions[i,:].copy()\n","                \n","            \n","            \n","            \n","            a=.01-l*((.01)/(1500*Max_iter)); # a decreases linearly fron 2 to 0\n","            \n","            # Update the Position of search agents including omegas\n","            for i in range(0,SearchAgents_no):\n","                for j in range (0,dim):     \n","                              \n","                    r1=random.random() # r1 is a random number in [0,1]\n","                    r2=random.random() # r2 is a random number in [0,1]\n","                    \n","                    A1=0.4*a*r1-a; # Equation (3.3)\n","                    C1=2*r2; # Equation (3.4)\n","                    \n","                    D_alpha=abs(C1*Alpha_pos[j]-Positions[i,j]); # Equation (3.5)-part 1\n","                  # X1=Alpha_pos[j]-A1*D_alpha; # Equation (3.6)-part 1\n","                    temp=transfer_functions_benchmark.s1(A1*D_alpha)\n","                    if temp<numpy.random.uniform(0,1):\n","                        temp=0\n","                    else:\n","                        temp=1\n","                    if (Alpha_pos[j]+temp)>=1:\n","                        X1=Alpha_pos[j]+temp\n","                    \n","                              \n","                    r1=random.random()\n","                    r2=random.random()\n","                    \n","                    A2=0.03*a*r1-a; # Equation (3.3)\n","                    C2=2*r2; # Equation (3.4)\n","                    \n","                    D_beta=abs(C2*Beta_pos[j]-Positions[i,j]); # Equation (3.5)-part 2\n","                  #  X2=Beta_pos[j]-A2*D_beta; # Equation (3.6)-part 2 \n","                    temp=transfer_functions_benchmark.s1(A2*D_beta)\n","                    \n","                    if temp<numpy.random.uniform(0,1):\n","                        temp=0\n","                    else:\n","                        temp=1\n","                        \n","                    if (Beta_pos[j]+temp)>=1:\n","                        X2=Beta_pos[j]+temp\n","                    \n","                    \n","                    r1=random.random()\n","                    r2=random.random() \n","                    \n","                    A3=0.3*a*r1-a; # Equation (3.3)\n","                    C3=2*r2; # Equation (3.4)\n","                    \n","                    D_delta=abs(C3*Delta_pos[j]-Positions[i,j]); # Equation (3.5)-part 3\n","                  # X3=Delta_pos[j]-A3*D_delta; # Equation (3.5)-part 3   \n","                    \n","                    temp=transfer_functions_benchmark.s1(A3*D_delta)\n","                    if temp<numpy.random.uniform(0,1):\n","                        temp=0\n","                    else:\n","                        temp=1\n","                        \n","                    if (Delta_pos[j]+temp)>=1:\n","                        X3=Delta_pos[j]+temp\n","                    \n","                Positions[i,j]=(X1+X2+X3)/3  # Equation (3.7)\n","                \n","                \n","            featurecount=0\n","            for f in range(0,dim):\n","                if Alpha_pos[f]==1:\n","                    featurecount=featurecount+1    \n","                \n","                              \n","                \n","                \n","                \n","                \n","            \n","            Convergence_curve1[l]=Alpha_score;\n","            Convergence_curve2[l]=featurecount;\n","            # if (l%1==0):\n","            #         print(['At iteration'+ str(l+1)+' the best fitness on trainig is:'+ str(Alpha_score)+', the best number of features: '+str(featurecount)]);\n","            #         LOGGER_FILE.info(['At iteration'+ str(l+1)+' the best fitness on trainig is:'+ str(Alpha_score)+', the best number of features: '+str(featurecount)]);\n","        \n","        \n","        \n","            \n","        \n","        \n","        \n","        \n","        timerEnd=time.time()  \n","        s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","        s.executionTime=timerEnd-timerStart\n","        s.bestIndividual=Alpha_pos\n","        s.convergence1=Convergence_curve1\n","        s.convergence2=Convergence_curve2\n","\n","        s.optimizer=\"GWO\"\n","        s.objfname=objf.__name__\n","        \n","        \n","        \n","        \n","        return s\n","        \n","\n"]},{"cell_type":"markdown","metadata":{"id":"Bbg2_cKGoE3s"},"source":["### 6- MFO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W1palqpvoIaz"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","\n","class mfo:\n","      def MFO(objf,lb,ub,dim,N,Max_iteration,trainInput,trainOutput,LOGGER_FILE):\n","\n","          #Initialize the positions of moths\n","        # Moth_pos=numpy.random.uniform(0,1,(N,dim)) *(ub-lb)+lb #generating continuous individuals\n","          Moth_pos=numpy.random.randint(2, size=(N,dim))          #generating binary individuals\n","          \n","          Moth_fitness=numpy.full(N,float(\"inf\"))\n","          #Moth_fitness=numpy.fell(float(\"inf\"))\n","          \n","          Convergence_curve1=numpy.zeros(Max_iteration)\n","          Convergence_curve2=numpy.zeros(Max_iteration)\n","\n","          \n","          sorted_population=numpy.copy(Moth_pos)\n","          fitness_sorted=numpy.zeros(N)\n","          #####################\n","          best_flames=numpy.copy(Moth_pos)\n","          best_flame_fitness=numpy.zeros(N)\n","          ####################\n","          double_population=numpy.zeros((2*N,dim))\n","          double_fitness=numpy.zeros(2*N)\n","          \n","          double_sorted_population=numpy.zeros((2*N,dim))\n","          double_fitness_sorted=numpy.zeros(2*N)\n","          #########################\n","          previous_population=numpy.zeros((N,dim));\n","          previous_fitness=numpy.zeros(N)\n","\n","\n","          s=solution()\n","\n","          print(\"MFO is optimizing  \\\"\"+objf.__name__+\"\\\"\")\n","          LOGGER_FILE.info(\"MFO is optimizing  \\\"\"+objf.__name__+\"\\\"\")    \n","\n","          timerStart=time.time() \n","          s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","          \n","          Iteration=1;    \n","          \n","          # Main loop\n","          while (Iteration<Max_iteration+1):\n","              \n","              # Number of flames Eq. (3.14) in the paper\n","              Flame_no=round(N-Iteration*((N-1)/Max_iteration));\n","              \n","              for i in range(0,N):\n","                  \n","                  # Check if moths go out of the search spaceand bring it back\n","                # Moth_pos[i,:]=numpy.clip(Moth_pos[i,:], lb, ub) \n","          \n","          # the following statement insures that at least one feature is selected\n","        #(i.e the randomly generated individual has at least one value 1)       \n","                  while numpy.sum(Moth_pos[i,:])==0:   \n","                      Moth_pos[i,:]=numpy.random.randint(2, size=(1,dim))\n","\n","                  # evaluate moths\n","                  Moth_fitness[i]=objf(Moth_pos[i,:],trainInput,trainOutput,dim)\n","                  \n","              \n","                \n","              if Iteration==1:\n","                  # Sort the first population of moths\n","                  fitness_sorted=numpy.sort(Moth_fitness)\n","                  I=numpy.argsort(Moth_fitness)\n","                  \n","                  sorted_population=Moth_pos[I,:]\n","                    \n","                  \n","                  #Update the flames\n","                  best_flames=sorted_population;\n","                  best_flame_fitness=fitness_sorted;\n","              else:\n","          #        \n","          #        # Sort the moths\n","                  double_population=numpy.concatenate((previous_population,best_flames),axis=0)\n","                  double_fitness=numpy.concatenate((previous_fitness, best_flame_fitness),axis=0);\n","          #        \n","                  double_fitness_sorted =numpy.sort(double_fitness);\n","                  I2 =numpy.argsort(double_fitness);\n","          #        \n","          #        \n","                  for newindex in range(0,2*N):\n","                      double_sorted_population[newindex,:]=numpy.array(double_population[I2[newindex],:])           \n","                  \n","                  fitness_sorted=double_fitness_sorted[0:N]\n","                  sorted_population=double_sorted_population[0:N,:]\n","          #        \n","          #        # Update the flames\n","                  best_flames=sorted_population;\n","                  best_flame_fitness=fitness_sorted;\n","          \n","          #    \n","          #   # Update the position best flame obtained so far\n","              Best_flame_score=fitness_sorted[0]\n","              Best_flame_pos=sorted_population[0,:]\n","          #      \n","              previous_population=Moth_pos;\n","              previous_fitness=Moth_fitness;\n","              \n","              \n","              \n","              \n","              \n","              \n","              \n","              featurecount=0\n","              for f in range(0,dim):\n","                  if Best_flame_pos[f]==1:\n","                      featurecount=featurecount+1\n","              \n","              \n","      #        print(Best_flame_pos)\n","      #        print(Best_flame_score)\n","      #        \n","      #   Convergence_curve[Iteration-1]=(Best_flame_score)\n","              Convergence_curve1[Iteration-1]=Best_flame_score# store the best number of features\n","              Convergence_curve2[Iteration-1]=featurecount#store the best fitness on testing returened from F11\n","\n","\n","            #Display best fitness along the iteration\n","              # if (Iteration%1==0):\n","              #     print(['At iteration'+ str(Iteration+1)+' the best fitness on trainig is:'+ str(Best_flame_score)+', the best number of features: '+str(featurecount)]);\n","              #     LOGGER_FILE.info(['At iteration'+ str(Iteration+1)+' the best fitness on trainig is:'+ str(Best_flame_score)+', the best number of features: '+str(featurecount)]);\n","              \n","              \n","              \n","              # a linearly dicreases from -1 to -2 to calculate t in Eq. (3.12)\n","              a=-1+Iteration*((-1)/Max_iteration);\n","              \n","\n","              \n","              # Loop counter\n","              for i in range(0,N):\n","          #        \n","                  for j in range(0,dim):\n","                      if (i<=Flame_no): #Update the position of the moth with respect to its corresponsing flame\n","          #                \n","                          # D in Eq. (3.13)\n","                          distance_to_flame=abs(sorted_population[i,j]-Moth_pos[i,j])\n","                          b=1\n","                          t=(a-1)*random.random()+1;\n","          #                \n","          #                % Eq. (3.12)\n","                          Moth_pos[i,j]=distance_to_flame*math.exp(b*t)*math.cos(t*2*math.pi)+sorted_population[i,j]#update statement\n","                          ss= transfer_functions_benchmark.s1(Moth_pos[i,j])\n","                          \n","                          if (random.random()<ss): \n","                            Moth_pos[i,j]=1;\n","                          else:\n","                            Moth_pos[i,j]=0;\n","\n","          #            end\n","          #            \n","                      if i>Flame_no: # Upaate the position of the moth with respct to one flame\n","          #                \n","          #                % Eq. (3.13)\n","                          distance_to_flame=abs(sorted_population[i,j]-Moth_pos[i,j]);\n","                          b=1;\n","                          t=(a-1)*random.random()+1;\n","          #                \n","          #                % Eq. (3.12)\n","                          Moth_pos[i,j]=distance_to_flame*math.exp(b*t)*math.cos(t*2*math.pi)+sorted_population[Flame_no,j]#update statement\n","                          ss= transfer_functions_benchmark.s1(Moth_pos[i,j])\n","                          \n","                          if (random.random()<ss): \n","                            Moth_pos[i,j]=1;\n","                          else:\n","                            Moth_pos[i,j]=0;\n","              #Display best fitness along the iteration\n","      #        if (Iteration%1==0):\n","      #            print(['At iteration '+ str(Iteration)+ ' the best fitness is '+ str(Best_flame_score)]);\n","      #            Convergence_curve[Iteration-1]=(Best_flame_score)\n","\n","          \n","\n","          \n","              Iteration=Iteration+1; \n","\n","          timerEnd=time.time()  \n","          s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","          s.executionTime=timerEnd-timerStart\n","          s.bestIndividual=Best_flame_pos\n","          s.convergence1=Convergence_curve1\n","          s.convergence2=Convergence_curve2\n","\n","          s.optimizer=\"MFO\"\n","          s.objfname=objf.__name__\n","          \n","          \n","          \n","          return s"]},{"cell_type":"markdown","metadata":{"id":"lyspS7JcpGgs"},"source":["### 7- MVO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C4E8g1PMpImF"},"outputs":[],"source":["import random\n","import numpy\n","import time\n","import math\n","import sklearn\n","from numpy import asarray\n","from sklearn.preprocessing import normalize\n","from random import randint\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","\n","\n","def normr(Mat):\n","   \"\"\"normalize the columns of the matrix\n","   B= normr(A) normalizes the row\n","   the dtype of A is float\"\"\"\n","   Mat=Mat.reshape(1, -1)\n","     # Enforce dtype float\n","   if Mat.dtype!='float':\n","      Mat = asarray(Mat,dtype=float)\n","\n","   # if statement to enforce dtype float\n","   B = normalize(Mat,norm='l2',axis=1)\n","   B=numpy.reshape(B,-1)\n","   return B\n","\n","def randk(t):\n","    if (t%2)==0:\n","        s=0.25\n","    else:\n","         s=0.75\n","    return s\n","\n","def RouletteWheelSelection(weights):\n","  accumulation = numpy.cumsum(weights)\n","  p = random.random() * accumulation[-1]\n","  chosen_index = -1;\n","  for index in range (0, len(accumulation)):\n","    if (accumulation[index] > p):\n","      chosen_index = index;\n","      break;\n","  \n","  choice = chosen_index;\n","\n","  return choice\n","\n","class mvo:\n","    def  MVO(objf,lb,ub,dim,N,Max_time,trainInput,trainOutput,LOGGER_FILE):\n","        \"parameters\"\n","        #dim=30\n","        #lb=-100\n","        #ub=100\n","        WEP_Max=1;\n","        WEP_Min=0.2\n","        #Max_time=1000\n","        #N=50\n","        \n","        \n","        #initialization stage of the population(either continuous or discrete binary individual generation)\n","        \n","      # Universes=numpy.random.uniform(0,1,(N,dim)) *(ub-lb)+lb \n","        \n","      \n","      # this statement for generating of continuous individuals \n","        \n","        Universes=numpy.random.randint(2, size=(N,dim))  \n","        #this statement for generating binary individuals with discrete values either 0 or 1 ---[0,2)\n","        #suitable for feature selection problem\n","        #this can be done in another way by generating continuous individuals and \n","        #then applying threshold based conversion to convert the values of the individual \n","        #above certain threshold to 1 and the values below the threshold to 0\n","      \n","            \n","\n","\n","        Sorted_universes=numpy.copy(Universes)\n","        \n","        convergence1=numpy.zeros(Max_time)\n","        convergence2=numpy.zeros(Max_time)\n","\n","        \n","        Best_universe=[0]*dim;\n","        Best_universe_Inflation_rate= float(\"inf\")\n","              \n","        \n","        s=solution()\n","\n","        \n","        Time=1;\n","        ############################################\n","        print(\"MVO is optimizing  \\\"\"+objf.__name__+\"\\\"\")    \n","        LOGGER_FILE.info(\"MVO is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","\n","        timerStart=time.time() \n","        s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","        while (Time<Max_time+1):\n","        \n","            \"Eq. (3.3) in the paper\"\n","            WEP=WEP_Min+Time*((WEP_Max-WEP_Min)/Max_time)\n","          \n","            TDR=1-(math.pow(Time,1/6)/math.pow(Max_time,1/6))\n","          \n","            Inflation_rates=[0]*len(Universes)\n","            \n","          \n","          \n","            for i in range(0,N):\n","              #  Universes[i,:]=numpy.clip(Universes[i,:], lb, ub)\n","        \n","      # the following statement insures that at least one feature is selected\n","      #(i.e the randomly generated individual has at least one value 1)       \n","              \n","                while numpy.sum(Universes[i,:])==0:   \n","                    Universes[i,:]=numpy.random.randint(2, size=(1,dim))\n","                    \n","        \n","                Inflation_rates[i]=objf(Universes[i,:],trainInput,trainOutput,dim);\n","              \n","          \n","                  \n","                if Inflation_rates[i]<Best_universe_Inflation_rate :\n","                            \n","                    Best_universe_Inflation_rate=Inflation_rates[i]\n","                    Best_universe=numpy.array(Universes[i,:])\n","                    \n","                    \n","                featurecount=0\n","                for f in range(0,dim):\n","                    if Best_universe[f]==1:\n","                        featurecount=featurecount+1\n","                    \n","                convergence1[Time-1]=Best_universe_Inflation_rate# store the best number of features\n","                convergence2[Time-1]=featurecount#store the best fitness on testing returened from F11\n","\n","                \n","                    \n","            # if (Time%1==0):\n","            #       print(['At iteration '+ str(Time)+ ' the best fitness on trainig is: '+ str(Best_universe_Inflation_rate)+', the best number of features: '+str(featurecount)]);\n","            #       LOGGER_FILE.info(['At iteration '+ str(Time)+ ' the best fitness on trainig is: '+ str(Best_universe_Inflation_rate)+', the best number of features: '+str(featurecount)]);\n","\n","                \n","                \n","                \n","                \n","                \n","            \n","            sorted_Inflation_rates = numpy.sort(Inflation_rates)\n","            sorted_indexes = numpy.argsort(Inflation_rates)\n","            \n","            for newindex in range(0,N):\n","                Sorted_universes[newindex,:]=numpy.array(Universes[sorted_indexes[newindex],:])   \n","                \n","            normalized_sorted_Inflation_rates=numpy.copy(normr(sorted_Inflation_rates))\n","        \n","            \n","            Universes[0,:]= numpy.array(Sorted_universes[0,:])\n","        \n","            for i in range(1,N):\n","                Back_hole_index=i\n","                for j in range(0,dim):\n","                    r1=random.random()\n","                    \n","                    if r1<normalized_sorted_Inflation_rates[i]:\n","                        White_hole_index=RouletteWheelSelection(-sorted_Inflation_rates);\n","        \n","                        if White_hole_index==-1:\n","                            White_hole_index=0;\n","                        White_hole_index=0;\n","                        Universes[Back_hole_index,j]=Sorted_universes[White_hole_index,j];\n","                \n","                \n","                \n","                #update statemnts of the universe using transfer functions instead of conventional operators\n","                    r2=random.random() \n","                    \n","                    \n","                    if r2<WEP:\n","                        r3=random.random() \n","                        if r3<0.5:                    \n","    #                        Universes[i,j]=Best_universe[j]+TDR*((ub-lb)*random.random()+lb) #random.uniform(0,1)+lb);\n","                            Universes[i,j]=Best_universe[j]+TDR*random.random()\n","                            # Universes[i,j]=1 / (1. + numpy.exp(- 10 * ( Universes[i,j] - .5)))\n","                            ss= transfer_functions_benchmark.s1(Universes[i,j])\n","                        \n","                            if (random.random()<ss): \n","                                Universes[i,j]=1;\n","                            else:\n","                                Universes[i,j]=0;\n","                            \n","                        if r3>0.5:          \n","    #                        Universes[i,j]=Best_universe[j]-TDR*((ub-lb)*random.random()+lb) #random.uniform(0,1)+lb);\n","                            Universes[i,j]=Best_universe[j]-TDR*random.random()\n","                            # Universes[i,j]=1 / (1. + numpy.exp(- 10 * ( Universes[i,j] - .5)))\n","                            ss= transfer_functions_benchmark.s1(Universes[i,j])\n","                        \n","                            if (random.random()<ss): \n","                                Universes[i,j]=1;\n","                            else:\n","                                Universes[i,j]=0;\n","            \n","            \n","          \n","            \n","            \n","            \n","            Time=Time+1\n","      \n","        \n","        timerEnd=time.time()  \n","        s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","        s.executionTime=timerEnd-timerStart\n","        s.bestIndividual=Best_universe\n","        s.convergence1=convergence1\n","        s.convergence2=convergence2\n","\n","        s.optimizer=\"MVO\"\n","        s.objfname=objf.__name__\n","\n","        return s\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UDs3KkZupcQz"},"source":["### 8- WOA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PB9AH_8lpeg9"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","\n","\n","class woa:\n","      def WOA(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","\n","\n","          #dim=30\n","          #SearchAgents_no=50\n","          #lb=-100\n","          #ub=100\n","          #Max_iter=500\n","              \n","          \n","          # initialize position vector and score for the leader\n","          Leader_pos=numpy.zeros(dim)\n","          Leader_score=float(\"inf\")  #change this to -inf for maximization problems\n","          \n","          \n","          #Initialize the positions of search agents\n","        # Positions=numpy.random.uniform(0,1,(SearchAgents_no,dim)) *(ub-lb)+lb #generating continuous individuals\n","          Positions=numpy.random.randint(2, size=(SearchAgents_no,dim))#generating binary individuals\n","          #Initialize convergence\n","          convergence_curve1=numpy.zeros(Max_iter)\n","          convergence_curve2=numpy.zeros(Max_iter)\n","\n","          \n","          ############################\n","          s=solution()\n","\n","          print(\"WOA is optimizing  \\\"\"+objf.__name__+\"\\\"\")    \n","          LOGGER_FILE.info(\"WOA is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","\n","          timerStart=time.time() \n","          s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","          ############################\n","          \n","          t=0  # Loop counter\n","          \n","          # Main loop\n","          while t<Max_iter:\n","              for i in range(0,SearchAgents_no):\n","                  \n","                  # Return back the search agents that go beyond the boundaries of the search space\n","                  \n","                  #Positions[i,:]=checkBounds(Positions[i,:],lb,ub)          \n","                # Positions[i,:]=numpy.clip(Positions[i,:], lb, ub)\n","                  \n","                  \n","                  # the following statement insures that at least one feature is selected\n","                  #(i.e the randomly generated individual has at least one value 1)\n","                  while numpy.sum(Positions[i,:])==0:   \n","                      Positions[i,:]=numpy.random.randint(2, size=(1,dim))\n","\n","                  # Calculate objective function for each search agent\n","                  fitness=objf(Positions[i,:],trainInput,trainOutput,dim);\n","                  # print(fitness)\n","\n","                  # Update the leader\n","                  if fitness<Leader_score: # Change this to > for maximization problem\n","                      Leader_score=fitness; # Update alpha\n","                      Leader_pos=Positions[i,:].copy() # copy current whale position into the leader position\n","                  \n","                  \n","                  featurecount=0\n","                  for f in range(0,dim):\n","                    if Leader_pos[f]==1:\n","                      featurecount=featurecount+1\n","                  \n","                  \n","                  convergence_curve1[t]=Leader_score\n","                  convergence_curve2[t]=featurecount\n","                  # if (t%1==0):\n","                  #   print(['At iteration '+ str(t)+ ' the best fitness on trainig is: '+ str(Leader_score)+'the best number of features: '+str(featurecount)]);\n","                  #   LOGGER_FILE.info(['At iteration '+ str(t)+ ' the best fitness on trainig is: '+ str(Leader_score)+'the best number of features: '+str(featurecount)]);\n","              \n","              \n","              \n","                      \n","              a=2-t*((2)/Max_iter); # a decreases linearly fron 2 to 0 in Eq. (2.3)\n","              \n","              # a2 linearly decreases from -1 to -2 to calculate t in Eq. (3.12)\n","              a2=-1+t*((-1)/Max_iter);\n","              \n","              # Update the Position of search agents \n","              for i in range(0,SearchAgents_no):\n","                  r1=random.random() # r1 is a random number in [0,1]\n","                  r2=random.random() # r2 is a random number in [0,1]\n","                  \n","                  A=2*a*r1-a  # Eq. (2.3) in the paper\n","                  C=2*r2      # Eq. (2.4) in the paper\n","                  \n","                  \n","                  b=1;               #  parameters in Eq. (2.5)\n","                  l=(a2-1)*random.random()+1   #  parameters in Eq. (2.5)\n","                  \n","                  p = random.random()        # p in Eq. (2.6)\n","                  \n","                  for j in range(0,dim):\n","                      \n","                      if p<0.5:\n","                          if abs(A)>=1:\n","                              rand_leader_index = math.floor(SearchAgents_no*random.random());\n","                              X_rand = Positions[rand_leader_index, :]\n","                              D_X_rand=abs(C*X_rand[j]-Positions[i,j]) \n","                              Positions[i,j]=X_rand[j]-A*D_X_rand   #update statement\n","                              Positions[i,j]= transfer_functions_benchmark.v1(Positions[i,j])\n","                              \n","                          elif abs(A)<1:\n","                              D_Leader=abs(C*Leader_pos[j]-Positions[i,j]) \n","                              Positions[i,j]=Leader_pos[j]-A*D_Leader    #update statement  \n","                              \n","                              ss= transfer_functions_benchmark.s1(Positions[i,j])\n","                          \n","                              if (random.random()<ss): \n","                                  Positions[i,j]=1;\n","                              else:\n","                                  Positions[i,j]=0;\n","\n","                          \n","                      elif p>=0.5:\n","                        \n","                          distance2Leader=abs(Leader_pos[j]-Positions[i,j])\n","                          # Eq. (2.5)\n","                          Positions[i,j]=distance2Leader*math.exp(b*l)*math.cos(l*2*math.pi)+Leader_pos[j]\n","                          Positions[i,j]= transfer_functions_benchmark.v1(Positions[i,j])\n","                          \n","                          ss= transfer_functions_benchmark.s1(Positions[i,j])\n","                          \n","                          if (random.random()<ss): \n","                              Positions[i,j]=1;\n","                          else:\n","                              Positions[i,j]=0;\n","                          \n","\n","            \n","              \n","            \n","              t=t+1\n","          \n","          timerEnd=time.time()  \n","          s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","          s.executionTime=timerEnd-timerStart\n","          s.bestIndividual=Leader_pos\n","\n","          s.convergence1=convergence_curve1\n","          print(\"convergence_curve1\")\n","          # print(np.shape(convergence_curve1)) #(10,)\n","          # print(convergence_curve1) #[0.01114948 0.01114948 0.01114948 0.01114948 0.01102585 0.01083698 0.01083698 0.01083698 0.01083698 0.01069704]        \n","\n","          s.convergence2=convergence_curve2\n","          print(\"convergence_curve2\")\n","          # print(np.shape(convergence_curve2)) #(10,)\n","          # print(convergence_curve2) #[62. 62. 62. 62. 48. 58. 58. 58. 58. 50.]\n","\n","          s.optimizer=\"WOA\"\n","          s.objfname=objf.__name__\n","\n","          return s\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mIAkqO8ZnL-D"},"source":["## 9- HGS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QPo4PVRHnbZT"},"outputs":[],"source":["from numpy import where, clip, logical_and, exp, abs, ones, sum, array, mean\n","from numpy.random import uniform, normal, rand\n","from copy import deepcopy\n","from colorama import Fore, Back, Style\n","import time\n","from sklearn.preprocessing import Binarizer\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","from random import seed\n","import numpy\n","import random\n","\n","\n","class Root:\n","    \"\"\" This is root of all Algorithms \"\"\"\n","\n","    ID_MIN_PROB = 0  # min problem\n","    ID_MAX_PROB = -1  # max problem\n","\n","    ID_POS = 0  # Position\n","    ID_FIT = 1  # Fitness\n","\n","    EPSILON = 10E-10    # Avoid division by 0 \n","\n","    DEFAULT_LB = -1\n","    DEFAULT_UB = 1\n","\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True):\n","        self.verbose = verbose\n","        self.obj_func = obj_func\n","        if lb is None:\n","            print(\"Lower bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.lb = array(lb)\n","        if ub is None:\n","            print(\"Upper bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.ub = array(ub)\n","           \n","        if len(lb) != len(ub):\n","            print(\"Lower bound and Upper bound need to have the same length\")\n","            exit(0)\n"," \n","        \n","        self.problem_size = len(lb)\n","        self.solution, self.loss_train, self.fs_counts = None, [], []\n","\n","    def create_solution(self, minmax=0):\n","        \"\"\" Return solution with 2 element: position of solution and fitness of solution\n","        Parameters\n","        ----------\n","        minmax\n","            0 - minimum problem, else - maximum problem\n","        \"\"\"\n","        position = uniform(self.lb, self.ub)\n","        # print(position)\n","        # position=numpy.random.randint(2, size=(self.pop_size,self.dim))\n","        # for i in range(0,self.pop_size):\n","        #   while numpy.sum(position[i,:])==0:   \n","        #           position[i,:]=numpy.random.randint(2, size=(1,self.dim))\n","        fitness, binary_pos = self.get_fitness_position(position=position, minmax=minmax)\n","        return [position, fitness, binary_pos]\n","\n","    def get_fitness_position(self,position=None, minmax=0):\n","        \"\"\"     Assumption that objective function always return the original value\n","        :param position: 1-D numpy array\n","        :param minmax: 0- min problem, 1 - max problem\n","        :return:\n","        \"\"\"\n","        # print(position)\n","        Positions=position;\n","         \n","        \n","        for j in range(0,self.dim):\n","          ss= transfer_functions_benchmark.s1(position[j])\n","          if (random.random()<ss): \n","              Positions[j]=1;\n","          else:\n","              Positions[j]=0;\n","\n","        return self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) if minmax == 0 else 1.0 / (self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) + self.EPSILON), Positions     # Avoid division by 0\n","\n","    def get_fitness_solution(self,solution=None, minmax=0):\n","        return self.get_fitness_position(solution[self.ID_POS],self.trainInput,self.trainOutput,self.dim, minmax)\n","\n","    def get_global_best_global_worst_solution(self, pop=None, id_fit=None, id_best=None):\n","        sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n","        if id_best == self.ID_MIN_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MAX_PROB])\n","        elif id_best == self.ID_MAX_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MIN_PROB])\n","\n","    def update_global_best_global_worst_solution(self, pop=None, id_best=None, id_worst=None, g_best=None):\n","        \"\"\" Sort the copy of population and update the current best position. Return the new current best position \"\"\"\n","        sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n","        current_best = sorted_pop[id_best]\n","        g_best = deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n","        return g_best, sorted_pop[id_worst]\n","\n","\n","    def amend_position(self, position=None):\n","        return clip(position, self.lb, self.ub)\n","\n","    def amend_position_random(self, position=None):\n","        return where(logical_and(self.lb <= position, position <= self.ub), position, uniform(self.lb, self.ub))\n","\n","\n","\n","class HGSClass(Root):\n","\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True, epoch=1000, pop_size=100, L=0.08, LH=10000,trainInput=None,trainOutput=None,dim=None, logger=None):\n","        super().__init__(obj_func, lb, ub, verbose)\n","        # print(trainInput.shape)\n","        self.epoch = epoch\n","        self.pop_size = pop_size\n","        self.L = L\n","        self.LH = LH\n","        self.trainInput=trainInput\n","        self.dim=dim\n","        self.trainOutput=trainOutput\n","        self.logger = logger\n","        \n","\n","    def get_hunger_list(self, pop=None, hunger_list=array, g_best=None, g_worst=None):\n","        # min_index = pop.index(min(pop, key=lambda x: x[self.ID_FIT]))\n","        # Eq (2.8) and (2.9)\n","        for i in range(0, self.pop_size):\n","            r = rand()\n","            # space: since we pass lower bound and upper bound as list. Better take the mean of them.\n","            space = mean(self.ub - self.lb)\n","            H = (pop[i][self.ID_FIT] - g_best[self.ID_FIT]) / (g_worst[self.ID_FIT] - g_best[self.ID_FIT] + self.EPSILON) * r * 2 * space\n","            if H < self.LH:\n","                H = self.LH * (1 + r)\n","            hunger_list[i] += H\n","\n","            if g_best[self.ID_FIT] == pop[i][self.ID_FIT]:\n","                hunger_list[i] = 0\n","        return hunger_list\n","\n","    def sech(self, x):\n","        return 2 / (exp(x) + exp(-x))\n","\n","    def train(self):\n","        # Hungry value of all solutions\n","        hunger_list = ones(self.pop_size)\n","\n","        # Create population\n","        pop = [self.create_solution(self) for _ in range(self.pop_size)]\n","\n","        ## Eq. (2.2)\n","        ### Find the current best and current worst\n","        g_best, g_worst = self.get_global_best_global_worst_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","\n","        hunger_list = self.get_hunger_list(pop, hunger_list, g_best, g_worst)\n","        \n","        # Loop\n","        for epoch in range(self.epoch):\n","\n","            ## Eq. (2.4)\n","            shrink = 2 * (1 - (epoch+1) / self.epoch)\n","\n","            for i in range(0, self.pop_size):\n","                #### Variation control\n","                E = self.sech(pop[i][self.ID_FIT] - g_best[self.ID_FIT])\n","\n","                # R is a ranging controller added to limit the range of activity, in which the range of R is gradually reduced to 0\n","                R = 2 * shrink * rand() - shrink        # Eq. (2.3)\n","\n","                ## Calculate the hungry weight of each position\n","                if rand() < self.L:\n","                    W1 = hunger_list[i] * self.pop_size / (sum(hunger_list) + self.EPSILON) * rand()\n","                else:\n","                    W1 = 1\n","                W2 = (1 - exp(-abs(hunger_list[i] - sum(hunger_list)))) * rand() * 2\n","\n","                ### Udpate position of individual Eq. (2.1)    \n","                r1 = rand()\n","                r2 = rand()\n","                if r1 < self.L:\n","                    pos_new = pop[i][self.ID_POS] * (1 + normal(0, 1))\n","                else:\n","                    if r2 > E:\n","                        pos_new = W1 * g_best[self.ID_POS] + R * W2 * abs(g_best[self.ID_POS] - pop[i][self.ID_POS])\n","                    else:\n","                        pos_new = W1 * g_best[self.ID_POS] - R * W2 * abs(g_best[self.ID_POS] - pop[i][self.ID_POS])\n","                fit_new, pos_new_binary = self.get_fitness_position(pos_new)\n","                pop[i] = [pos_new, fit_new, pos_new_binary]\n","\n","            ## Update global best and global worst\n","            g_best, g_worst = self.update_global_best_global_worst_solution(pop, self.ID_MIN_PROB, self.ID_MAX_PROB, g_best)\n","\n","            ## Update hunger list\n","            hunger_list = self.get_hunger_list(pop, hunger_list, g_best, g_worst)\n","\n","            self.loss_train.append(g_best[self.ID_FIT])\n","            self.fs_counts.append(sum(g_best[2]))\n","\n","            if (epoch%1==0):\n","                print(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[2]))])\n","                self.logger.info(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[2]))])\n","                \n","        self.solution = g_best\n","        return g_best[2], g_best[self.ID_FIT], self.loss_train, self.fs_counts\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"So7fPlfinNvT"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","# from .HGSClass import HGSClass\n","from numpy.random import uniform, normal, rand\n","\n","class hgs:\n","\n","      def HGS(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","    \n","          #Max_iter=1000\n","          #lb=-100\n","          #ub=100\n","          #dim=30  \n","          #SearchAgents_no=5\n","          \n","          # initialize alpha, beta, and delta_pos\n","          g_best=numpy.zeros(dim)\n","          Alpha_score=float(\"inf\")\n","          XXX=1\n","          verbose = True\n","          # Loop counter\n","          print(\"HGS is optimizing  \\\"\"+objf.__name__+\"\\\"\")   \n","          LOGGER_FILE.info(\"HGS is optimizing  \\\"\"+objf.__name__+\"\\\"\")    \n","          s=solution()\n","          timerStart=time.time() \n","          s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","          lb = [lb] * dim\n","          ub = [ub] * dim\n","          # print(trainInput.shape)\n","\n","          best_pos1, best_fit1, list_loss1, fs_counts = HGSClass(objf, lb, ub, verbose, Max_iter, SearchAgents_no,0.08, 10000,trainInput=trainInput,trainOutput=trainOutput,dim=dim, logger = LOGGER_FILE).train()\n","          # print(md1)\n","          # best_pos1, best_fit1, list_loss1 = md1.train()\n","          \n","          timerEnd=time.time()  \n","          s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","          s.executionTime=timerEnd-timerStart\n","          s.bestIndividual=best_pos1\n","          s.convergence1=list_loss1\n","          s.convergence2=fs_counts\n","\n","\n","          s.optimizer=\"HGS\"\n","          s.objfname=objf.__name__\n","          \n","          \n","          return s"]},{"cell_type":"markdown","metadata":{"id":"15c1ckC1l_Bb"},"source":["## 10- DYHGS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CECPlm64mOqz"},"outputs":[],"source":["from numpy import where, clip, logical_and, exp, abs, ones, sum, array, mean\n","from numpy.random import uniform, normal, rand\n","from copy import deepcopy\n","from colorama import Fore, Back, Style\n","import time\n","from sklearn.preprocessing import Binarizer\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","from random import seed\n","import numpy\n","import random\n","\n","\n","class Root:\n","    \"\"\" This is root of all Algorithms \"\"\"\n","\n","    ID_MIN_PROB = 0  # min problem\n","    ID_MAX_PROB = -1  # max problem\n","\n","    ID_POS = 0  # Position\n","    ID_FIT = 1  # Fitness\n","\n","    EPSILON = 10E-10    # Avoid division by 0 \n","\n","    DEFAULT_LB = -1\n","    DEFAULT_UB = 1\n","\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True):\n","        self.verbose = verbose\n","        self.obj_func = obj_func\n","        if lb is None:\n","            print(\"Lower bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.lb = array(lb)\n","        if ub is None:\n","            print(\"Upper bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.ub = array(ub)\n","           \n","        if len(lb) != len(ub):\n","            print(\"Lower bound and Upper bound need to have the same length\")\n","            exit(0)\n"," \n","        \n","        self.problem_size = len(lb)\n","        self.solution, self.loss_train, self.fs_counts = None, [], []\n","\n","    def create_solution(self, minmax=0):\n","        \"\"\" Return solution with 2 element: position of solution and fitness of solution\n","        Parameters\n","        ----------\n","        minmax\n","            0 - minimum problem, else - maximum problem\n","        \"\"\"\n","  \n","\n","\n","         \n","        position = uniform(self.lb, self.ub)\n","        # print(position)\n","        # position=numpy.random.randint(2, size=(self.pop_size,self.dim))\n","        # for i in range(0,self.pop_size):\n","        #   while numpy.sum(position[i,:])==0:   \n","        #           position[i,:]=numpy.random.randint(2, size=(1,self.dim))\n","        fitness, binary_pos = self.get_fitness_position(position=position, minmax=minmax)\n","        return [position, fitness, binary_pos]\n","\n","    def create_solution_OBL(self,position=None, minmax=0):\n","      position1=deepcopy(position);\n","      position2=numpy.zeros((self.pop_size,self.dim));\n","      position2_binary=numpy.zeros((self.pop_size,self.dim));\n","\n","      pos=numpy.zeros((self.pop_size,self.dim));\n","      pos1=numpy.zeros((self.pop_size,self.dim));\n","      pos1_binary=numpy.zeros((self.pop_size,self.dim));\n","      pos_binary=numpy.zeros((self.pop_size,self.dim));\n","\n","#      print(pos[1])\n","\n","      double_population=numpy.zeros((2*self.pop_size,self.dim))\n","      double_population_binary=numpy.zeros((2*self.pop_size,self.dim))\n","\n","      double_fitness=numpy.zeros(2*self.pop_size)\n","      double_sorted_population=numpy.zeros((2*self.pop_size,self.dim))\n","      double_sorted_population_binary=numpy.zeros((2*self.pop_size,self.dim))\n","      \n","      double_fitness_sorted=numpy.zeros(2*self.pop_size)\n","      fitnesspos=numpy.zeros(self.pop_size)\n","      fitnesspos2=numpy.zeros(self.pop_size)\n","      fitness1=numpy.zeros(self.pop_size)\n","      LB=numpy.ones(self.dim)*self.lb\n","      UB=numpy.ones(self.dim)*self.ub\n"," #     print(rand())\n","\n","      for i in range(0,self.pop_size):\n","        position2[i]=position1[i][0]\n","        position2_binary[i]=position1[i][2]\n","        # pos[i]=position1[i][0]+rand()*(rand()*(UB-LB-position1[i][0])-position1[i][0])   \n","        gen = position1[i][0]+rand()*(rand()*(UB-LB-position1[i][0])-position1[i][0]) \n","        \n","        if max(gen) > self.ub[0] or min(gen) < self.lb[0]:\n","            gen = self.amend_position_random(gen)            \n","        pos[i]= gen\n","           \n","        fitnesspos[i], pos_binary[i] = self.get_fitness_position(position=pos[i], minmax=minmax)\n","        fitness1[i]=position1[i][1]\n","  #      print(fitness1[i])\n","      \n","      double_population=numpy.concatenate((position2,pos),axis=0)\n","      double_population_binary=numpy.concatenate((position2_binary,pos_binary),axis=0)\n","      double_fitness=numpy.concatenate((fitness1, fitnesspos),axis=0);\n","      double_fitness_sorted =numpy.sort(double_fitness);\n","      \n","      I2 =numpy.argsort(double_fitness);\n","      for newindex in range(0,2*self.pop_size):\n","          double_sorted_population[newindex,:]=numpy.array(double_population[I2[newindex],:])           \n","          double_sorted_population_binary[newindex,:]=numpy.array(double_population_binary[I2[newindex],:])           \n","        \n","      fitnesspos2=double_fitness_sorted[0:self.pop_size]\n","      \n","      pos1=double_sorted_population[0:self.pop_size]\n","      pos1_binary=double_sorted_population_binary[0:self.pop_size]\n","      \n","      out = list(zip(pos1, fitnesspos2, pos1_binary))\n","\n","      # for i in range(0,self.pop_size):\n","      #   position1[i][0]=pos1[i]\n","      #   position1[i][1]=fitnesspos2[i]\n","      return out\n","\n","    def get_binary_position(self,position=None, minmax=0):\n","      \"\"\"     Assumption that objective function always return the original value\n","      :param position: 1-D numpy array\n","      :param minmax: 0- min problem, 1 - max problem\n","      :return:\n","      \"\"\"\n","      # print(position)\n","      Positions=deepcopy(position);\n","        \n","      \n","      for j in range(0,self.dim):\n","        ss= transfer_functions_benchmark.s1(position[j])\n","        if (random.random()<ss): \n","            Positions[j]=1;\n","        else:\n","            Positions[j]=0;\n","            \n","\n","      return  Positions\n","  \n","\n","\n","\n","    def get_fitness_position(self,position=None, minmax=0):\n","        \"\"\"     Assumption that objective function always return the original value\n","        :param position: 1-D numpy array\n","        :param minmax: 0- min problem, 1 - max problem\n","        :return:\n","        \"\"\"\n","        # print(position)\n","        Positions=deepcopy(position);\n","         \n","        k=0\n","        for j in range(0,self.dim):\n","          ss= transfer_functions_benchmark.s1(position[j])\n","          # print(ss)\n","          if (random.random()<ss): \n","              Positions[j]=1;\n","          else:\n","              Positions[j]=0;\n","              k=k+1\n","          # print(Positions)    \n","          # if all(Positions)==0:\n","          #    Positions[j]=1;\n","\n","\n","        return self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) if minmax == 0 else 1.0 / (self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) + self.EPSILON), Positions     # Avoid division by 0\n","\n","    def get_fitness_solution(self,solution=None, minmax=0):\n","        return self.get_fitness_position(solution[self.ID_POS],self.trainInput,self.trainOutput,self.dim, minmax)\n","\n","    def get_global_best_global_worst_solution(self, pop=None, id_fit=None, id_best=None):\n","        sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n","        if id_best == self.ID_MIN_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MAX_PROB])\n","        elif id_best == self.ID_MAX_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MIN_PROB])\n","\n","    def update_global_best_global_worst_solution(self, pop=None, id_best=None, id_worst=None, g_best=None):\n","        \"\"\" Sort the copy of population and update the current best position. Return the new current best position \"\"\"\n","        sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n","        current_best = sorted_pop[id_best]\n","        g_best = deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n","        return g_best, sorted_pop[id_worst]\n","\n","\n","    def amend_position(self, position=None):\n","        return clip(position, self.lb, self.ub)\n","\n","    def amend_position_random(self, position=None):\n","        return where(logical_and(self.lb <= position, position <= self.ub), position, uniform(self.lb, self.ub))\n","\n","\n","\n","class DYHGSClass(Root):\n","\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True, epoch=1000, pop_size=100, L=0.08, LH=10000,trainInput=None,trainOutput=None,dim=None, logger=None):\n","        super().__init__(obj_func, lb, ub, verbose)\n","\n","        self.epoch = epoch\n","        self.pop_size = pop_size\n","        self.L = L\n","        self.LH = LH\n","        self.trainInput=trainInput\n","        self.dim=dim\n","        self.trainOutput=trainOutput\n","        self.logger = logger\n","        \n","\n","    def get_hunger_list(self, pop=None, hunger_list=array, g_best=None, g_worst=None):\n","        # min_index = pop.index(min(pop, key=lambda x: x[self.ID_FIT]))\n","        # Eq (2.8) and (2.9)\n","        for i in range(0, self.pop_size):\n","            r = rand()\n","            # space: since we pass lower bound and upper bound as list. Better take the mean of them.\n","            space = mean(self.ub - self.lb)\n","            H = (pop[i][self.ID_FIT] - g_best[self.ID_FIT]) / (g_worst[self.ID_FIT] - g_best[self.ID_FIT] + self.EPSILON) * r * 2 * space\n","            if H < self.LH:\n","                H = self.LH * (1 + r)\n","            hunger_list[i] += H\n","\n","            if g_best[self.ID_FIT] == pop[i][self.ID_FIT]:\n","                hunger_list[i] = 0\n","        return hunger_list\n","\n","    def sech(self, x):\n","        return 2 / (exp(x) + exp(-x))\n","\n","    def train(self):\n","        # Hungry value of all solutions\n","        hunger_list = ones(self.pop_size)\n","\n","        # Create population\n","        pop = [self.create_solution(self) for _ in range(self.pop_size)]\n","        # print(len(pop))\n","        pop = self.create_solution_OBL(pop)\n","        # print(len(pop))\n","        # print(self.ID_FIT)\n","        popbest=pop\n","\n","        ## Eq. (2.2)\n","        ### Find the current best and current worst\n","        g_best, g_worst = self.get_global_best_global_worst_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","\n","        hunger_list = self.get_hunger_list(pop, hunger_list, g_best, g_worst)\n","        vel=numpy.zeros((self.pop_size,self.dim));\n","\n","        # Loop\n","        for epoch in range(self.epoch):\n","\n","            ## Eq. (2.4)\n","            shrink = 2 * (1 - (epoch+1) / self.epoch)\n","\n","            for i in range(0, self.pop_size):\n","                if epoch==0:\n","                  popbest[i]=pop[i]\n","                else:\n","                  if popbest[i][self.ID_FIT]>pop[i][self.ID_FIT]:\n","                     popbest[i]=pop[i]\n","\n","                #### Variation control\n","                E = self.sech(pop[i][self.ID_FIT] - g_best[self.ID_FIT])\n","\n","                # R is a ranging controller added to limit the range of activity, in which the range of R is gradually reduced to 0\n","                R = 2 * shrink * rand() - shrink        # Eq. (2.3)\n","\n","                ## Calculate the hungry weight of each position\n","                if rand() < self.L:\n","                    W1 = hunger_list[i] * self.pop_size / (sum(hunger_list) + self.EPSILON) * rand()\n","                else:\n","                    W1 = 1\n","                W2 = (1 - exp(-abs(hunger_list[i] - sum(hunger_list)))) * rand() * 2\n","\n","                ### Udpate position of individual Eq. (2.1)\n","                r1 = rand()\n","                r2 = rand()\n","                if rand()>0.2:\n","                  # print('HGS')\n","                  if r1 < self.L:\n","                      pos_new = pop[i][self.ID_POS] * (1 + normal(0, 1))\n","                  else:\n","                      if r2 > E:\n","                          pos_new = W1 * g_best[self.ID_POS] + R * W2 * abs(g_best[self.ID_POS] - pop[i][self.ID_POS])\n","                      else:\n","                          pos_new = W1 * g_best[self.ID_POS] - R * W2 * abs(g_best[self.ID_POS] - pop[i][self.ID_POS])\n","                else:\n","                  r1=random.random()\n","                  r2=random.random()\n","                  wMax=0.4\n","                  wMin=0.2\n","                  w=wMax-epoch*((wMax-wMin)/self.epoch);\n","                  \n","                  # print('PSO')\n","                  c1=2\n","                  c2=1.7\n","                  vel[i]=w*vel[i]+c1*r1*(popbest[i][self.ID_POS]-pop[i][self.ID_POS])+c2*r2*(g_best[self.ID_POS]-pop[i][self.ID_POS])\n","                  pos_new=pop[i][self.ID_POS]+vel[i]\n","\n","                if max(pos_new) > self.ub[0] or min(pos_new) < self.lb[0]:\n","                    pos_new = self.amend_position_random(pos_new)\n","                fit_new, pos_bin = self.get_fitness_position(pos_new)\n","                pop[i] = [pos_new, fit_new, pos_bin]\n","\n","            ## Update global best and global worst\n","            g_best, g_worst = self.update_global_best_global_worst_solution(pop, self.ID_MIN_PROB, self.ID_MAX_PROB, g_best)\n","\n","            ## Update hunger list\n","            hunger_list = self.get_hunger_list(pop, hunger_list, g_best, g_worst)\n","            # print(len(g_best))\n","\n","            self.loss_train.append(g_best[self.ID_FIT])\n","            self.fs_counts.append(sum(g_best[2]))\n","            # if self.verbose:\n","            #     print(\"> Epoch: {}, Best fit: {}\".format(epoch + 1, g_best[self.ID_FIT]))\n","                \n","            if (epoch%1==0):\n","                \n","                print(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[2]))])\n","                self.logger.info(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[2]))])\n","          \n","        # print(self.ub[0])\n","        # print(g_best)\n","        # g_best[0] = g_best[2] #self.get_binary_position(g_best[0])\n","        \n","                    # fit_new = self.get_fitness_position(g_best)       \n","        self.solution = g_best\n","        return g_best[2], g_best[self.ID_FIT], self.loss_train, self.fs_counts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3JWRyLCOmDSL"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","# from .DYHGSClass import DYHGSClass\n","from numpy.random import uniform, normal, rand\n","\n","    \n","class dyhgs:\n","\n","      def DYHGS(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","    \n","            #Max_iter=1000\n","            #lb=-100\n","            #ub=100\n","            #dim=30  \n","            #SearchAgents_no=5\n","            lb=0\n","            # initialize alpha, beta, and delta_pos\n","            g_best=numpy.zeros(dim)\n","            Alpha_score=float(\"inf\")\n","            XXX=1\n","            verbose = True\n","            # Loop counter\n","            print(\"DYHGS is optimizing  \\\"\"+objf.__name__+\"\\\"\")   \n","            LOGGER_FILE.info(\"DYHGS is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","            s=solution()\n","            timerStart=time.time() \n","            s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            lb = [lb] * dim\n","            ub = [ub] * dim\n","            # print(trainInput.shape)\n","\n","            best_pos1, best_fit1, list_loss1, fs_counts = DYHGSClass(objf, lb, ub, verbose, Max_iter, SearchAgents_no,0.08, 10000,trainInput=trainInput,trainOutput=trainOutput,dim=dim, logger = LOGGER_FILE).train()\n","            # print(md1)\n","            # best_pos1, best_fit1, list_loss1 = md1.train()\n","            \n","            timerEnd=time.time()  \n","            s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            s.executionTime=timerEnd-timerStart\n","            s.bestIndividual=best_pos1\n","            s.convergence1=list_loss1\n","            s.convergence2=fs_counts\n","            \n","\n","            s.optimizer=\"DYHGS\"\n","            s.objfname=objf.__name__\n","            \n","            \n","            \n","            \n","            return s"]},{"cell_type":"markdown","metadata":{"id":"I5oOitUyXzl-"},"source":["## 11- CGO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PIOAsxAQX3AI"},"outputs":[],"source":["\n","import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","\n","import numpy as np\n","from statistics import *\n","import random\n","from random import randint\n","from numpy.random import rand, randint\n","\n","\n","def bound(x, UB, LB):\n","    x[x > UB]= UB[x > UB]\n","    x[x < LB]=LB[x < LB]\n","    return x\n","\n","def Sphere(xx):\n","    sum=0\n","    for i in range(0,len(xx)):\n","      sum = sum + xx[i]**2\n","    return sum\n","\n","\n","class cgo:\n","      def CGO(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","\n","          Var_Number= dim\n","          Seed_Number= SearchAgents_no  #50\n","\n","          ObjFuncName=objf\n","          MaxIter=Max_iter\n","          \n","          LB = lb * np.ones((1, Var_Number))# Lower bound of variable ;\n","          UB = ub * np.ones((1, Var_Number))# Upper bound of variable ;\n","\n","          Fun_eval=[]\n","          Seed_list=[]\n","\n","          #////////////////////////////////////////////////////////////////\n","\n","          Seed=numpy.random.randint(2, size=(SearchAgents_no,dim))\n","          \n","          for i in range(0,Seed_Number):   \n","\n","              # the following statement insures that at least one feature is selected\n","              #(i.e the randomly generated individual has at least one value 1)\n","              \n","              while numpy.sum(Seed[i,:])==0:   \n","                  Seed[i,:]=numpy.random.randint(2, size=(1,dim))\n","              \n","              # Calculate objective function for each search agent\n","              Fun_eval.append(objf(Seed[i,:],trainInput,trainOutput,dim));         \n","\n","          ####################################\n","          sol=solution()\n","\n","          print(\"CGO is optimizing  \\\"\"+objf.__name__+\"\\\"\")    \n","          LOGGER_FILE.info(\"CGO is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","\n","          timerStart=time.time() \n","          sol.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","          ############################\n","\n","          Seed_list=Seed\n","\n","          Conv_History=[MaxIter]\n","          FeatCount=[MaxIter]\n","          \n","          for Iter in range(0,MaxIter): #100\n","              for i in range(0,Seed_Number): #25\n","                      # Update the best Seed\n","                      index=Fun_eval.index(min(Fun_eval))\n","                      BestSeed = Seed_list[index]\n","\n","              #% Generate New Solutions\n","              # Random Numbers\n","                      I=np.random.randint(1, 3, 12) # Beta and Gamma\n","                      Ir=np.random.randint(0, 2, 5)\n","\n","                      RandGroupNumber =  randint(1, Seed_Number)# randperm(Seed_Number, 1)\n","                      RandGroup = np.random.randint(1, Seed_Number, RandGroupNumber) #randperm(Seed_Number, RandGroupNumber)\n","\n","\n","                      # Mean of Random Group\n","                      s=[]\n","                      m=[]\n","                      for i in range(len(RandGroup)):\n","                          idx=RandGroup[i]\n","                          s.append(Seed_list[idx])\n","                      MeanGroup = np.multiply(np.mean(s, axis=0),(int(len(RandGroup) != 1))) + s[0] * (int(len(RandGroup) == 1)) # what's .*? element-wise product of two arrays\n","                      # New Seed\n","                      Alfa=[]\n","                      Alfa.append(rand(1, Var_Number))\n","                      Alfa.append(2*rand(1, Var_Number)-1)\n","                      Alfa.append(Ir[0]*rand(1, Var_Number)+1)\n","                      Alfa.append(Ir[1]*rand(1, Var_Number)+(not Ir[1]))\n","                      Alfa=np.reshape(Alfa, [4,Var_Number]) # 10 is length of LB\n","                      # print(np.shape(Alfa)) #(4, 10)\n","\n","                      SelectedAlfa=[]\n","                      ii=randint(1, 5, 3) \n","\n","                      for i in range(len(ii)):\n","                        SelectedAlfa.append(Alfa[i])        \n","                    \n","\n","                      NewSeed=[]\n","\n","                      NewSeed.append(Seed_list[i]+np.multiply(SelectedAlfa[0],(I[0]*BestSeed-I[1]*MeanGroup)))\n","                      NewSeed.append(BestSeed+np.multiply(SelectedAlfa[1],(I[2]*MeanGroup-I[3]*Seed_list[i])))\n","                      NewSeed.append(MeanGroup+np.multiply(SelectedAlfa[2],(I[4]*BestSeed-I[5]*Seed_list[i])))\n","                      NewSeed.append(np.random.uniform(LB,UB))\n","\n","                      NewSeed1=[]\n","                      NewSeed1.append(NewSeed[0])\n","                      NewSeed1.append(NewSeed[1])\n","                      NewSeed1.append(NewSeed[2])\n","                      NewSeed1.append(NewSeed[3][0])\n","\n","                      NewSeed=[]\n","                      for k in range(len(NewSeed1)):\n","                            NewSeed.append(NewSeed1[k].tolist())\n","\n","                      # print(np.shape(NewSeed)) # (4, 10)\n","                      NewSeed=np.reshape(NewSeed, [4,1,Var_Number]) \n","                      NewSeed=np.reshape(NewSeed, [4,Var_Number])\n","\n","                      Fun_evalNew=[]\n","                      x_list=[]\n","                      \n","                      # print(np.shape(NewSeed)) // (4, 128)\n","\n","                      #//////////////////////////////////////////////////////\n","                      for j in range(len(NewSeed)):\n","                          for i in range(len(NewSeed[0])):\n","                            \n","                              if NewSeed[j][i]>0.0:\n","                                    NewSeed[j][i]=1\n","                              else:\n","                                    NewSeed[j][i]=0\n","\n","                              # Fun_evalNew.append(objf(NewSeed[i,:],trainInput,trainOutput,dim));\n","                          Fun_evalNew.append(objf(NewSeed[j,:],trainInput,trainOutput,dim));           \n","                      \n","\n","                      # NewSeed=np.reshape(NewSeed, [4,Var_Number])\n","                      # print(NewSeed)\n","\n","\n","                      x_list = np.concatenate((Seed_list, NewSeed)) #29*10\n","                      Seed_list=x_list\n","\n","                      x_eval=np.concatenate((Fun_eval, Fun_evalNew)) #29\n","                      Fun_eval=x_eval.tolist()\n","\n","                      \n","          SortOrder=[i[0] for i in sorted(enumerate(x_eval), key=lambda x:x[1])]\n","          Fun_eval.sort()\n","          # print(Fun_eval)\n","          Sort_x_list=[]\n","          for index in range(len(SortOrder)):\n","              Sort_x_list.append(x_list[SortOrder[index]])\n","\n","          Seed_list=Sort_x_list\n","\n","          BestFitness=min(Fun_eval)\n","          idbest = Fun_eval.index(BestFitness)\n","          BestSeed=Seed_list[idbest]\n","\n","          Seed_list[0:Seed_Number]\n","          Fun_eval[0:Seed_Number]\n","\n","          #///////////////////////////////////////////////////\n","                \n","          featurecount=0\n","          for f in range(0,Var_Number):\n","            if BestSeed[f]==1:\n","              featurecount=featurecount+1             \n","          \n","          Conv_History.append(BestFitness)\n","          FeatCount.append(featurecount)\n","\n","          #///////////////////////////////////////////////////  \n","\n","          ############################################\n","          timerEnd=time.time()  \n","          sol.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","          sol.executionTime=timerEnd-timerStart\n","          \n","          print(Conv_History[0])\n","          print(BestFitness)\n","\n","          sol.bestIndividual=BestSeed#Leader_pos = [0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0]\n","\n","          sol.convergence1=Conv_History#convergence_curve1              \n","          print(np.shape(Conv_History)) #(10,)\n","          print(Conv_History) \n","\n","          sol.convergence2=FeatCount#convergence_curve2\n","          print(np.shape(FeatCount)) #(10,)\n","          print(FeatCount) #[62. 62. 62. 62. 48. 58. 58. 58. 58. 50.]\n","\n","          sol.optimizer=\"CGO\"\n","          sol.objfname=objf.__name__\n","\n","          \n","          ############################################\n","\n","          return sol #Conv_History\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_7VCEZ1iVwE8"},"source":["## 12- AOA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BYChBl7DVvZ8"},"outputs":[],"source":["class Root:\n","    \"\"\" This is root of all Algorithms \"\"\"\n","\n","    ID_MIN_PROB = 0  # min problem\n","    ID_MAX_PROB = -1  # max problem\n","\n","    ID_POS = 0  # Position\n","    ID_FIT = 1  # Fitness\n","\n","    EPSILON = 10E-10    # Avoid division by 0 \n","\n","    DEFAULT_LB = -1\n","    DEFAULT_UB = 1\n","\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True):\n","        self.verbose = verbose\n","        self.obj_func = obj_func\n","        if lb is None:\n","            print(\"Lower bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.lb = array(lb)\n","        if ub is None:\n","            print(\"Upper bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.ub = array(ub)\n","           \n","        if len(lb) != len(ub):\n","            print(\"Lower bound and Upper bound need to have the same length\")\n","            exit(0)\n","        \n","        self.problem_size = len(lb)\n","        self.solution, self.loss_train, self.fs_counts = None, [], []\n","\n","    def create_solution(self, minmax=0):\n","        \"\"\" Return solution with 2 element: position of solution and fitness of solution\n","        Parameters\n","        ----------\n","        minmax\n","            0 - minimum problem, else - maximum problem\n","        \"\"\"\n","        position = uniform(self.lb, self.ub)\n","        # print(position)\n","        # position=numpy.random.randint(2, size=(self.pop_size,self.dim))\n","        # for i in range(0,self.pop_size):\n","        #   while numpy.sum(position[i,:])==0:   \n","        #           position[i,:]=numpy.random.randint(2, size=(1,self.dim))\n","        fitness, binary_pos = self.get_fitness_position(position=position, minmax=minmax)\n","        return [position, fitness, binary_pos]\n","\n","    def get_fitness_position(self,position=None, minmax=0):\n","        \"\"\"     Assumption that objective function always return the original value\n","        :param position: 1-D numpy array\n","        :param minmax: 0- min problem, 1 - max problem\n","        :return:\n","        \"\"\"\n","        # print(position)\n","        Positions=position;\n","         \n","        \n","        for j in range(0,self.dim):\n","          ss= transfer_functions_benchmark.s1(position[j])\n","          if (random.random()<ss): \n","              Positions[j]=1;\n","          else:\n","              Positions[j]=0;\n","\n","        return self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) if minmax == 0 else 1.0 / (self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) + self.EPSILON), Positions     # Avoid division by 0\n","\n","    def get_fitness_solution(self,solution=None, minmax=0):\n","        return self.get_fitness_position(solution[self.ID_POS],self.trainInput,self.trainOutput,self.dim, minmax)\n","\n","    def get_global_best_global_worst_solution(self, pop=None, id_fit=None, id_best=None):\n","        sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n","        if id_best == self.ID_MIN_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MAX_PROB])\n","        elif id_best == self.ID_MAX_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MIN_PROB])\n","\n","\n","    def update_global_best_global_worst_solution(self, pop=None, id_best=None, id_worst=None, g_best=None):\n","        \"\"\" Sort the copy of population and update the current best position. Return the new current best position \"\"\"\n","        sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n","        current_best = sorted_pop[id_best]\n","        g_best = deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n","        return g_best, sorted_pop[id_worst]\n","\n","\n","    def amend_position(self, position=None):\n","        return clip(position, self.lb, self.ub)\n","\n","    def amend_position_random(self, position=None):\n","        return where(logical_and(self.lb <= position, position <= self.ub), position, uniform(self.lb, self.ub))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2RV2ORCV8T9"},"outputs":[],"source":["class AOAClass(Root):\n","\n","    def __init__(self, obj_func=None, lb=None, ub=None, verbose=True, epoch=750, pop_size=100,alpha=5, miu=0.5, moa_min=0.2, moa_max=0.9,trainInput=None,trainOutput=None,dim=None, logger=None):\n","        # super().__init__({\"obj_func\":obj_func, \"lb\":lb, \"ub\":ub, \"verbose\":verbose})#, \"trainInput\"=trainInput,\"trainOutput\"=trainOutput,\"dim\"=dim})\n","        super().__init__(obj_func,lb, ub, verbose)\n","        # print(trainInput.shape)\n","        self.epoch = epoch\n","        self.pop_size = pop_size\n","        self.alpha = alpha          # Default: 5, fixed parameter, sensitive exploitation parameter\n","        self.miu = miu              # Default: 0.5, fixed parameter , control parameter to adjust the search process\n","        self.moa_min = moa_min      # Default: 0.2, range min of Math Optimizer Accelerated\n","        self.moa_max = moa_max      # Default: 0.9, range max of Math Optimizer Accelerated\n","\n","        self.trainInput=trainInput\n","        self.dim=dim\n","        self.trainOutput=trainOutput\n","        self.logger = logger\n","               \n","\n","    # def get_hunger_list(self, pop=None, hunger_list=array, g_best=None, g_worst=None):\n","    #     # min_index = pop.index(min(pop, key=lambda x: x[self.ID_FIT]))\n","    #     # Eq (2.8) and (2.9)\n","    #     for i in range(0, self.pop_size):\n","    #         r = rand()\n","    #         # space: since we pass lower bound and upper bound as list. Better take the mean of them.\n","    #         space = mean(self.ub - self.lb)\n","    #         H = (pop[i][self.ID_FIT] - g_best[self.ID_FIT]) / (g_worst[self.ID_FIT] - g_best[self.ID_FIT] + self.EPSILON) * r * 2 * space\n","    #         if H < self.LH:\n","    #             H = self.LH * (1 + r)\n","    #         hunger_list[i] += H\n","\n","    #         if g_best[self.ID_FIT] == pop[i][self.ID_FIT]:\n","    #             hunger_list[i] = 0\n","    #     return hunger_list\n","\n","    # def sech(self, x):\n","    #     return 2 / (exp(x) + exp(-x))\n","\n","    def train(self):\n","        pop = [self.create_solution() for _ in range(self.pop_size)]\n","        pop=np.asarray(pop)\n","\n","        # print(pop.shape)\n","        # print(self.ID_FIT) #1\n","        # print(self.ID_MIN_PROB) #0\n","\n","        # pop.tolist()\n","        # print(pop)\n","        # g_best = self.get_global_best_solution(pop)#, self.ID_FIT, self.ID_MIN_PROB)\n","        g_best, g_worst = self.get_global_best_global_worst_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","\n","        g_best=np.asarray(g_best)\n","\n","\n","        for epoch in range(self.epoch):\n","            moa = self.moa_min + epoch * ((self.moa_max - self.moa_min) / self.epoch)           # Eq. 2\n","            mop = 1 - (epoch ** (1.0 / self.alpha)) / (self.epoch ** (1.0 / self.alpha))        # Eq. 4\n","\n","            for i in range(0, self.pop_size):\n","                pos_new = pop[i][self.ID_POS]\n","                for j in range(0, self.problem_size):\n","                    r1, r2, r3 = rand(3)\n","                    if r1 > moa:        # Exploration phase\n","                        # print(g_best[self.ID_POS][j])\n","                        # print(mop + self.EPSILON)\n","                        # print(self.ub[j] - self.lb[j])\n","                        # print(self.miu + self.lb[j])\n","                        \n","                        if r2 < 0.5:\n","                            \n","                            pos_new[j] = g_best[self.ID_POS][j] / (mop + self.EPSILON) * ((self.ub[j] - self.lb[j]) * self.miu + self.lb[j])\n","                        else:\n","                            pos_new[j] = g_best[self.ID_POS][j] * mop  * ((self.ub[j] - self.lb[j]) * self.miu + self.lb[j])\n","                    \n","                    else:               # Exploitation phase\n","                        if r3 < 0.5:\n","                            pos_new[j] = g_best[self.ID_POS][j] - mop * ((self.ub[j] - self.lb[j]) * self.miu + self.lb[j])\n","                        else:\n","                            pos_new[j] = g_best[self.ID_POS][j] + mop * ((self.ub[j] - self.lb[j]) * self.miu + self.lb[j])\n","                \n","                fit_new, pos_new_binary = self.get_fitness_position(pos_new)\n","                pop[i] = [pos_new, fit_new, pos_new_binary]\n","\n","            ## Update global best and global worst\n","            g_best, g_worst = self.update_global_best_global_worst_solution(pop, self.ID_MIN_PROB, self.ID_MAX_PROB, g_best)\n","\n","            # g_best = self.update_global_best_solution(pop, self.ID_MIN_PROB, g_best)\n","            self.loss_train.append(g_best[self.ID_FIT])\n","        #     if self.verbose:\n","        #         print(\"> Epoch: {}, Best fit: {}\".format(epoch + 1, g_best[self.ID_FIT]))\n","        # self.solution = g_best\n","        # return g_best[self.ID_POS], g_best[self.ID_FIT], self.loss_train\n","\n","            self.fs_counts.append(sum(g_best[2]))\n","            # if self.verbose:\n","            #     print(\"> Epoch: {}, Best fit: {}\".format(epoch + 1, g_best[self.ID_FIT]))\n","                \n","            if (epoch%1==0):\n","                \n","                print(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[2]))])\n","                self.logger.info(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[2]))])\n","          \n","        # print(self.ub[0])\n","        # print(g_best)\n","        # g_best[0] = g_best[2] #self.get_binary_position(g_best[0])\n","        \n","                    # fit_new = self.get_fitness_position(g_best)       \n","        self.solution = g_best\n","        return g_best[2], g_best[self.ID_FIT], self.loss_train, self.fs_counts\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Te1J0GUV_W3"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","# from .DYHGSClass import DYHGSClass\n","from numpy.random import uniform, normal, rand\n","\n","    \n","class aoa:\n","\n","      def AOA(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","    \n","            #Max_iter=1000\n","            #lb=-100\n","            #ub=100\n","            #dim=30  \n","            #SearchAgents_no=5\n","            lb=0\n","            # initialize alpha, beta, and delta_pos\n","            g_best=numpy.zeros(dim)\n","            Alpha_score=float(\"inf\")\n","            XXX=1\n","            verbose = True\n","            # Loop counter\n","            print(\"AOA is optimizing  \\\"\"+objf.__name__+\"\\\"\")   \n","            LOGGER_FILE.info(\"AOA is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","            s=solution()\n","            timerStart=time.time() \n","            s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            lb = [lb] * dim\n","            ub = [ub] * dim\n","            # print(trainInput.shape)\n","\n","            best_pos1, best_fit1, list_loss1, fs_counts = AOAClass(objf, lb, ub, verbose, Max_iter, SearchAgents_no,0.08, 10000,trainInput=trainInput,trainOutput=trainOutput,dim=dim, logger = LOGGER_FILE).train()\n","            # print(md1)\n","            # best_pos1, best_fit1, list_loss1 = md1.train()\n","            \n","            timerEnd=time.time()  \n","            s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            s.executionTime=timerEnd-timerStart\n","            s.bestIndividual=best_pos1\n","            s.convergence1=list_loss1\n","            s.convergence2=fs_counts\n","            \n","\n","            s.optimizer=\"AOA\"\n","            s.objfname=objf.__name__\n","            \n","            \n","            \n","            \n","            return s"]},{"cell_type":"markdown","metadata":{"id":"fZjcj-gbWmTJ"},"source":["## 12- LVAOA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l138MHu0WmTW"},"outputs":[],"source":["class Root:\n","    \"\"\" This is root of all Algorithms \"\"\"\n","\n","    ID_MIN_PROB = 0  # min problem\n","    ID_MAX_PROB = -1  # max problem\n","\n","    ID_POS = 0  # Position\n","    ID_FIT = 1  # Fitness\n","\n","    EPSILON = 10E-10    # Avoid division by 0 \n","\n","    DEFAULT_LB = -1\n","    DEFAULT_UB = 1\n","\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True):\n","        self.verbose = verbose\n","        self.obj_func = obj_func\n","        if lb is None:\n","            print(\"Lower bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.lb = array(lb)\n","        if ub is None:\n","            print(\"Upper bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.ub = array(ub)\n","           \n","        if len(lb) != len(ub):\n","            print(\"Lower bound and Upper bound need to have the same length\")\n","            exit(0)\n","        \n","        self.problem_size = len(lb)\n","        self.solution, self.loss_train, self.fs_counts = None, [], []\n","\n","    def create_solution(self, minmax=0):\n","        \"\"\" Return solution with 2 element: position of solution and fitness of solution\n","        Parameters\n","        ----------\n","        minmax\n","            0 - minimum problem, else - maximum problem\n","        \"\"\"\n","        position = uniform(self.lb, self.ub)\n","        # print(position)\n","        # position=numpy.random.randint(2, size=(self.pop_size,self.dim))\n","        # for i in range(0,self.pop_size):\n","        #   while numpy.sum(position[i,:])==0:   \n","        #           position[i,:]=numpy.random.randint(2, size=(1,self.dim))\n","        fitness, binary_pos = self.get_fitness_position(position=position, minmax=minmax)\n","        return [position, fitness, binary_pos]\n","\n","    def create_solution_OBL(self,position=None, minmax=0):\n","          position1=deepcopy(position);\n","          position2=numpy.zeros((self.pop_size,self.dim));\n","          position2_binary=numpy.zeros((self.pop_size,self.dim));\n","\n","          pos=numpy.zeros((self.pop_size,self.dim));\n","          pos1=numpy.zeros((self.pop_size,self.dim));\n","          pos1_binary=numpy.zeros((self.pop_size,self.dim));\n","          pos_binary=numpy.zeros((self.pop_size,self.dim));\n","\n","    #      print(pos[1])\n","\n","          double_population=numpy.zeros((2*self.pop_size,self.dim))\n","          double_population_binary=numpy.zeros((2*self.pop_size,self.dim))\n","\n","          double_fitness=numpy.zeros(2*self.pop_size)\n","          double_sorted_population=numpy.zeros((2*self.pop_size,self.dim))\n","          double_sorted_population_binary=numpy.zeros((2*self.pop_size,self.dim))\n","          \n","          double_fitness_sorted=numpy.zeros(2*self.pop_size)\n","          fitnesspos=numpy.zeros(self.pop_size)\n","          fitnesspos2=numpy.zeros(self.pop_size)\n","          fitness1=numpy.zeros(self.pop_size)\n","          LB=numpy.ones(self.dim)*self.lb\n","          UB=numpy.ones(self.dim)*self.ub\n","    #     print(rand())\n","\n","          for i in range(0,self.pop_size):\n","            position2[i]=position1[i][0]\n","            position2_binary[i]=position1[i][2]\n","            # pos[i]=position1[i][0]+rand()*(rand()*(UB-LB-position1[i][0])-position1[i][0])   \n","            gen = position1[i][0]+rand()*(rand()*(UB-LB-position1[i][0])-position1[i][0]) \n","            \n","            if max(gen) > self.ub[0] or min(gen) < self.lb[0]:\n","                gen = self.amend_position_random(gen)            \n","            pos[i]= gen\n","              \n","            fitnesspos[i], pos_binary[i] = self.get_fitness_position(position=pos[i], minmax=minmax)\n","            fitness1[i]=position1[i][1]\n","      #      print(fitness1[i])\n","          \n","          double_population=numpy.concatenate((position2,pos),axis=0)\n","          double_population_binary=numpy.concatenate((position2_binary,pos_binary),axis=0)\n","          double_fitness=numpy.concatenate((fitness1, fitnesspos),axis=0);\n","          double_fitness_sorted =numpy.sort(double_fitness);\n","          \n","          I2 =numpy.argsort(double_fitness);\n","          for newindex in range(0,2*self.pop_size):\n","              double_sorted_population[newindex,:]=numpy.array(double_population[I2[newindex],:])           \n","              double_sorted_population_binary[newindex,:]=numpy.array(double_population_binary[I2[newindex],:])           \n","            \n","          fitnesspos2=double_fitness_sorted[0:self.pop_size]\n","          \n","          pos1=double_sorted_population[0:self.pop_size]\n","          pos1_binary=double_sorted_population_binary[0:self.pop_size]\n","          \n","          out = list(zip(pos1, fitnesspos2, pos1_binary))\n","\n","          # for i in range(0,self.pop_size):\n","          #   position1[i][0]=pos1[i]\n","          #   position1[i][1]=fitnesspos2[i]\n","          return out\n","\n","    def get_fitness_position(self,position=None, minmax=0):\n","        \"\"\"     Assumption that objective function always return the original value\n","        :param position: 1-D numpy array\n","        :param minmax: 0- min problem, 1 - max problem\n","        :return:\n","        \"\"\"\n","        # print(position)\n","        Positions=position;\n","         \n","        \n","        for j in range(0,self.dim):\n","          ss= transfer_functions_benchmark.s1(position[j])\n","          if (random.random()<ss): \n","              Positions[j]=1;\n","          else:\n","              Positions[j]=0;\n","\n","        return self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) if minmax == 0 else 1.0 / (self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) + self.EPSILON), Positions     # Avoid division by 0\n","\n","    def get_fitness_solution(self,solution=None, minmax=0):\n","        return self.get_fitness_position(solution[self.ID_POS],self.trainInput,self.trainOutput,self.dim, minmax)\n","\n","    def get_global_best_global_worst_solution(self, pop=None, id_fit=None, id_best=None):\n","        sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n","        if id_best == self.ID_MIN_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MAX_PROB])\n","        elif id_best == self.ID_MAX_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MIN_PROB])\n","\n","\n","    def update_global_best_global_worst_solution(self, pop=None, id_best=None, id_worst=None, g_best=None):\n","        \"\"\" Sort the copy of population and update the current best position. Return the new current best position \"\"\"\n","        sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n","        current_best = sorted_pop[id_best]\n","        g_best = deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n","        return g_best, sorted_pop[id_worst]\n","\n","\n","    def amend_position(self, position=None):\n","        return clip(position, self.lb, self.ub)\n","\n","    def amend_position_random(self, position=None):\n","        return where(logical_and(self.lb <= position, position <= self.ub), position, uniform(self.lb, self.ub))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fs-Pwzl2WmTX"},"outputs":[],"source":["class LVAOAClass(Root):\n","\n","    # def __init__(self, obj_func=None, lb=None, ub=None, verbose=True, epoch=750, pop_size=100,alpha=5, miu=0.5, moa_min=0.2, moa_max=0.9,trainInput=None,trainOutput=None,dim=None, logger=None):\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True, epoch=1000, pop_size=100, L=0.08, LH=10000,alpha=5, miu=0.5, moa_min=0.2, moa_max=0.9,trainInput=None,trainOutput=None,dim=None, logger=None):\n","        # super().__init__({\"obj_func\":obj_func, \"lb\":lb, \"ub\":ub, \"verbose\":verbose})#, \"trainInput\"=trainInput,\"trainOutput\"=trainOutput,\"dim\"=dim})\n","        super().__init__(obj_func,lb, ub, verbose)\n","        # print(trainInput.shape)\n","        self.epoch = epoch\n","        self.pop_size = pop_size\n","        self.L = L\n","        self.LH = LH\n","        self.alpha = alpha          # Default: 5, fixed parameter, sensitive exploitation parameter\n","        self.miu = miu              # Default: 0.5, fixed parameter , control parameter to adjust the search process\n","        self.moa_min = moa_min      # Default: 0.2, range min of Math Optimizer Accelerated\n","        self.moa_max = moa_max      # Default: 0.9, range max of Math Optimizer Accelerated\n","\n","        self.trainInput=trainInput\n","        self.dim=dim\n","        self.trainOutput=trainOutput\n","        self.logger = logger\n","               \n","\n","    def get_hunger_list(self, pop=None, hunger_list=array, g_best=None, g_worst=None):\n","        # min_index = pop.index(min(pop, key=lambda x: x[self.ID_FIT]))\n","        # Eq (2.8) and (2.9)\n","        for i in range(0, self.pop_size):\n","            r = rand()\n","            # space: since we pass lower bound and upper bound as list. Better take the mean of them.\n","            space = mean(self.ub - self.lb)\n","            H = (pop[i][self.ID_FIT] - g_best[self.ID_FIT]) / (g_worst[self.ID_FIT] - g_best[self.ID_FIT] + self.EPSILON) * r * 2 * space\n","            if H < self.LH:\n","                H = self.LH * (1 + r)\n","            hunger_list[i] += H\n","\n","            if g_best[self.ID_FIT] == pop[i][self.ID_FIT]:\n","                hunger_list[i] = 0\n","        return hunger_list\n","\n","    def sech(self, x):\n","        return 2 / (exp(x) + exp(-x))\n","\n","    def get_simple_levy_step(self):\n","        beta = 1.5\n","        sigma = (gamma(1 + beta) * sin(pi * beta / 2) / (gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n","        u = normal(0, 1, self.problem_size) * sigma\n","        v = normal(1, self.problem_size)\n","        step = u / abs(v) ** (1 / beta)\n","        return step\n","\n","    def train(self):\n","        pop = [self.create_solution() for _ in range(self.pop_size)]\n","        # pop=np.asarray(pop)\n"," \n","        # new\n","        # pop = self.create_solution_OBL(pop)\n"," \n","        # print(pop.shape)\n","        # print(self.ID_FIT) #1\n","        # print(self.ID_MIN_PROB) #0\n","\n","        # pop.tolist()\n","        # print(pop)\n","        # g_best = self.get_global_best_solution(pop)#, self.ID_FIT, self.ID_MIN_PROB)\n","        g_best, g_worst = self.get_global_best_global_worst_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","\n","        g_best=np.asarray(g_best)\n","        \n","        # new\n","        # Hungry value of all solutions\n","        hunger_list = ones(self.pop_size)\n","\n","        hunger_list = self.get_hunger_list(pop, hunger_list, g_best, g_worst)\n","\n","        for epoch in range(self.epoch):\n","            moa = self.moa_min + epoch * ((self.moa_max - self.moa_min) / self.epoch)           # Eq. 2\n","            mop = 1 - (epoch ** (1.0 / self.alpha)) / (self.epoch ** (1.0 / self.alpha))        # Eq. 4            \n","            # new\n","            ## Eq. (2.4)\n","            shrink = 2 * (1 - (epoch+1) / self.epoch)\n","            \n","            for i in range(0, self.pop_size):\n","                \n","                # new________________________________________________________________________\n","                #### Variation control\n","                E = self.sech(pop[i][self.ID_FIT] - g_best[self.ID_FIT])\n","\n","                # R is a ranging controller added to limit the range of activity, in which the range of R is gradually reduced to 0\n","                R = 2 * shrink * rand() - shrink        # Eq. (2.3)\n","\n","                ## Calculate the hungry weight of each position\n","                if rand() < self.L:\n","                    W1 = hunger_list[i] * self.pop_size / (sum(hunger_list) + self.EPSILON) * rand()\n","                else:\n","                    W1 = 1\n","                W2 = (1 - exp(-abs(hunger_list[i] - sum(hunger_list)))) * rand() * 2\n","                #____________________________________________________________________________________\n","                \n","                pos_new = pop[i][self.ID_POS]\n","                for j in range(0, self.problem_size):\n","                    r1, r2, r3 = rand(3)        \n","                    \n","                    ### Udpate position of individual Eq. (2.1)\n","                    \n","                    # if r1 < self.L:\n","                    #     pos_new = pop[i][self.ID_POS] * (1 + normal(0, 1))\n","                    # else:\n","                    #     if r2 > E:\n","                    #         pos_new = W1 * g_best[self.ID_POS] + R * W2 * abs(g_best[self.ID_POS] - pop[i][self.ID_POS])\n","                    #     else:\n","                    #         pos_new = W1 * g_best[self.ID_POS] - R * W2 * abs(g_best[self.ID_POS] - pop[i][self.ID_POS])\n","                    # fit_new, pos_new_binary = self.get_fitness_position(pos_new)\n","                    # pop[i] = [pos_new, fit_new, pos_new_binary]\n","\n","                    if r1 > moa:        # Exploration phase\n","                        if r2 < 0.5:\n","                            pos_new[j] = g_best[self.ID_POS][j] / (mop + self.EPSILON) * ((self.ub[j] - self.lb[j]) * self.miu + self.lb[j])\n","                            # new\n","                            # pos_new = pop[i][self.ID_POS] / (1 + normal(0, 1))\n","                            # pos_new = (W1 * g_best[self.ID_POS]) / (R * W2 * abs(g_best[self.ID_POS]) - pop[i][self.ID_POS])\n","                        else:\n","                            pos_new[j] = g_best[self.ID_POS][j] * mop  * ((self.ub[j] - self.lb[j]) * self.miu + self.lb[j])\n","                            # new\n","                            # pos_new = pop[i][self.ID_POS] * (1 + normal(0, 1))\n","                            # pos_new = (W1 * g_best[self.ID_POS]) * (R * W2 * abs(g_best[self.ID_POS]) - pop[i][self.ID_POS])\n","                    \n","                    else:               # Exploitation phase\n","                        if r3 < 0.5:\n","                            # pos_new[j] = g_best[self.ID_POS][j] - mop * ((self.ub[j] - self.lb[j]) * self.miu + self.lb[j])\n","                            # new\n","                            pos_new = W1 * g_best[self.ID_POS] + R * W2 * abs(g_best[self.ID_POS] - pop[i][self.ID_POS])\n","                        else:\n","                            # pos_new[j] = g_best[self.ID_POS][j] + mop * ((self.ub[j] - self.lb[j]) * self.miu + self.lb[j])\n","                            # new\n","                            pos_new = W1 * g_best[self.ID_POS] - R * W2 * abs(g_best[self.ID_POS] - pop[i][self.ID_POS])\n","\n","                fit_new, pos_new_binary = self.get_fitness_position(pos_new)\n","                \n","                # new\n","                # idx = choice(list(set(range(0, self.pop_size)) - {i}))\n","                # pos_new = g_best[self.ID_POS] * self.get_simple_levy_step() + pop[idx][self.ID_POS]          # Eq. 5\n","\n","                pop[i] = [pos_new, fit_new, pos_new_binary]\n","\n","            ## Update global best and global worst\n","            g_best, g_worst = self.update_global_best_global_worst_solution(pop, self.ID_MIN_PROB, self.ID_MAX_PROB, g_best)\n","\n","            # new\n","            ## Update hunger list\n","            hunger_list = self.get_hunger_list(pop, hunger_list, g_best, g_worst)\n","\n","            # g_best = self.update_global_best_solution(pop, self.ID_MIN_PROB, g_best)\n","            self.loss_train.append(g_best[self.ID_FIT])\n","        #     if self.verbose:\n","        #         print(\"> Epoch: {}, Best fit: {}\".format(epoch + 1, g_best[self.ID_FIT]))\n","        # self.solution = g_best\n","        # return g_best[self.ID_POS], g_best[self.ID_FIT], self.loss_train\n","\n","            self.fs_counts.append(sum(g_best[2]))\n","            # if self.verbose:\n","            #     print(\"> Epoch: {}, Best fit: {}\".format(epoch + 1, g_best[self.ID_FIT]))\n","                \n","            if (epoch%1==0):\n","                \n","                print(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[2]))])\n","                self.logger.info(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[2]))])\n","          \n","        # print(self.ub[0])\n","        # print(g_best)\n","        # g_best[0] = g_best[2] #self.get_binary_position(g_best[0])\n","        \n","                    # fit_new = self.get_fitness_position(g_best)       \n","        self.solution = g_best\n","        return g_best[2], g_best[self.ID_FIT], self.loss_train, self.fs_counts\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w4w9Rh3LWmTX"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","# from .DYHGSClass import DYHGSClass\n","from numpy.random import uniform, normal, rand\n","\n","    \n","class lvaoa:\n","\n","      def LVAOA(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","    \n","            #Max_iter=1000\n","            #lb=-100\n","            #ub=100\n","            #dim=30  \n","            #SearchAgents_no=5\n","            lb=0\n","            # initialize alpha, beta, and delta_pos\n","            g_best=numpy.zeros(dim)\n","            Alpha_score=float(\"inf\")\n","            XXX=1\n","            verbose = True\n","            # Loop counter\n","            print(\"LVAOA is optimizing  \\\"\"+objf.__name__+\"\\\"\")   \n","            LOGGER_FILE.info(\"LVAOA is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","            s=solution()\n","            timerStart=time.time() \n","            s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            lb = [lb] * dim\n","            ub = [ub] * dim\n","            # print(trainInput.shape)\n","\n","            best_pos1, best_fit1, list_loss1, fs_counts = LVAOAClass(objf, lb, ub, verbose, Max_iter, SearchAgents_no,0.08, 10000,trainInput=trainInput,trainOutput=trainOutput,dim=dim, logger = LOGGER_FILE).train()\n","            # print(md1)\n","            # best_pos1, best_fit1, list_loss1 = md1.train()\n","            \n","            timerEnd=time.time()  \n","            s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            s.executionTime=timerEnd-timerStart\n","            s.bestIndividual=best_pos1\n","            s.convergence1=list_loss1\n","            s.convergence2=fs_counts\n","            \n","\n","            s.optimizer=\"LVAOA\"\n","            s.objfname=objf.__name__\n","            \n","            \n","            \n","            \n","            return s"]},{"cell_type":"markdown","metadata":{"id":"uLs3QAGTW5Ju"},"source":["## 13- AO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3bTrl_uhW-bf"},"outputs":[],"source":["class Root:\n","    \"\"\" This is root of all Algorithms \"\"\"\n","\n","    ID_MIN_PROB = 0  # min problem\n","    ID_MAX_PROB = -1  # max problem\n","\n","    ID_POS = 0  # Position\n","    ID_FIT = 1  # Fitness\n","\n","    EPSILON = 10E-10    # Avoid division by 0 \n","\n","    DEFAULT_LB = -1\n","    DEFAULT_UB = 1\n","\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True):\n","        self.verbose = verbose\n","        self.obj_func = obj_func\n","        if lb is None:\n","            print(\"Lower bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.lb = array(lb)\n","        if ub is None:\n","            print(\"Upper bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.ub = array(ub)\n","           \n","        if len(lb) != len(ub):\n","            print(\"Lower bound and Upper bound need to have the same length\")\n","            exit(0)\n","        \n","        self.problem_size = len(lb)\n","        self.solution, self.loss_train, self.fs_counts = None, [], []\n","\n","    def create_solution(self, minmax=0):\n","        \"\"\" Return solution with 2 element: position of solution and fitness of solution\n","        Parameters\n","        ----------\n","        minmax\n","            0 - minimum problem, else - maximum problem\n","        \"\"\"\n","        position = uniform(self.lb, self.ub)\n","        # print(position)\n","        # position=numpy.random.randint(2, size=(self.pop_size,self.dim))\n","        # for i in range(0,self.pop_size):\n","        #   while numpy.sum(position[i,:])==0:   \n","        #           position[i,:]=numpy.random.randint(2, size=(1,self.dim))\n","        fitness, binary_pos = self.get_fitness_position(position=position, minmax=minmax)\n","        return [position, fitness, binary_pos]\n","\n","    def get_fitness_position(self,position=None, minmax=0):\n","        \"\"\"     Assumption that objective function always return the original value\n","        :param position: 1-D numpy array\n","        :param minmax: 0- min problem, 1 - max problem\n","        :return:\n","        \"\"\"\n","        # print(position)\n","        Positions=position;\n","         \n","        \n","        for j in range(0,self.dim):\n","          ss= transfer_functions_benchmark.s1(position[j])\n","          if (random.random()<ss): \n","              Positions[j]=1;\n","          else:\n","              Positions[j]=0;\n","\n","        return self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) if minmax == 0 else 1.0 / (self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) + self.EPSILON), Positions     # Avoid division by 0\n","\n","    def get_fitness_solution(self,solution=None, minmax=0):\n","        return self.get_fitness_position(solution[self.ID_POS],self.trainInput,self.trainOutput,self.dim, minmax)\n","\n","    def get_global_best_global_worst_solution(self, pop=None, id_fit=None, id_best=None):\n","        sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n","        if id_best == self.ID_MIN_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MAX_PROB])\n","        elif id_best == self.ID_MAX_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MIN_PROB])\n","\n","\n","    def update_global_best_global_worst_solution(self, pop=None, id_best=None, id_worst=None, g_best=None):\n","        \"\"\" Sort the copy of population and update the current best position. Return the new current best position \"\"\"\n","        sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n","        current_best = sorted_pop[id_best]\n","        g_best = deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n","        return g_best, sorted_pop[id_worst]\n","\n","\n","    def amend_position(self, position=None):\n","        return clip(position, self.lb, self.ub)\n","\n","    def amend_position_random(self, position=None):\n","        return where(logical_and(self.lb <= position, position <= self.ub), position, uniform(self.lb, self.ub))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x7aVdCNYXLSf"},"outputs":[],"source":["from numpy.random import normal, rand, choice\n","from numpy import mean, pi, sin, cos, array\n","from math import gamma\n","# from mealpy.optimizer import Root\n","\n","\n","class AOClass(Root):\n","    \n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True, epoch=1000, pop_size=100, L=0.08, LH=10000,trainInput=None,trainOutput=None,dim=None, logger=None):\n","    # def __init__(self, obj_func=None, lb=None, ub=None, verbose=True, epoch=750, pop_size=100,trainInput=None,trainOutput=None,dim=None, logger=None):\n","        # super().__init__({\"obj_func\":obj_func, \"lb\":lb, \"ub\":ub, \"verbose\":verbose})#, \"trainInput\"=trainInput,\"trainOutput\"=trainOutput,\"dim\"=dim})\n","        super().__init__(obj_func,lb, ub, verbose)\n","        # print(trainInput.shape)\n","        self.epoch = epoch\n","        self.pop_size = pop_size\n","      \n","        self.trainInput=trainInput\n","        self.dim=dim\n","        self.trainOutput=trainOutput\n","        self.logger = logger\n","               \n","    # def __init__(self, obj_func=None, lb=None, ub=None, verbose=True, epoch=750, pop_size=100, **kwargs):\n","    #     super().__init__(obj_func, lb, ub, verbose, kwargs)\n","    #     self.epoch = epoch\n","    #     self.pop_size = pop_size\n","\n","    def get_simple_levy_step(self):\n","        beta = 1.5\n","        sigma = (gamma(1 + beta) * sin(pi * beta / 2) / (gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n","        u = normal(0, 1, self.problem_size) * sigma\n","        v = normal(1, self.problem_size)\n","        step = u / abs(v) ** (1 / beta)\n","        return step\n","    \n","    def train(self):\n","        alpha = 0.1\n","        delta = 0.1\n","\n","        pop = [self.create_solution() for _ in range(self.pop_size)]\n","        # g_best = self.get_global_best_solution(pop=pop, id_fit=self.ID_FIT, id_best=self.ID_MIN_PROB)\n","        g_best, g_worst = self.get_global_best_global_worst_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","\n","        for epoch in range(self.epoch):\n","            g1 = 2 * rand() - 1                 # Eq. 16\n","            g2 = 2 * (1 - epoch / self.epoch)   # Eq. 17\n","\n","            dim_list = array(list(range(1, self.problem_size + 1)))\n","            miu = 0.00565\n","            r0 = 10\n","            r = r0 + miu * dim_list\n","            w = 0.005\n","            phi0 = 3 * pi / 2\n","            phi = -w * dim_list + phi0\n","            x = r * sin(phi)           # Eq.(9)\n","            y = r * cos(phi)           # Eq.(10)\n","            QF = (epoch+1) ** ((2 * rand() - 1) / (1 - self.epoch) ** 2)   # Eq.(15)        Quality function\n","\n","            for i in range(self.pop_size):\n","                x_mean = mean(array([item[self.ID_FIT] for item in pop]), axis=0)\n","                if (epoch+1) <= (2/3) * self.epoch:        # Eq. 3, 4  #exploration\n","                    if rand() < 0.5:\n","                        pos_new = g_best[self.ID_POS] * (1 - (epoch+1)/self.epoch) + rand() * (x_mean - g_best[self.ID_POS])\n","                    else:\n","                        idx = choice(list(set(range(0, self.pop_size)) - {i}))\n","                        pos_new = g_best[self.ID_POS] * self.get_simple_levy_step() + pop[idx][self.ID_POS] + rand() * (y - x)          # Eq. 5\n","                else: #exploitation\n","                    if rand() < 0.5:\n","                        pos_new = alpha * (g_best[self.ID_POS] - x_mean) - rand() * (rand() * (self.ub - self.lb) + self.lb) * delta    # Eq. 13\n","                    else:\n","                        pos_new = QF * g_best[self.ID_POS] - (g2 * pop[i][self.ID_POS] * rand()) - g2 * self.get_simple_levy_step() + rand() * g1   # Eq. 14\n","                \n","                # fit_new = self.get_fitness_position(pos_new)\n","                print(pos_new)\n","                fit_new, pos_new_binary = self.get_fitness_position(pos_new)\n","\n","                if fit_new < pop[i][self.ID_FIT]:\n","                    # pop[i] = [pos_new, fit_new]\n","                    pop[i] = [pos_new, fit_new, pos_new_binary]\n","\n","            # g_best = self.update_global_best_solution(pop, self.ID_MIN_PROB, g_best)\n","            ## Update global best and global worst\n","            g_best, g_worst = self.update_global_best_global_worst_solution(pop, self.ID_MIN_PROB, self.ID_MAX_PROB, g_best)\n","\n","            self.loss_train.append(g_best[self.ID_FIT])\n","        #     if self.verbose:\n","        #         print(\">Epoch: {}, Best fit: {}\".format(epoch + 1, g_best[self.ID_FIT]))\n","        # self.solution = g_best\n","        # return g_best[self.ID_POS], g_best[self.ID_FIT], self.loss_train\n","\n","            self.fs_counts.append(sum(g_best[2]))\n","            \n","            if (epoch%1==0):\n","                \n","                print(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[2]))])\n","                self.logger.info(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[2]))])\n","          \n","        self.solution = g_best\n","        return g_best[2], g_best[self.ID_FIT], self.loss_train, self.fs_counts\n","\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xf94_ZZTXN-X"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","# from .DYHGSClass import DYHGSClass\n","from numpy.random import uniform, normal, rand\n","\n","    \n","class ao:\n","\n","      def AO(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","    \n","            #Max_iter=1000\n","            #lb=-100\n","            #ub=100\n","            #dim=30  \n","            #SearchAgents_no=5\n","            lb=0\n","            # initialize alpha, beta, and delta_pos\n","            g_best=numpy.zeros(dim)\n","            Alpha_score=float(\"inf\")\n","            XXX=1\n","            verbose = True\n","            # Loop counter\n","            print(\"AO is optimizing  \\\"\"+objf.__name__+\"\\\"\")   \n","            LOGGER_FILE.info(\"AO is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","            s=solution()\n","            timerStart=time.time() \n","            s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            lb = [lb] * dim\n","            ub = [ub] * dim\n","            # print(trainInput.shape)\n","\n","            best_pos1, best_fit1, list_loss1, fs_counts = AOClass(objf, lb, ub, verbose, Max_iter, SearchAgents_no,0.08, 10000,trainInput=trainInput,trainOutput=trainOutput,dim=dim, logger = LOGGER_FILE).train()\n","            # print(md1)\n","            # best_pos1, best_fit1, list_loss1 = md1.train()\n","            \n","            timerEnd=time.time()  \n","            s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            s.executionTime=timerEnd-timerStart\n","            s.bestIndividual=best_pos1\n","            s.convergence1=list_loss1\n","            s.convergence2=fs_counts\n","            \n","\n","            s.optimizer=\"AO\"\n","            s.objfname=objf.__name__\n","            \n","            return s"]},{"cell_type":"markdown","metadata":{"id":"7Ny-xGTf0SKy"},"source":["## 13- DOLAO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OmzS3MEi0SKz"},"outputs":[],"source":["class Root:\n","    \"\"\" This is root of all Algorithms \"\"\"\n","\n","    ID_MIN_PROB = 0  # min problem\n","    ID_MAX_PROB = -1  # max problem\n","\n","    ID_POS = 0  # Position\n","    ID_FIT = 1  # Fitness\n","\n","    EPSILON = 10E-10    # Avoid division by 0 \n","\n","    DEFAULT_LB = -1\n","    DEFAULT_UB = 1\n","\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True):\n","        self.verbose = verbose\n","        self.obj_func = obj_func\n","        if lb is None:\n","            print(\"Lower bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.lb = array(lb)\n","        if ub is None:\n","            print(\"Upper bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.ub = array(ub)\n","           \n","        if len(lb) != len(ub):\n","            print(\"Lower bound and Upper bound need to have the same length\")\n","            exit(0)\n","        \n","        self.problem_size = len(lb)\n","        self.solution, self.loss_train, self.fs_counts = None, [], []\n","\n","    def create_solution(self, minmax=0):\n","        \"\"\" Return solution with 2 element: position of solution and fitness of solution\n","        Parameters\n","        ----------\n","        minmax\n","            0 - minimum problem, else - maximum problem\n","        \"\"\"\n","        position = uniform(self.lb, self.ub)\n","        # print(position)\n","        # position=numpy.random.randint(2, size=(self.pop_size,self.dim))\n","        # for i in range(0,self.pop_size):\n","        #   while numpy.sum(position[i,:])==0:   \n","        #           position[i,:]=numpy.random.randint(2, size=(1,self.dim))\n","        fitness, binary_pos = self.get_fitness_position(position=position, minmax=minmax)\n","        return [position, fitness, binary_pos]\n","\n","    def get_fitness_position(self,position=None, minmax=0):\n","        \"\"\"     Assumption that objective function always return the original value\n","        :param position: 1-D numpy array\n","        :param minmax: 0- min problem, 1 - max problem\n","        :return:\n","        \"\"\"\n","        # print(position)\n","        Positions=position;\n","         \n","        \n","        for j in range(0,self.dim):\n","          ss= transfer_functions_benchmark.s1(position[j])\n","          if (random.random()<ss): \n","              Positions[j]=1;\n","          else:\n","              Positions[j]=0;\n","\n","        return self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) if minmax == 0 else 1.0 / (self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) + self.EPSILON), Positions     # Avoid division by 0\n","\n","    def get_fitness_solution(self,solution=None, minmax=0):\n","        return self.get_fitness_position(solution[self.ID_POS],self.trainInput,self.trainOutput,self.dim, minmax)\n","\n","    def get_global_best_global_worst_solution(self, pop=None, id_fit=None, id_best=None):\n","        sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n","        if id_best == self.ID_MIN_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MAX_PROB])\n","        elif id_best == self.ID_MAX_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MIN_PROB])\n","\n","\n","    def update_global_best_global_worst_solution(self, pop=None, id_best=None, id_worst=None, g_best=None):\n","        \"\"\" Sort the copy of population and update the current best position. Return the new current best position \"\"\"\n","        sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n","        current_best = sorted_pop[id_best]\n","        g_best = deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n","        return g_best, sorted_pop[id_worst]\n","\n","\n","    def amend_position(self, position=None):\n","        return clip(position, self.lb, self.ub)\n","\n","    def amend_position_random(self, position=None):\n","        return where(logical_and(self.lb <= position, position <= self.ub), position, uniform(self.lb, self.ub))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpLE4trZ0SK0"},"outputs":[],"source":["from numpy.random import normal, rand, choice\n","from numpy import mean, pi, sin, cos, array\n","from math import gamma\n","# from mealpy.optimizer import Root\n","\n","\n","class DOLAOClass(Root):\n","    \n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True, epoch=1000, pop_size=100, L=0.08, LH=10000,trainInput=None,trainOutput=None,dim=None, logger=None):\n","    # def __init__(self, obj_func=None, lb=None, ub=None, verbose=True, epoch=750, pop_size=100,trainInput=None,trainOutput=None,dim=None, logger=None):\n","        # super().__init__({\"obj_func\":obj_func, \"lb\":lb, \"ub\":ub, \"verbose\":verbose})#, \"trainInput\"=trainInput,\"trainOutput\"=trainOutput,\"dim\"=dim})\n","        super().__init__(obj_func,lb, ub, verbose)\n","        # print(trainInput.shape)\n","        self.epoch = epoch\n","        self.pop_size = pop_size\n","      \n","        self.trainInput=trainInput\n","        self.dim=dim\n","        self.trainOutput=trainOutput\n","        self.logger = logger\n","               \n","    # def __init__(self, obj_func=None, lb=None, ub=None, verbose=True, epoch=750, pop_size=100, **kwargs):\n","    #     super().__init__(obj_func, lb, ub, verbose, kwargs)\n","    #     self.epoch = epoch\n","    #     self.pop_size = pop_size\n","\n","    def get_simple_levy_step(self):\n","        beta = 1.5\n","        sigma = (gamma(1 + beta) * sin(pi * beta / 2) / (gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n","        u = normal(0, 1, self.problem_size) * sigma\n","        v = normal(1, self.problem_size)\n","        step = u / abs(v) ** (1 / beta)\n","        return step\n","        \n","    def create_solution_OBL(self,position=None, minmax=0):\n","          position1=deepcopy(position);\n","          position2=numpy.zeros((self.pop_size,self.dim));\n","          position2_binary=numpy.zeros((self.pop_size,self.dim));\n","\n","          pos=numpy.zeros((self.pop_size,self.dim));\n","          pos1=numpy.zeros((self.pop_size,self.dim));\n","          pos1_binary=numpy.zeros((self.pop_size,self.dim));\n","          pos_binary=numpy.zeros((self.pop_size,self.dim));\n","\n","    #      print(pos[1])\n","\n","          double_population=numpy.zeros((2*self.pop_size,self.dim))\n","          double_population_binary=numpy.zeros((2*self.pop_size,self.dim))\n","\n","          double_fitness=numpy.zeros(2*self.pop_size)\n","          double_sorted_population=numpy.zeros((2*self.pop_size,self.dim))\n","          double_sorted_population_binary=numpy.zeros((2*self.pop_size,self.dim))\n","          \n","          double_fitness_sorted=numpy.zeros(2*self.pop_size)\n","          fitnesspos=numpy.zeros(self.pop_size)\n","          fitnesspos2=numpy.zeros(self.pop_size)\n","          fitness1=numpy.zeros(self.pop_size)\n","          LB=numpy.ones(self.dim)*self.lb\n","          UB=numpy.ones(self.dim)*self.ub\n","    #     print(rand())\n","\n","          for i in range(0,self.pop_size):\n","            position2[i]=position1[i][0]\n","            position2_binary[i]=position1[i][2]\n","            # pos[i]=position1[i][0]+rand()*(rand()*(UB-LB-position1[i][0])-position1[i][0])   \n","            gen = position1[i][0]+rand()*(rand()*(UB-LB-position1[i][0])-position1[i][0]) \n","            \n","            if max(gen) > self.ub[0] or min(gen) < self.lb[0]:\n","                gen = self.amend_position_random(gen)            \n","            pos[i]= gen\n","              \n","            fitnesspos[i], pos_binary[i] = self.get_fitness_position(position=pos[i], minmax=minmax)\n","            fitness1[i]=position1[i][1]\n","      #      print(fitness1[i])\n","          \n","          double_population=numpy.concatenate((position2,pos),axis=0)\n","          double_population_binary=numpy.concatenate((position2_binary,pos_binary),axis=0)\n","          double_fitness=numpy.concatenate((fitness1, fitnesspos),axis=0);\n","          double_fitness_sorted =numpy.sort(double_fitness);\n","          \n","          I2 =numpy.argsort(double_fitness);\n","          for newindex in range(0,2*self.pop_size):\n","              double_sorted_population[newindex,:]=numpy.array(double_population[I2[newindex],:])           \n","              double_sorted_population_binary[newindex,:]=numpy.array(double_population_binary[I2[newindex],:])           \n","            \n","          fitnesspos2=double_fitness_sorted[0:self.pop_size]\n","          \n","          pos1=double_sorted_population[0:self.pop_size]\n","          pos1_binary=double_sorted_population_binary[0:self.pop_size]\n","          \n","          out = list(zip(pos1, fitnesspos2, pos1_binary))\n","\n","          # for i in range(0,self.pop_size):\n","          #   position1[i][0]=pos1[i]\n","          #   position1[i][1]=fitnesspos2[i]\n","          return out\n","\n","\n","    def train(self):\n","        alpha = 0.1\n","        delta = 0.1\n","\n","        pop = [self.create_solution() for _ in range(self.pop_size)]\n","\n","        # new\n","        pop = self.create_solution_OBL(pop)\n"," \n","        # g_best = self.get_global_best_solution(pop=pop, id_fit=self.ID_FIT, id_best=self.ID_MIN_PROB)\n","        g_best, g_worst = self.get_global_best_global_worst_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","\n","        for epoch in range(self.epoch):\n","            g1 = 2 * rand() - 1                 # Eq. 16\n","            g2 = 2 * (1 - epoch / self.epoch)   # Eq. 17\n","\n","            dim_list = array(list(range(1, self.problem_size + 1)))\n","            miu = 0.00565\n","            r0 = 10\n","            r = r0 + miu * dim_list\n","            w = 0.005\n","            phi0 = 3 * pi / 2\n","            phi = -w * dim_list + phi0\n","            x = r * sin(phi)           # Eq.(9)\n","            y = r * cos(phi)           # Eq.(10)\n","            QF = (epoch+1) ** ((2 * rand() - 1) / (1 - self.epoch) ** 2)   # Eq.(15)        Quality function\n","\n","            for i in range(self.pop_size):\n","                x_mean = mean(array([item[self.ID_FIT] for item in pop]), axis=0)\n","                if (epoch+1) <= (2/3) * self.epoch:        # Eq. 3, 4\n","                    if rand() < 0.5:\n","                        pos_new = g_best[self.ID_POS] * (1 - (epoch+1)/self.epoch) + rand() * (x_mean - g_best[self.ID_POS])\n","                    else:\n","                        idx = choice(list(set(range(0, self.pop_size)) - {i}))\n","                        pos_new = g_best[self.ID_POS] * self.get_simple_levy_step() + pop[idx][self.ID_POS] + rand() * (y - x)          # Eq. 5\n","                else:\n","                    if rand() < 0.5:\n","                        pos_new = alpha * (g_best[self.ID_POS] - x_mean) - rand() * (rand() * (self.ub - self.lb) + self.lb) * delta    # Eq. 13\n","                    else:\n","                        pos_new = QF * g_best[self.ID_POS] - (g2 * pop[i][self.ID_POS] * rand()) - g2 * self.get_simple_levy_step() + rand() * g1   # Eq. 14\n","                \n","                # fit_new = self.get_fitness_position(pos_new)\n","                fit_new, pos_new_binary = self.get_fitness_position(pos_new)\n","\n","                if fit_new < pop[i][self.ID_FIT]:\n","                    # pop[i] = [pos_new, fit_new]\n","                    pop[i] = [pos_new, fit_new, pos_new_binary]\n","\n","            # g_best = self.update_global_best_solution(pop, self.ID_MIN_PROB, g_best)\n","            ## Update global best and global worst\n","            g_best, g_worst = self.update_global_best_global_worst_solution(pop, self.ID_MIN_PROB, self.ID_MAX_PROB, g_best)\n","\n","            self.loss_train.append(g_best[self.ID_FIT])\n","        #     if self.verbose:\n","        #         print(\">Epoch: {}, Best fit: {}\".format(epoch + 1, g_best[self.ID_FIT]))\n","        # self.solution = g_best\n","        # return g_best[self.ID_POS], g_best[self.ID_FIT], self.loss_train\n","\n","            self.fs_counts.append(sum(g_best[2]))\n","            \n","            if (epoch%1==0):\n","                \n","                print(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[2]))])\n","                self.logger.info(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[2]))])\n","          \n","        self.solution = g_best\n","        return g_best[2], g_best[self.ID_FIT], self.loss_train, self.fs_counts\n","\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nChkdVd30SK2"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","# from .DYHGSClass import DYHGSClass\n","from numpy.random import uniform, normal, rand\n","\n","    \n","class dolao:\n","\n","      def DOLAO(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","    \n","            #Max_iter=1000\n","            #lb=-100\n","            #ub=100\n","            #dim=30  \n","            #SearchAgents_no=5\n","            lb=0\n","            # initialize alpha, beta, and delta_pos\n","            g_best=numpy.zeros(dim)\n","            Alpha_score=float(\"inf\")\n","            XXX=1\n","            verbose = True\n","            # Loop counter\n","            print(\"DOLAO is optimizing  \\\"\"+objf.__name__+\"\\\"\")   \n","            LOGGER_FILE.info(\"DOLAO is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","            s=solution()\n","            timerStart=time.time() \n","            s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            lb = [lb] * dim\n","            ub = [ub] * dim\n","            # print(trainInput.shape)\n","\n","            best_pos1, best_fit1, list_loss1, fs_counts = DOLAOClass(objf, lb, ub, verbose, Max_iter, SearchAgents_no,0.08, 10000,trainInput=trainInput,trainOutput=trainOutput,dim=dim, logger = LOGGER_FILE).train()\n","            # print(md1)\n","            # best_pos1, best_fit1, list_loss1 = md1.train()\n","            \n","            timerEnd=time.time()  \n","            s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            s.executionTime=timerEnd-timerStart\n","            s.bestIndividual=best_pos1\n","            s.convergence1=list_loss1\n","            s.convergence2=fs_counts\n","            \n","\n","            s.optimizer=\"DOLAO\"\n","            s.objfname=objf.__name__\n","            \n","            return s"]},{"cell_type":"markdown","metadata":{"id":"s0FneYoeXW8B"},"source":["## 14- ArchOA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JJDwFO-EXWfH"},"outputs":[],"source":["class Root:\n","    \"\"\" This is root of all Algorithms \"\"\"\n","\n","    ID_MIN_PROB = 0  # min problem\n","    ID_MAX_PROB = -1  # max problem\n","\n","    ID_POS = 0  # Position\n","    ID_FIT = 1  # Fitness\n","\n","    EPSILON = 10E-10    # Avoid division by 0 \n","\n","    DEFAULT_LB = -1\n","    DEFAULT_UB = 1\n","\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True):\n","        self.verbose = verbose\n","        self.obj_func = obj_func\n","        if lb is None:\n","            print(\"Lower bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.lb = array(lb)\n","        if ub is None:\n","            print(\"Upper bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.ub = array(ub)\n","           \n","        if len(lb) != len(ub):\n","            print(\"Lower bound and Upper bound need to have the same length\")\n","            exit(0)\n","        \n","        self.problem_size = len(lb)\n","        self.solution, self.loss_train, self.fs_counts = None, [], []\n","\n","    def create_solution(self, minmax=0):\n","        \"\"\" Return solution with 2 element: position of solution and fitness of solution\n","        Parameters\n","        ----------\n","        minmax\n","            0 - minimum problem, else - maximum problem\n","        \"\"\"\n","        position = uniform(self.lb, self.ub)\n","        # print(position)\n","        # position=numpy.random.randint(2, size=(self.pop_size,self.dim))\n","        # for i in range(0,self.pop_size):\n","        #   while numpy.sum(position[i,:])==0:   \n","        #           position[i,:]=numpy.random.randint(2, size=(1,self.dim))\n","        fitness, binary_pos = self.get_fitness_position(position=position, minmax=minmax)\n","        return [position, fitness, binary_pos]\n","\n","    def get_fitness_position(self,position=None, minmax=0):\n","        \"\"\"     Assumption that objective function always return the original value\n","        :param position: 1-D numpy array\n","        :param minmax: 0- min problem, 1 - max problem\n","        :return:\n","        \"\"\"\n","        # print(position)\n","        Positions=position;\n","         \n","        \n","        for j in range(0,self.dim):\n","          ss= transfer_functions_benchmark.s1(position[j])\n","          if (random.random()<ss): \n","              Positions[j]=1;\n","          else:\n","              Positions[j]=0;\n","\n","        return self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) if minmax == 0 else 1.0 / (self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) + self.EPSILON), Positions     # Avoid division by 0\n","\n","    def get_fitness_solution(self,solution=None, minmax=0):\n","        return self.get_fitness_position(solution[self.ID_POS],self.trainInput,self.trainOutput,self.dim, minmax)\n","\n","    def get_global_best_global_worst_solution(self, pop=None, id_fit=None, id_best=None):\n","        sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n","        if id_best == self.ID_MIN_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MAX_PROB])\n","        elif id_best == self.ID_MAX_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MIN_PROB])\n","\n","\n","    def update_global_best_global_worst_solution(self, pop=None, id_best=None, id_worst=None, g_best=None):\n","        \"\"\" Sort the copy of population and update the current best position. Return the new current best position \"\"\"\n","        sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n","        current_best = sorted_pop[id_best]\n","        g_best = deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n","        return g_best, sorted_pop[id_worst]\n","\n","\n","    def amend_position(self, position=None):\n","        return clip(position, self.lb, self.ub)\n","\n","    def amend_position_random(self, position=None):\n","        return where(logical_and(self.lb <= position, position <= self.ub), position, uniform(self.lb, self.ub))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uGWO-gl8XloV"},"outputs":[],"source":["from numpy.random import normal, rand, choice\n","from numpy import mean, pi, sin, cos, array\n","from math import gamma\n","# from mealpy.optimizer import Root\n","\n","from numpy import exp\n","from numpy.random import uniform, choice, rand\n","from numpy import min as np_min\n","from numpy import max as np_max\n","\n","class ArchOAClass(Root):\n","    \n","    ID_POS = 0\n","    ID_FIT = 1\n","    ID_DEN = 2  # Density\n","    ID_VOL = 3  # Volume\n","    ID_ACC = 4  # Acceleration\n","        \n","    def __init__(self, obj_func=None, lb=None, ub=None, verbose=True, epoch=750, pop_size=100, c1=2, c2=6, c3=2, c4=0.5,trainInput=None,trainOutput=None,dim=None, logger=None):\n","    # def __init__(self, obj_func=None, lb=None, ub=None, verbose=True, epoch=750, pop_size=100,trainInput=None,trainOutput=None,dim=None, logger=None):\n","        # super().__init__({\"obj_func\":obj_func, \"lb\":lb, \"ub\":ub, \"verbose\":verbose})#, \"trainInput\"=trainInput,\"trainOutput\"=trainOutput,\"dim\"=dim})\n","        super().__init__(obj_func,lb, ub, verbose)\n","        # print(trainInput.shape)\n","        self.epoch = epoch\n","        self.pop_size = pop_size\n","        self.c1 = c1    # Default belongs [1, 2]\n","        self.c2 = c2    # Default belongs [2, 4, 6]\n","        self.c3 = c3    # Default belongs [1, 2]\n","        self.c4 = c4    # Default belongs [0.5, 1]\n","\n","        self.trainInput=trainInput\n","        self.dim=dim\n","        self.trainOutput=trainOutput\n","        self.logger = logger\n","               \n","    def create_solution(self, minmax=0):\n","        pos = uniform(self.lb, self.ub, self.problem_size)\n","        fit, bin_pos = self.get_fitness_position(pos, minmax=minmax)\n","        den = uniform(self.lb, self.ub, self.problem_size)\n","        vol = uniform(self.lb, self.ub, self.problem_size)\n","        acc = self.lb + uniform(self.lb, self.ub, self.problem_size) * (self.ub - self.lb)\n","        return [pos, fit, den, vol, acc]\n","    \n","    def train(self):\n","        pop = [self.create_solution() for _ in range(self.pop_size)]\n","        # g_best = self.get_global_best_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","        g_best, g_worst = self.get_global_best_global_worst_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","\n","        acc_upper = 0.9\n","        acc_lower = 0.1\n","\n","        for epoch in range(0, self.epoch):\n","            tf = exp((epoch + 1) / self.epoch - 1)                          # Transfer operator Eq. 8\n","            ddf = exp(1 - (epoch + 1)/self.epoch) - (epoch+1)/self.epoch    # Density decreasing factor Eq. 9\n","\n","            list_acc = []\n","            ## Calculate new density, volume and acceleration\n","            for i in range(0, self.pop_size):\n","                # Update density and volume of each object using Eq. 7\n","                new_den = pop[i][self.ID_DEN] + uniform() * (g_best[self.ID_DEN] - pop[i][self.ID_DEN])\n","                new_vol = pop[i][self.ID_VOL] + uniform() * (g_best[self.ID_VOL] - pop[i][self.ID_VOL])\n","\n","                if tf <= 0.5:       # Exploration phase\n","                    # Update acceleration using Eq. 10 and normalize acceleration using Eq. 12\n","                    id_rand = choice(list(set(range(0, self.pop_size)) - {i}))\n","                    new_acc = (pop[id_rand][self.ID_DEN] + pop[id_rand][self.ID_VOL] * pop[id_rand][self.ID_ACC]) / (new_den * new_vol)\n","                else:\n","                    new_acc = (g_best[self.ID_DEN] + g_best[self.ID_VOL] * g_best[self.ID_ACC]) / (new_den * new_vol)\n","                list_acc.append(new_acc)\n","                pop[i][self.ID_DEN] = new_den\n","                pop[i][self.ID_VOL] = new_vol\n","            min_acc = np_min(list_acc)\n","            max_acc = np_max(list_acc)\n","            ## Normalize acceleration using Eq. 12\n","            for i in range(0, self.pop_size):\n","                pop[i][self.ID_ACC] = acc_upper * (pop[i][self.ID_ACC] - min_acc) / (max_acc - min_acc) + acc_lower\n","\n","            for i in range(0, self.pop_size):\n","                if tf <= 0.5:   # update position using Eq. 13\n","                    id_rand = choice(list(set(range(0, self.pop_size)) - {i}))\n","                    pos_new = pop[i][self.ID_POS] + self.c1 * uniform() * pop[i][self.ID_ACC] * ddf * (pop[id_rand][self.ID_POS] - pop[i][self.ID_POS])\n","                else:\n","                    p = 2 * rand()  - self.c4\n","                    f = 1 if p <= 0.5 else -1\n","                    t = self.c3 * tf\n","                    pos_new = g_best[self.ID_POS] + f * self.c2 * rand() * pop[i][self.ID_ACC] * ddf * (t * g_best[self.ID_POS] - pop[i][self.ID_POS])\n","                # fit_new = self.get_fitness_position(pos_new)\n","                fit_new, pos_new_binary = self.get_fitness_position(pos_new)\n","\n","                if fit_new < pop[i][self.ID_FIT]:\n","                    pop[i][self.ID_POS] = pos_new_binary#pos_new\n","                    pop[i][self.ID_FIT] = fit_new\n","                    \n","                     # pop[i] = [pos_new, fit_new]\n","                    # pop[i] = [pos_new, fit_new, pos_new_binary]\n","\n","            # g_best = self.update_global_best_solution(pop, self.ID_MIN_PROB, g_best)\n","            ## Update global best and global worst\n","            g_best, g_worst = self.update_global_best_global_worst_solution(pop, self.ID_MIN_PROB, self.ID_MAX_PROB, g_best)\n","\n","            self.loss_train.append(g_best[self.ID_FIT])\n","\n","        #     if self.verbose:\n","        #         print(\">Epoch: {}, Best fit: {}\".format(epoch + 1, g_best[self.ID_FIT]))\n","        # self.solution = g_best\n","        # return g_best[self.ID_POS], g_best[self.ID_FIT], self.loss_train\n","            self.fs_counts.append(sum(g_best[self.ID_POS]))\n","            \n","            if (epoch%1==0):\n","                \n","                print(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[self.ID_POS]))])\n","                self.logger.info(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[self.ID_POS]))])\n","          \n","        self.solution = g_best\n","        return g_best[self.ID_POS], g_best[self.ID_FIT], self.loss_train, self.fs_counts\n","\n","            "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NztA6mAvXoUw"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","# from .DYHGSClass import DYHGSClass\n","from numpy.random import uniform, normal, rand\n","\n","    \n","class archoa:\n","\n","      def ArchOA(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","    \n","            #Max_iter=1000\n","            #lb=-100\n","            #ub=100\n","            #dim=30  \n","            #SearchAgents_no=5\n","            lb=0\n","            # initialize alpha, beta, and delta_pos\n","            g_best=numpy.zeros(dim)\n","            Alpha_score=float(\"inf\")\n","            XXX=1\n","            verbose = True\n","            # Loop counter\n","            print(\"ArchOA is optimizing  \\\"\"+objf.__name__+\"\\\"\")   \n","            LOGGER_FILE.info(\"ArchOA is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","            s=solution()\n","            timerStart=time.time() \n","            s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            lb = [lb] * dim\n","            ub = [ub] * dim\n","            # print(trainInput.shape)\n","\n","            best_pos1, best_fit1, list_loss1, fs_counts = ArchOAClass(objf, lb, ub, verbose, Max_iter, SearchAgents_no,0.08, 10000,trainInput=trainInput,trainOutput=trainOutput,dim=dim, logger = LOGGER_FILE).train()\n","            # print(md1)\n","            # best_pos1, best_fit1, list_loss1 = md1.train()\n","            \n","            timerEnd=time.time()  \n","            s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            s.executionTime=timerEnd-timerStart\n","            s.bestIndividual=best_pos1\n","            s.convergence1=list_loss1\n","            s.convergence2=fs_counts\n","            \n","\n","            s.optimizer=\"ArchOA\"\n","            s.objfname=objf.__name__\n","            \n","            return s"]},{"cell_type":"markdown","metadata":{"id":"T6ga-8bIvfp2"},"source":["## 15- HBA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ui0PHHikvhki"},"outputs":[],"source":["class Root:\n","    \"\"\" This is root of all Algorithms \"\"\"\n","\n","    ID_MIN_PROB = 0  # min problem\n","    ID_MAX_PROB = -1  # max problem\n","\n","    ID_POS = 0  # Position\n","    ID_FIT = 1  # Fitness\n","\n","    EPSILON = 10E-10    # Avoid division by 0 \n","\n","    DEFAULT_LB = -1\n","    DEFAULT_UB = 1\n","\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True):\n","        self.verbose = verbose\n","        self.obj_func = obj_func\n","        if lb is None:\n","            print(\"Lower bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.lb = array(lb)\n","        if ub is None:\n","            print(\"Upper bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.ub = array(ub)\n","           \n","        if len(lb) != len(ub):\n","            print(\"Lower bound and Upper bound need to have the same length\")\n","            exit(0)\n","        \n","        self.problem_size = len(lb)\n","        self.solution, self.loss_train, self.fs_counts = None, [], []\n","\n","    def create_solution(self, minmax=0):\n","        \"\"\" Return solution with 2 element: position of solution and fitness of solution\n","        Parameters\n","        ----------\n","        minmax\n","            0 - minimum problem, else - maximum problem\n","        \"\"\"\n","        position = uniform(self.lb, self.ub)\n","        # print(position)\n","        # position=numpy.random.randint(2, size=(self.pop_size,self.dim))\n","        # for i in range(0,self.pop_size):\n","        #   while numpy.sum(position[i,:])==0:   \n","        #           position[i,:]=numpy.random.randint(2, size=(1,self.dim))\n","        fitness, binary_pos = self.get_fitness_position(position=position, minmax=minmax)\n","        return [position, fitness, binary_pos]\n","\n","    def get_fitness_position(self,position=None, minmax=0):\n","        \"\"\"     Assumption that objective function always return the original value\n","        :param position: 1-D numpy array\n","        :param minmax: 0- min problem, 1 - max problem\n","        :return:\n","        \"\"\"\n","        # print(position)\n","        Positions=position;\n","         \n","        \n","        for j in range(0,self.dim):\n","          ss= transfer_functions_benchmark.s1(position[j])\n","          if (random.random()<ss): \n","              Positions[j]=1;\n","          else:\n","              Positions[j]=0;\n","\n","        return self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) if minmax == 0 else 1.0 / (self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) + self.EPSILON), Positions     # Avoid division by 0\n","\n","    def get_fitness_solution(self,solution=None, minmax=0):\n","        return self.get_fitness_position(solution[self.ID_POS],self.trainInput,self.trainOutput,self.dim, minmax)\n","\n","    def get_global_best_global_worst_solution(self, pop=None, id_fit=None, id_best=None):\n","        sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n","        if id_best == self.ID_MIN_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MAX_PROB])\n","        elif id_best == self.ID_MAX_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MIN_PROB])\n","\n","\n","    def update_global_best_global_worst_solution(self, pop=None, id_best=None, id_worst=None, g_best=None):\n","        \"\"\" Sort the copy of population and update the current best position. Return the new current best position \"\"\"\n","        sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n","        current_best = sorted_pop[id_best]\n","        g_best = deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n","        return g_best, sorted_pop[id_worst]\n","\n","\n","    def amend_position(self, position=None):\n","        return clip(position, self.lb, self.ub)\n","\n","    def amend_position_random(self, position=None):\n","        return where(logical_and(self.lb <= position, position <= self.ub), position, uniform(self.lb, self.ub))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"coEIFhnIfmg4"},"outputs":[],"source":["from numpy.linalg import norm\n","\n","class HBAClass(Root):\n","\n","    def __init__(self, obj_func=None, lb=None, ub=None, verbose=True, epoch=750, pop_size=100,alpha=5, miu=0.5, moa_min=0.2, moa_max=0.9,trainInput=None,trainOutput=None,dim=None, logger=None):\n","        # super().__init__({\"obj_func\":obj_func, \"lb\":lb, \"ub\":ub, \"verbose\":verbose})#, \"trainInput\"=trainInput,\"trainOutput\"=trainOutput,\"dim\"=dim})\n","        super().__init__(obj_func,lb, ub, verbose)\n","        # print(trainInput.shape)\n","        self.epoch = epoch\n","        self.pop_size = pop_size\n","        self.alpha = alpha          # Default: 5, fixed parameter, sensitive exploitation parameter\n","        self.objf=obj_func\n","        self.pop_size=pop_size\n","\n","        self.trainInput=trainInput\n","        self.dim=dim\n","        self.trainOutput=trainOutput\n","        self.logger = logger\n","        \n","    def Intensity(N,Xprey,X):\n","        di=[]\n","        S=[]\n","        for i in range(0,N):\n","\n","            di.append(np.power(( norm((np.array(X[i-1,:])-np.array(Xprey)+np.spacing(1)))), 2))\n","            S.append(np.power(( norm((X[i-1,:]-X[i]+np.spacing(1)))),2))\n","        # print(len(di))\n","        di.append(np.power((norm((X[N-1,:]-Xprey+np.spacing(1)))),2))\n","        S.append(np.power((norm((X[N-1,:]-X[0,:]+np.spacing(1)))),2))\n","        I=[]\n","        for i in range(0,N):\n","            r2=random.random()#rand(1)\n","            \n","            I.append(r2*S[i]/(4*math.pi*di[i]))\n","        \n","        return I\n","\n","    def train(self):\n","\n","        beta=6\n","        C=2\n","        vec_flag=[1,-1]\n","\n","        #Initialize population with random positions\n","        pop = [self.create_solution() for _ in range(self.pop_size)]\n","        pop=np.asarray(pop)      \n","        \n","        \n","        #####################################################################################\n","        #Evaluate initial random solutions\n","        Fitness=numpy.zeros(self.pop_size)\n","        #Evaluate the ftness of each honey badger position xi using objective function \n","        for i in range(self.pop_size):\n","            #assign to fi\n","            Fitness[i], pos_bin = self.get_fitness_position(pop[i][self.ID_POS])\n","\n","        #assign fitness to fprey.\n","        fmin = min(Fitness) #fprey\n","\n","        #Save best position xprey\n","        g_best, g_worst = self.get_global_best_global_worst_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","        g_best=np.asarray(g_best) #xprey\n","\n","        #####################################################################################\n","        # S=numpy.zeros((self.epoch,self.dim))\n","        # S=numpy.copy(numpy.random.randint(2, size=(self.epoch,self.dim)))#generating binary individuals\n","\n","        # Fitness=numpy.zeros(self.pop_size)\n","        # # #Evaluate initial random solutions\n","        # for i in range(self.pop_size):        \n","        #   Fitness[i]=self.objf(S[i,:],self.trainInput,self.trainOutput,self.dim)\n","        \n","\n","        # print(Fitness)\n","        # fmin = min(Fitness) #fprey\n","        # print(fmin)\n","        #####################################################################################\n","          \n","\n","        for epoch in range(self.epoch):\n","            #Update the decreasing factor  using (3).\n","            alpha=C*exp(-epoch/self.epoch)   #density factor in Eq. (3)\n","\n","            for i in range(0, self.pop_size):\n","                #Calculate the intensity Ii using Eq. (2).\n","                I=HBAClass.Intensity(self.pop_size,g_best,pop) #intensity in Eq. (2)\n","                \n","                r =random.random()\n","                F=vec_flag[math.floor(2*random.random())]\n","                pos_new = pop[i][self.ID_POS]\n","\n","                for j in range(0, self.problem_size):\n","                                     \n","                    di=np.array(g_best)-np.array(pop[i-1,:])\n","\n","                    if r<.5:\n","                        r3=random.random()\n","                        r4=random.random()\n","                        r5=random.random()\n","                        \n","                        x=np.asarray(F)*r3*alpha*di*abs(cos(2*pi*r4)*(1-math.cos(2*math.pi*r5)))\n","                        y=g_best[self.ID_POS][j] +F*beta*I[self.ID_POS][j]*g_best[self.ID_POS][j]\n","                        \n","                        Z=y+x\n","                        pos_new[j]=Z[1,]\n","    \n","                    else:\n","                        r7=random.random()\n","                        \n","                        Z=g_best[self.ID_POS][j]+F*r7*alpha*di\n","                        pos_new[j]=Z[1,]\n","                \n","                fit_new, pos_new_binary = self.get_fitness_position(pos_new)\n","                \n","                # #######################################################################################\n","                \n","                # # Update if the solution improves\n","                if fit_new < pop[i][self.ID_FIT]:\n","                    pop[i][self.ID_POS] = pos_new_binary#pos_new\n","                    pop[i][self.ID_FIT] = fit_new\n","\n","                \n","                # Update the current best solution\n","                if fit_new<=fmin:\n","                    # g_best=pos_new_binary\n","                    ## Update global best and global worst\n","                    g_best, g_worst = self.update_global_best_global_worst_solution(pop, self.ID_MIN_PROB, self.ID_MAX_PROB, g_best)\n","                    fmin=fit_new\n","\n","                #######################################################################################\n","                fit_new=fmin\n","                pop[i] = [pos_new, fit_new, pos_new_binary]\n","            \n","            ## Update global best and global worst\n","            g_best, g_worst = self.update_global_best_global_worst_solution(pop, self.ID_MIN_PROB, self.ID_MAX_PROB, g_best)\n","\n","            # g_best = self.update_global_best_solution(pop, self.ID_MIN_PROB, g_best)\n","            self.loss_train.append(g_best[self.ID_FIT])\n","        #     if self.verbose:\n","        #         print(\"> Epoch: {}, Best fit: {}\".format(epoch + 1, g_best[self.ID_FIT]))\n","        # self.solution = g_best\n","        # return g_best[self.ID_POS], g_best[self.ID_FIT], self.loss_train\n","\n","            self.fs_counts.append(sum(g_best[2]))\n","            # if self.verbose:\n","            #     print(\"> Epoch: {}, Best fit: {}\".format(epoch + 1, g_best[self.ID_FIT]))\n","                \n","            if (epoch%1==0):\n","                \n","                print(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[2]))])\n","                self.logger.info(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[2]))])\n","          \n","        self.solution = g_best\n","        return g_best[2], g_best[self.ID_FIT], self.loss_train, self.fs_counts\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oO1AY5DYvweQ"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","# from .DYHGSClass import DYHGSClass\n","from numpy.random import uniform, normal, rand\n","\n","    \n","class hba:\n","\n","      def HBA(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","    \n","            #Max_iter=1000\n","            #lb=-100\n","            #ub=100\n","            #dim=30  \n","            #SearchAgents_no=5\n","            lb=0\n","            # initialize alpha, beta, and delta_pos\n","            g_best=numpy.zeros(dim)\n","            Alpha_score=float(\"inf\")\n","            XXX=1\n","            verbose = True\n","            # Loop counter\n","            print(\"HBA is optimizing  \\\"\"+objf.__name__+\"\\\"\")   \n","            LOGGER_FILE.info(\"HBA is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","            s=solution()\n","            timerStart=time.time() \n","            s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            lb = [lb] * dim\n","            ub = [ub] * dim\n","            # print(trainInput.shape)\n","\n","            best_pos1, best_fit1, list_loss1, fs_counts = HBAClass(objf, lb, ub, verbose, Max_iter, SearchAgents_no,0.08, 10000,trainInput=trainInput,trainOutput=trainOutput,dim=dim, logger = LOGGER_FILE).train()\n","            # print(md1)\n","            # best_pos1, best_fit1, list_loss1 = md1.train()\n","            \n","            timerEnd=time.time()  \n","            s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            s.executionTime=timerEnd-timerStart\n","            s.bestIndividual=best_pos1\n","            s.convergence1=list_loss1\n","            s.convergence2=fs_counts\n","            \n","\n","            s.optimizer=\"HBA\"\n","            s.objfname=objf.__name__\n","            \n","            \n","            \n","            \n","            return s"]},{"cell_type":"markdown","metadata":{"id":"9m9MIWW708x1"},"source":["## 16- LVHBA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o24rlDx71NOb"},"outputs":[],"source":["class Root:\n","    \"\"\" This is root of all Algorithms \"\"\"\n","\n","    ID_MIN_PROB = 0  # min problem\n","    ID_MAX_PROB = -1  # max problem\n","\n","    ID_POS = 0  # Position\n","    ID_FIT = 1  # Fitness\n","\n","    EPSILON = 10E-10    # Avoid division by 0 \n","\n","    DEFAULT_LB = -1\n","    DEFAULT_UB = 1\n","\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True):\n","        self.verbose = verbose\n","        self.obj_func = obj_func\n","        if lb is None:\n","            print(\"Lower bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.lb = array(lb)\n","        if ub is None:\n","            print(\"Upper bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.ub = array(ub)\n","           \n","        if len(lb) != len(ub):\n","            print(\"Lower bound and Upper bound need to have the same length\")\n","            exit(0)\n","        \n","        self.problem_size = len(lb)\n","        self.solution, self.loss_train, self.fs_counts = None, [], []\n","\n","    def create_solution(self, minmax=0):\n","        \"\"\" Return solution with 2 element: position of solution and fitness of solution\n","        Parameters\n","        ----------\n","        minmax\n","            0 - minimum problem, else - maximum problem\n","        \"\"\"\n","        position = uniform(self.lb, self.ub)\n","        # print(position)\n","        # position=numpy.random.randint(2, size=(self.pop_size,self.dim))\n","        # for i in range(0,self.pop_size):\n","        #   while numpy.sum(position[i,:])==0:   \n","        #           position[i,:]=numpy.random.randint(2, size=(1,self.dim))\n","        fitness, binary_pos = self.get_fitness_position(position=position, minmax=minmax)\n","        return [position, fitness, binary_pos]\n","\n","    def get_fitness_position(self,position=None, minmax=0):\n","        \"\"\"     Assumption that objective function always return the original value\n","        :param position: 1-D numpy array\n","        :param minmax: 0- min problem, 1 - max problem\n","        :return:\n","        \"\"\"\n","        # print(position)\n","        Positions=position;\n","         \n","        \n","        for j in range(0,self.dim):\n","          ss= transfer_functions_benchmark.s1(position[j])\n","          if (random.random()<ss): \n","              Positions[j]=1;\n","          else:\n","              Positions[j]=0;\n","\n","        return self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) if minmax == 0 else 1.0 / (self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) + self.EPSILON), Positions     # Avoid division by 0\n","\n","    def get_fitness_solution(self,solution=None, minmax=0):\n","        return self.get_fitness_position(solution[self.ID_POS],self.trainInput,self.trainOutput,self.dim, minmax)\n","\n","    def get_global_best_global_worst_solution(self, pop=None, id_fit=None, id_best=None):\n","        sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n","        if id_best == self.ID_MIN_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MAX_PROB])\n","        elif id_best == self.ID_MAX_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MIN_PROB])\n","\n","\n","    def update_global_best_global_worst_solution(self, pop=None, id_best=None, id_worst=None, g_best=None):\n","        \"\"\" Sort the copy of population and update the current best position. Return the new current best position \"\"\"\n","        sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n","        current_best = sorted_pop[id_best]\n","        g_best = deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n","        return g_best, sorted_pop[id_worst]\n","\n","\n","    def amend_position(self, position=None):\n","        return clip(position, self.lb, self.ub)\n","\n","    def amend_position_random(self, position=None):\n","        return where(logical_and(self.lb <= position, position <= self.ub), position, uniform(self.lb, self.ub))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tbWiKxXA9t7b"},"outputs":[],"source":["from numpy.linalg import norm\n","\n","class LVHBAClass(Root):\n","\n","    def __init__(self, obj_func=None, lb=None, ub=None, verbose=True, epoch=750, pop_size=100,alpha=5, miu=0.5, moa_min=0.2, moa_max=0.9,trainInput=None,trainOutput=None,dim=None, logger=None):\n","        # super().__init__({\"obj_func\":obj_func, \"lb\":lb, \"ub\":ub, \"verbose\":verbose})#, \"trainInput\"=trainInput,\"trainOutput\"=trainOutput,\"dim\"=dim})\n","        super().__init__(obj_func,lb, ub, verbose)\n","        # print(trainInput.shape)\n","        self.epoch = epoch\n","        self.pop_size = pop_size\n","        self.alpha = alpha          # Default: 5, fixed parameter, sensitive exploitation parameter\n","        self.objf=obj_func\n","        self.pop_size=pop_size\n","\n","        self.trainInput=trainInput\n","        self.dim=dim\n","        self.trainOutput=trainOutput\n","        self.logger = logger\n","        \n","    def Intensity(N,Xprey,X):\n","        di=[]\n","        S=[]\n","        for i in range(0,N):\n","\n","            di.append(np.power(( norm((np.array(X[i-1,:])-np.array(Xprey)+np.spacing(1)))), 2))\n","            S.append(np.power(( norm((X[i-1,:]-X[i]+np.spacing(1)))),2))\n","        # print(len(di))\n","        di.append(np.power((norm((X[N-1,:]-Xprey+np.spacing(1)))),2))\n","        S.append(np.power((norm((X[N-1,:]-X[0,:]+np.spacing(1)))),2))\n","        I=[]\n","        for i in range(0,N):\n","            r2=random.random()#rand(1)\n","            \n","            I.append(r2*S[i]/(4*math.pi*di[i]))\n","        \n","        return I\n","    \n","    def get_simple_levy_step(self):\n","        beta = 1.5\n","        sigma = (gamma(1 + beta) * sin(pi * beta / 2) / (gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n","        u = normal(0, 1, self.problem_size) * sigma\n","        v = normal(1, self.problem_size)\n","        step = u / abs(v) ** (1 / beta)\n","        return step\n","\n","    def train(self):\n","\n","        beta=6\n","        C=2\n","        vec_flag=[1,-1]\n","\n","        #Initialize population with random positions\n","        pop = [self.create_solution() for _ in range(self.pop_size)]\n","        pop=np.asarray(pop)      \n","        \n","        \n","        #####################################################################################\n","        #Evaluate initial random solutions\n","        Fitness=numpy.zeros(self.pop_size)\n","        #Evaluate the ftness of each honey badger position xi using objective function \n","        for i in range(self.pop_size):\n","            #assign to fi\n","            Fitness[i], pos_bin = self.get_fitness_position(pop[i][self.ID_POS])\n","\n","        #assign fitness to fprey.\n","        fmin = min(Fitness) #fprey\n","\n","        #Save best position xprey\n","        g_best, g_worst = self.get_global_best_global_worst_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","        g_best=np.asarray(g_best) #xprey\n","\n","        #####################################################################################\n","        # S=numpy.zeros((self.epoch,self.dim))\n","        # S=numpy.copy(numpy.random.randint(2, size=(self.epoch,self.dim)))#generating binary individuals\n","\n","        # Fitness=numpy.zeros(self.pop_size)\n","        # # #Evaluate initial random solutions\n","        # for i in range(self.pop_size):        \n","        #   Fitness[i]=self.objf(S[i,:],self.trainInput,self.trainOutput,self.dim)\n","        \n","\n","        # print(Fitness)\n","        # fmin = min(Fitness) #fprey\n","        # print(fmin)\n","        #####################################################################################\n","          \n","\n","        for epoch in range(self.epoch):\n","            #Update the decreasing factor  using (3).\n","            alpha=C*exp(-epoch/self.epoch)   #density factor in Eq. (3)\n","\n","            dim_list = array(list(range(1, self.problem_size + 1)))\n","            miu = 0.00565\n","            r0 = 10\n","            r = r0 + miu * dim_list\n","            w = 0.005\n","            phi0 = 3 * pi / 2\n","            phi = -w * dim_list + phi0\n","            x = r * sin(phi)           # Eq.(9)\n","            y = r * cos(phi)           # Eq.(10)\n","            # QF = (epoch+1) ** ((2 * rand() - 1) / (1 - self.epoch) ** 2)   # Eq.(15)        Quality function\n","\n","            for i in range(0, self.pop_size):\n","                #Calculate the intensity Ii using Eq. (2).\n","                I=LVHBAClass.Intensity(self.pop_size,g_best,pop) #intensity in Eq. (2)\n","                \n","                r =random.random()\n","                F=vec_flag[math.floor(2*random.random())]\n","                pos_new = pop[i][self.ID_POS]\n","\n","                LF=self.get_simple_levy_step()\n","                for j in range(0, self.problem_size):\n","                                     \n","                    di=np.array(g_best)-np.array(pop[i-1,:])\n","\n","                    if r<.5:\n","                        r3=random.random()\n","                        r4=random.random()\n","                        r5=random.random()\n","                        \n","                        x1=g_best[self.ID_POS][j] +F*beta*I[self.ID_POS][j]*g_best[self.ID_POS][j]\n","                        x2=np.asarray(F)*r3*alpha*di*abs(cos(2*pi*r4)*(1-math.cos(2*math.pi*r5)))\n","                        \n","                        Z=x1+x2\n","                        pos_new[j]=Z[1,]\n","    \n","                    else:\n","\n","                        idx = choice(list(set(range(0, self.pop_size)) - {i}))   \n","                        pos_new[j] = g_best[self.ID_POS][j] * LF[j] + pop[idx][self.ID_POS][j] + rand() * (y[j] - x[j])          # Eq. 5\n","\n","                        # r7=random.random()\n","                        # Z=g_best[self.ID_POS][j]+F*r7*alpha*di\n","                        # pos_new[j]=Z[1,]\n","                \n","                fit_new, pos_new_binary = self.get_fitness_position(pos_new)\n","                \n","                # #######################################################################################\n","                \n","                # # Update if the solution improves\n","                if fit_new < pop[i][self.ID_FIT]:\n","                    #Set xi = xnew and fi = fnew.\n","                    pop[i][self.ID_POS] = pos_new_binary\n","                    pop[i][self.ID_FIT] = fit_new\n","                \n","                # Update the current best solution\n","                if fit_new<=fmin:\n","                    #Set xprey = xnew and fprey = fnew.\n","                    g_best[self.ID_POS]=pos_new_binary\n","                    fmin=fit_new\n","\n","                #######################################################################################\n","                fit_new=fmin\n","                pop[i] = [pos_new, fit_new, pos_new_binary]\n","            \n","            ## Update global best and global worst\n","            g_best, g_worst = self.update_global_best_global_worst_solution(pop, self.ID_MIN_PROB, self.ID_MAX_PROB, g_best)\n","\n","            # g_best = self.update_global_best_solution(pop, self.ID_MIN_PROB, g_best)\n","            self.loss_train.append(g_best[self.ID_FIT])\n","        #     if self.verbose:\n","        #         print(\"> Epoch: {}, Best fit: {}\".format(epoch + 1, g_best[self.ID_FIT]))\n","        # self.solution = g_best\n","        # return g_best[self.ID_POS], g_best[self.ID_FIT], self.loss_train\n","\n","            self.fs_counts.append(sum(g_best[self.ID_POS]))\n","            # if self.verbose:\n","            #     print(\"> Epoch: {}, Best fit: {}\".format(epoch + 1, g_best[self.ID_FIT]))\n","                \n","            if (epoch%1==0):\n","                \n","                print(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[self.ID_POS]))])\n","                self.logger.info(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[self.ID_POS]))])\n","          \n","        self.solution = g_best\n","        return g_best[self.ID_POS], g_best[self.ID_FIT], self.loss_train, self.fs_counts\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9hC0O53m1Qrv"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","# from .DYHGSClass import DYHGSClass\n","from numpy.random import uniform, normal, rand\n","\n","    \n","class lvhba:\n","\n","      def LVHBA(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","    \n","            #Max_iter=1000\n","            #lb=-100\n","            #ub=100\n","            #dim=30  \n","            #SearchAgents_no=5\n","            lb=0\n","            # initialize alpha, beta, and delta_pos\n","            g_best=numpy.zeros(dim)\n","            Alpha_score=float(\"inf\")\n","            XXX=1\n","            verbose = True\n","            # Loop counter\n","            print(\"LVHBA is optimizing  \\\"\"+objf.__name__+\"\\\"\")   \n","            LOGGER_FILE.info(\"LVHBA is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","            s=solution()\n","            timerStart=time.time() \n","            s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            lb = [lb] * dim\n","            ub = [ub] * dim\n","            # print(trainInput.shape)\n","\n","            best_pos1, best_fit1, list_loss1, fs_counts = LVHBAClass(objf, lb, ub, verbose, Max_iter, SearchAgents_no,0.08, 10000,trainInput=trainInput,trainOutput=trainOutput,dim=dim, logger = LOGGER_FILE).train()\n","            # print(md1)\n","            # best_pos1, best_fit1, list_loss1 = md1.train()\n","            \n","            timerEnd=time.time()  \n","            s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            s.executionTime=timerEnd-timerStart\n","            s.bestIndividual=best_pos1\n","            s.convergence1=list_loss1\n","            s.convergence2=fs_counts\n","            \n","            s.optimizer=\"LVHBA\"\n","            s.objfname=objf.__name__\n","            \n","            return s"]},{"cell_type":"markdown","metadata":{"id":"8fd-QWrzKCiH"},"source":["## 17- AHA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rZL251srKHs7"},"outputs":[],"source":["class Root:\n","    \"\"\" This is root of all Algorithms \"\"\"\n","\n","    ID_MIN_PROB = 0  # min problem\n","    ID_MAX_PROB = -1  # max problem\n","\n","    ID_POS = 0  # Position\n","    ID_FIT = 1  # Fitness\n","\n","    EPSILON = 10E-10    # Avoid division by 0 \n","\n","    DEFAULT_LB = -1\n","    DEFAULT_UB = 1\n","\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True):\n","        self.verbose = verbose\n","        self.obj_func = obj_func\n","        if lb is None:\n","            print(\"Lower bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.lb = array(lb)\n","        if ub is None:\n","            print(\"Upper bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.ub = array(ub)\n","           \n","        if len(lb) != len(ub):\n","            print(\"Lower bound and Upper bound need to have the same length\")\n","            exit(0)\n","        \n","        self.problem_size = len(lb)\n","        self.solution, self.loss_train, self.fs_counts = None, [], []\n","\n","    def create_solution(self, minmax=0):\n","        \"\"\" Return solution with 2 element: position of solution and fitness of solution\n","        Parameters\n","        ----------\n","        minmax\n","            0 - minimum problem, else - maximum problem\n","        \"\"\"\n","        position = uniform(self.lb, self.ub)\n","        # print(position)\n","        # position=numpy.random.randint(2, size=(self.pop_size,self.dim))\n","        # for i in range(0,self.pop_size):\n","        #   while numpy.sum(position[i,:])==0:   \n","        #           position[i,:]=numpy.random.randint(2, size=(1,self.dim))\n","        fitness, binary_pos = self.get_fitness_position(position=position, minmax=minmax)\n","        return [position, fitness, binary_pos]\n","\n","    def get_fitness_position(self,position=None, minmax=0):\n","        \"\"\"     Assumption that objective function always return the original value\n","        :param position: 1-D numpy array\n","        :param minmax: 0- min problem, 1 - max problem\n","        :return:\n","        \"\"\"\n","        # print(position)\n","        Positions=position;\n","         \n","        \n","        for j in range(0,self.dim):\n","          ss= transfer_functions_benchmark.s1(position[j])\n","          if (random.random()<ss): \n","              Positions[j]=1;\n","          else:\n","              Positions[j]=0;\n","\n","        return self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) if minmax == 0 else 1.0 / (self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) + self.EPSILON), Positions     # Avoid division by 0\n","\n","    def get_fitness_solution(self,solution=None, minmax=0):\n","        return self.get_fitness_position(solution[self.ID_POS],self.trainInput,self.trainOutput,self.dim, minmax)\n","\n","    def get_global_best_global_worst_solution(self, pop=None, id_fit=None, id_best=None):\n","        sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n","        if id_best == self.ID_MIN_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MAX_PROB])\n","        elif id_best == self.ID_MAX_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MIN_PROB])\n","\n","\n","    def update_global_best_global_worst_solution(self, pop=None, id_best=None, id_worst=None, g_best=None):\n","        \"\"\" Sort the copy of population and update the current best position. Return the new current best position \"\"\"\n","        sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n","        current_best = sorted_pop[id_best]\n","        g_best = deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n","        return g_best, sorted_pop[id_worst]\n","\n","\n","    def amend_position(self, position=None):\n","        return clip(position, self.lb, self.ub)\n","\n","    def amend_position_random(self, position=None):\n","        return where(logical_and(self.lb <= position, position <= self.ub), position, uniform(self.lb, self.ub))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fd3jdqKaYeSE"},"outputs":[],"source":["def space_bound(X, Up, Low):\n","    dim = len(X)\n","    S = (X > Up) + (X < Low)\n","    res = (np.random.rand(dim) * (np.array(Up) - np.array(Low)) + np.array(Low)) * S + X * (~S)\n","    return res\n","\n","def flight(r, direct_vector, self):\n","  # Diagonal flight\n","  if r < 1 / 3:\n","      rand_dim = randperm(self.dim)\n","      if self.dim >= 3:\n","          rand_num = np.ceil(np.random.rand() * (self.dim - 2))\n","      else:\n","          rand_num = np.ceil(np.random.rand() * (self.dim - 1))\n","\n","      direct_vector[i, rand_dim[:int(rand_num)]] = 1\n","  \n","  # Omnidirectional flight\n","  elif r > 2 / 3:\n","      direct_vector[i, :] = 1\n","  else:\n","      # Axial flight\n","      rand_num = np.ceil(np.random.rand() * (self.dim - 1))\n","      direct_vector[i, int(rand_num)] = 1\n","\n","  return direct_vector[i, :]  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VAXYJN-fYKLS"},"outputs":[],"source":["from numpy.linalg import norm\n","from torch import randperm\n","\n","class AHAClass(Root):\n","\n","    def __init__(self, obj_func=None, lb=None, ub=None, verbose=True, epoch=750, pop_size=100,alpha=5, miu=0.5, moa_min=0.2, moa_max=0.9,trainInput=None,trainOutput=None,dim=None, logger=None):\n","        # super().__init__({\"obj_func\":obj_func, \"lb\":lb, \"ub\":ub, \"verbose\":verbose})#, \"trainInput\"=trainInput,\"trainOutput\"=trainOutput,\"dim\"=dim})\n","        super().__init__(obj_func,lb, ub, verbose)\n","        # print(trainInput.shape)\n","        self.epoch = epoch\n","        # self.pop_size = pop_size\n","        self.alpha = alpha          # Default: 5, fixed parameter, sensitive exploitation parameter\n","        self.objf=obj_func\n","        self.npop=pop_size\n","\n","        self.trainInput=trainInput\n","        self.dim=dim\n","        self.trainOutput=trainOutput\n","        self.logger = logger\n","\n","    def train(self):\n","      \n","        ###################################(add)#############################################\n","        #Initialize population with random positions\n","        pop = [self.create_solution() for _ in range(self.npop)]\n","        # #assign fitness to fprey.\n","        g_best, g_worst = self.get_global_best_global_worst_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","\n","        Sol=numpy.random.randint(2, size=(self.npop,self.dim))     #generating binary individuals\n","        pop_pos=numpy.zeros((self.npop,self.dim))  # Population size, dim\n","        pop_pos=numpy.copy(Sol)\n","        #Evaluate initial random solutions\n","        pop_fit=numpy.zeros(self.npop)\n","        \n","        for i in range(0,self.npop):    \n","            pop_fit[i]=self.objf(pop_pos[i,:],self.trainInput,self.trainOutput,self.dim) #fitness\n","        #####################################################################################\n","\n","        best_f = float('inf')\n","        best_x = []\n","        for i in range(self.npop):\n","            if pop_fit[i] <= best_f:\n","                best_f = pop_fit[i]\n","                best_x = pop_pos[i, :]\n","\n","        his_best_fit = np.zeros(self.epoch)\n","        visit_table = np.zeros((self.npop, self.npop))\n","        diag_ind = np.diag_indices(self.npop)\n","        visit_table[diag_ind] = float('nan')\n","        \n","        # for epoch in range(self.epoch):\n","        max_it=self.epoch\n","        for it in range(max_it):\n","            # Direction\n","            visit_table[diag_ind] = float('-inf')\n","            for i in range(self.npop):\n","                direct_vector = np.zeros((self.npop, self.dim))\n","                r = np.random.rand()\n","                # Diagonal flight\n","                if r < 1 / 3:\n","                    rand_dim = randperm(self.dim)\n","                    if self.dim >= 3:\n","                        rand_num = np.ceil(np.random.rand() * (self.dim - 2))\n","                    else:\n","                        rand_num = np.ceil(np.random.rand() * (self.dim - 1))\n","\n","                    direct_vector[i, rand_dim[:int(rand_num)]] = 1\n","                \n","                # Omnidirectional flight\n","                elif r > 2 / 3:\n","                    direct_vector[i, :] = 1\n","                else:\n","                    # Axial flight\n","                    rand_num = np.ceil(np.random.rand() * (self.dim - 1))\n","                    direct_vector[i, int(rand_num)] = 1\n","                \n","                \n","                # Guided foraging\n","                if np.random.rand() < 0.5: # Exploration phase\n","                    MaxUnvisitedTime = max(visit_table[i, :])\n","                    TargetFoodIndex = visit_table[i, :].argmax()\n","                    MUT_Index = np.where(visit_table[i, :] == MaxUnvisitedTime)\n","                    if len(MUT_Index[0]) > 1:\n","                        Ind = pop_fit[MUT_Index].argmin()\n","                        TargetFoodIndex = MUT_Index[0][Ind]\n","\n","                    newPopPos = pop_pos[TargetFoodIndex, :] + np.random.randn() * direct_vector[i, :] * (pop_pos[i, :] - pop_pos[TargetFoodIndex, :])\n","                    # newPopPos = space_bound(newPopPos, self.ub, self.lb)                  \n","\n","                    newPopFit, pos_new_binary = self.get_fitness_position(newPopPos)\n","\n","                    if newPopFit < pop_fit[i]:\n","                        pop_fit[i] = newPopFit\n","                        pop_pos[i, :] = newPopPos\n","\n","                        visit_table[i, :] += 1\n","                        visit_table[i, TargetFoodIndex] = 0\n","                        visit_table[:, i] = np.max(visit_table, axis=1) + 1\n","                        visit_table[i, i] = float('-inf')\n","                    else:\n","                        visit_table[i, :] += 1\n","                        visit_table[i, TargetFoodIndex] = 0\n","\n","                else:     # Exploitation phase\n","\n","                    # Territorial foraging\n","                    newPopPos = pop_pos[i, :] + np.random.randn() * direct_vector[i, :] * pop_pos[i, :]                   \n","                    # newPopPos = space_bound(newPopPos, self.ub, self.lb)                  \n","\n","                    newPopFit, pos_new_binary = self.get_fitness_position(newPopPos)\n","                    \n","                    if newPopFit < pop_fit[i]:\n","                        pop_fit[i] = newPopFit\n","                        pop_pos[i, :] = newPopPos\n","                        visit_table[i, :] += 1\n","                        visit_table[:, i] = np.max(visit_table, axis=1) + 1\n","                        visit_table[i, i] = float('-inf')\n","                    else:\n","                        visit_table[i, :] += 1\n","            \n","            visit_table[diag_ind] = float('nan')\n","            \n","\n","            # Migration foraging\n","            # if np.mod(it, 2 * self.npop) == 0:\n","            #     visit_table[diag_ind] = float('-inf')\n","            #     MigrationIndex = pop_fit.argmax()\n","                \n","            #     S[MigrationIndex, :] = np.random.rand(self.dim) * (np.array(self.ub) - np.array(self.lb)) + np.array(self.lb)\n","            #     visit_table[MigrationIndex, :] += 1\n","            #     visit_table[:, MigrationIndex] = np.max(visit_table, axis=1) + 1\n","            #     visit_table[MigrationIndex, MigrationIndex] = float('-inf')\n","                \n","            #     # pop_fit[MigrationIndex] = ben_functions(pop_pos[MigrationIndex, :], fun_index)\n","            #     print(S[MigrationIndex,:])\n","            #     if numpy.sum(S[MigrationIndex,:])==0:   \n","            #           S[MigrationIndex,:]=numpy.random.randint(2, size=(1,self.dim))\n","                \n","            #     pop_fit[MigrationIndex]=self.objf(S[MigrationIndex,:],self.trainInput,self.trainOutput,self.dim) #fitness\n","            #     visit_table[diag_ind] = float('nan')\n","\n","            for i in range(self.npop):\n","                if pop_fit[i] < best_f:\n","                    best_f = pop_fit[i]\n","                    best_x = pop_pos[i, :]\n","            \n","            # his_best_fit[it] = best_f          \n","\n","                #####################################################################################\n","                # fit_new, pos_new_binary = self.get_fitness_position(best_x)\n","            # fit_new, pos_new_binary = self.get_fitness_position(best_x)\n","            pop[it] = [best_x, best_f, pos_new_binary]\n","            \n","            g_best, g_worst = self.update_global_best_global_worst_solution(pop, self.ID_MIN_PROB, self.ID_MAX_PROB, g_best)\n","\n","            self.loss_train.append(g_best[self.ID_FIT])\n","            self.fs_counts.append(sum(g_best[self.ID_POS]))\n","                \n","            if (it%1==0):\n","                \n","                print(['At iteration'+ str(it+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[self.ID_POS]))])\n","                self.logger.info(['At iteration'+ str(it+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[self.ID_POS]))])\n","        \n","        self.solution = g_best\n","        return g_best[self.ID_POS], g_best[self.ID_FIT], self.loss_train, self.fs_counts\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bqU3j9ziKVIH"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","# from .DYHGSClass import DYHGSClass\n","from numpy.random import uniform, normal, rand\n","\n","    \n","class aha:\n","\n","      def AHA(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","    \n","            #Max_iter=1000\n","            #lb=-100\n","            #ub=100\n","            #dim=30  \n","            #SearchAgents_no=5\n","            lb=0\n","            # initialize alpha, beta, and delta_pos\n","            g_best=numpy.zeros(dim)\n","            Alpha_score=float(\"inf\")\n","            XXX=1\n","            verbose = True\n","            # Loop counter\n","            print(\"AHA is optimizing  \\\"\"+objf.__name__+\"\\\"\")   \n","            LOGGER_FILE.info(\"AHA is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","            s=solution()\n","            timerStart=time.time() \n","            s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            lb = [lb] * dim\n","            ub = [ub] * dim\n","            # print(trainInput.shape)\n","\n","            best_pos1, best_fit1, list_loss1, fs_counts = AHAClass(objf, lb, ub, verbose, Max_iter, SearchAgents_no,0.08, 10000,trainInput=trainInput,trainOutput=trainOutput,dim=dim, logger = LOGGER_FILE).train()\n","            # print(md1)\n","            # best_pos1, best_fit1, list_loss1 = md1.train()\n","            \n","            timerEnd=time.time()  \n","            s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            s.executionTime=timerEnd-timerStart\n","            s.bestIndividual=best_pos1\n","            s.convergence1=list_loss1\n","            s.convergence2=fs_counts\n","            \n","            s.optimizer=\"AHA\"\n","            s.objfname=objf.__name__\n","            \n","            return s"]},{"cell_type":"markdown","metadata":{"id":"Rw7Sk_vjkTr3"},"source":["## 18-ARAHA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"04NDCOdXkZCq"},"outputs":[],"source":["class Root:\n","    \"\"\" This is root of all Algorithms \"\"\"\n","\n","    ID_MIN_PROB = 0  # min problem\n","    ID_MAX_PROB = -1  # max problem\n","\n","    ID_POS = 0  # Position\n","    ID_FIT = 1  # Fitness\n","\n","    EPSILON = 10E-10    # Avoid division by 0 \n","\n","    DEFAULT_LB = -1\n","    DEFAULT_UB = 1\n","\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True):\n","        self.verbose = verbose\n","        self.obj_func = obj_func\n","        if lb is None:\n","            print(\"Lower bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.lb = array(lb)\n","        if ub is None:\n","            print(\"Upper bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.ub = array(ub)\n","           \n","        if len(lb) != len(ub):\n","            print(\"Lower bound and Upper bound need to have the same length\")\n","            exit(0)\n","        \n","        self.problem_size = len(lb)\n","        self.solution, self.loss_train, self.fs_counts = None, [], []\n","\n","    def create_solution(self, minmax=0):\n","        \"\"\" Return solution with 2 element: position of solution and fitness of solution\n","        Parameters\n","        ----------\n","        minmax\n","            0 - minimum problem, else - maximum problem\n","        \"\"\"\n","        position = uniform(self.lb, self.ub)\n","        # print(position)\n","        # position=numpy.random.randint(2, size=(self.pop_size,self.dim))\n","        # for i in range(0,self.pop_size):\n","        #   while numpy.sum(position[i,:])==0:   \n","        #           position[i,:]=numpy.random.randint(2, size=(1,self.dim))\n","        fitness, binary_pos = self.get_fitness_position(position=position, minmax=minmax)\n","        return [position, fitness, binary_pos]\n","\n","    def get_fitness_position(self,position=None, minmax=0):\n","        \"\"\"     Assumption that objective function always return the original value\n","        :param position: 1-D numpy array\n","        :param minmax: 0- min problem, 1 - max problem\n","        :return:\n","        \"\"\"\n","        # print(position)\n","        Positions=position;\n","         \n","        \n","        for j in range(0,self.dim):\n","          ss= transfer_functions_benchmark.s1(position[j])\n","          if (random.random()<ss): \n","              Positions[j]=1;\n","          else:\n","              Positions[j]=0;\n","\n","        return self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) if minmax == 0 else 1.0 / (self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) + self.EPSILON), Positions     # Avoid division by 0\n","\n","    def get_fitness_solution(self,solution=None, minmax=0):\n","        return self.get_fitness_position(solution[self.ID_POS],self.trainInput,self.trainOutput,self.dim, minmax)\n","\n","    def get_global_best_global_worst_solution(self, pop=None, id_fit=None, id_best=None):\n","        sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n","        if id_best == self.ID_MIN_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MAX_PROB])\n","        elif id_best == self.ID_MAX_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MIN_PROB])\n","\n","\n","    def update_global_best_global_worst_solution(self, pop=None, id_best=None, id_worst=None, g_best=None):\n","        \"\"\" Sort the copy of population and update the current best position. Return the new current best position \"\"\"\n","        sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n","        current_best = sorted_pop[id_best]\n","        g_best = deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n","        return g_best, sorted_pop[id_worst]\n","\n","\n","    def amend_position(self, position=None):\n","        return clip(position, self.lb, self.ub)\n","\n","    def amend_position_random(self, position=None):\n","        return where(logical_and(self.lb <= position, position <= self.ub), position, uniform(self.lb, self.ub))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k-rwXQ0skZC1"},"outputs":[],"source":["from numpy.linalg import norm\n","from torch import randperm\n","\n","class ARAHAClass(Root):\n","\n","    def __init__(self, obj_func=None, lb=None, ub=None, verbose=True, epoch=750, pop_size=100,alpha=5, miu=0.5, moa_min=0.2, moa_max=0.9,trainInput=None,trainOutput=None,dim=None, logger=None):\n","        # super().__init__({\"obj_func\":obj_func, \"lb\":lb, \"ub\":ub, \"verbose\":verbose})#, \"trainInput\"=trainInput,\"trainOutput\"=trainOutput,\"dim\"=dim})\n","        super().__init__(obj_func,lb, ub, verbose)\n","        \n","\n","        miu=0.5\n","        moa_min=0.2\n","        moa_max=0.9\n","        self.alpha = alpha          # Default: 5, fixed parameter, sensitive exploitation parameter\n","        self.miu = miu              # Default: 0.5, fixed parameter , control parameter to adjust the search process\n","        self.moa_min = moa_min      # Default: 0.2, range min of Math Optimizer Accelerated\n","        self.moa_max = moa_max      # Default: 0.9, range max of Math Optimizer Accelerated\n","\n","        self.epoch = epoch\n","        # self.pop_size = pop_size\n","        self.objf=obj_func\n","        self.npop=pop_size\n","        \n","        self.trainInput=trainInput\n","        self.dim=dim\n","        self.trainOutput=trainOutput\n","        self.logger = logger\n","\n","    def train(self):\n","      \n","        ###################################(add)#############################################\n","        #Initialize population with random positions\n","        pop = [self.create_solution() for _ in range(self.npop)]\n","        # #assign fitness to fprey.\n","        g_best, g_worst = self.get_global_best_global_worst_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","\n","        Sol=numpy.random.randint(2, size=(self.npop,self.dim))     #generating binary individuals\n","        pop_pos=numpy.zeros((self.npop,self.dim))  # Population size, dim\n","        pop_pos=numpy.copy(Sol)\n","        #Evaluate initial random solutions\n","        pop_fit=numpy.zeros(self.npop)\n","        \n","        for i in range(0,self.npop):    \n","            pop_fit[i]=self.objf(pop_pos[i,:],self.trainInput,self.trainOutput,self.dim) #fitness\n","        #####################################################################################\n","\n","        best_f = float('inf')\n","        best_x = []\n","        for i in range(self.npop):\n","            if pop_fit[i] <= best_f:\n","                best_f = pop_fit[i]\n","                best_x = pop_pos[i, :]\n","\n","        \n","        his_best_fit = np.zeros(self.epoch)\n","        visit_table = np.zeros((self.npop, self.npop))\n","        diag_ind = np.diag_indices(self.npop)\n","        visit_table[diag_ind] = float('nan')\n","        \n","        # for epoch in range(self.epoch):\n","        max_it=self.epoch\n","\n","        for it in range(max_it):\n","            # moa = self.moa_min + it * ((self.moa_max - self.moa_min) / self.epoch)           # Eq. 2\n","            mop = 1 - (it ** (1.0 / self.alpha)) / (self.epoch ** (1.0 / self.alpha))        # Eq. 4\n","            r1, r2, r3 = rand(3)\n","\n","            # Direction\n","            visit_table[diag_ind] = float('-inf')\n","            for i in range(self.npop):\n","                direct_vector = np.zeros((self.npop, self.dim))\n","                r = np.random.rand()\n","                # Diagonal flight\n","                if r < 1 / 3:\n","                    rand_dim = randperm(self.dim)\n","                    if self.dim >= 3:\n","                        rand_num = np.ceil(np.random.rand() * (self.dim - 2))\n","                    else:\n","                        rand_num = np.ceil(np.random.rand() * (self.dim - 1))\n","\n","                    direct_vector[i, rand_dim[:int(rand_num)]] = 1\n","                \n","                # Omnidirectional flight\n","                elif r > 2 / 3:\n","                    direct_vector[i, :] = 1\n","                else:\n","                    # Axial flight\n","                    rand_num = np.ceil(np.random.rand() * (self.dim - 1))\n","                    direct_vector[i, int(rand_num)] = 1\n","                \n","                # Guided foraging\n","                if np.random.rand() < 0.5: # Exploration phase\n","                    MaxUnvisitedTime = max(visit_table[i, :])\n","                    TargetFoodIndex = visit_table[i, :].argmax()\n","                    MUT_Index = np.where(visit_table[i, :] == MaxUnvisitedTime)\n","                    if len(MUT_Index[0]) > 1:\n","                        Ind = pop_fit[MUT_Index].argmin()\n","                        TargetFoodIndex = MUT_Index[0][Ind]\n","\n","                    newPopPos = pop_pos[TargetFoodIndex, :] + np.random.randn() * direct_vector[i, :] * (pop_pos[i, :] - pop_pos[TargetFoodIndex, :])\n","                    newPopPos = space_bound(newPopPos, self.ub, self.lb)                  \n","\n","                    newPopFit, pos_new_binary = self.get_fitness_position(newPopPos)\n","\n","                    if newPopFit < pop_fit[i]:\n","                        pop_fit[i] = newPopFit\n","                        pop_pos[i, :] = newPopPos\n","\n","                        visit_table[i, :] += 1\n","                        visit_table[i, TargetFoodIndex] = 0\n","                        visit_table[:, i] = np.max(visit_table, axis=1) + 1\n","                        visit_table[i, i] = float('-inf')\n","                    else:\n","                        visit_table[i, :] += 1\n","                        visit_table[i, TargetFoodIndex] = 0\n","\n","                else:     # Exploitation phase\n","\n","                    # Territorial foraging\n","                    # newPopPos = pop_pos[i, :] + np.random.randn() * direct_vector[i, :] * pop_pos[i, :]  \n","\n","                    if r3 < 0.5:\n","                        newPopPos = pop_pos[i, :] - mop * ((self.ub - self.lb) * self.miu + self.lb)\n","                    else:\n","                        newPopPos = pop_pos[i, :] + mop * ((self.ub - self.lb) * self.miu + self.lb)\n","\n","                    # newPopPos = space_bound(newPopPos, self.ub, self.lb)                  \n","\n","                    newPopFit, pos_new_binary = self.get_fitness_position(newPopPos)\n","                    \n","                    if newPopFit < pop_fit[i]:\n","                        pop_fit[i] = newPopFit\n","                        pop_pos[i, :] = newPopPos\n","                        visit_table[i, :] += 1\n","                        visit_table[:, i] = np.max(visit_table, axis=1) + 1\n","                        visit_table[i, i] = float('-inf')\n","                    else:\n","                        visit_table[i, :] += 1\n","            \n","            visit_table[diag_ind] = float('nan')\n","            \n","\n","            # Migration foraging\n","            if np.mod(it, 2 * self.npop) == 0:\n","                visit_table[diag_ind] = float('-inf')\n","                MigrationIndex = pop_fit.argmax()\n","                \n","                pop_pos[MigrationIndex, :] = np.random.rand(self.dim) * (np.array(self.ub) - np.array(self.lb)) + np.array(self.lb)\n","                visit_table[MigrationIndex, :] += 1\n","                visit_table[:, MigrationIndex] = np.max(visit_table, axis=1) + 1\n","                visit_table[MigrationIndex, MigrationIndex] = float('-inf')\n","                \n","                # pop_fit[MigrationIndex] = ben_functions(pop_pos[MigrationIndex, :], fun_index)\n","                # print(pop_pos[MigrationIndex,:])\n","                if numpy.sum(pop_pos[MigrationIndex,:])==0:   \n","                      pop_pos[MigrationIndex,:]=numpy.random.randint(2, size=(1,self.dim))\n","                \n","                pop_fit[MigrationIndex]=self.objf(pop_pos[MigrationIndex,:],self.trainInput,self.trainOutput,self.dim) #fitness\n","                visit_table[diag_ind] = float('nan')\n","\n","            for i in range(self.npop):\n","                if pop_fit[i] < best_f:\n","                    best_f = pop_fit[i]\n","                    best_x = pop_pos[i, :]\n","                pop[i] = [best_x, best_f, pos_new_binary]            \n","\n","            his_best_fit[it] = best_f          \n","\n","                #####################################################################################\n","                \n","            # pop[i] = [best_x, best_f, pos_new_binary]\n","            \n","            g_best, g_worst = self.update_global_best_global_worst_solution(pop, self.ID_MIN_PROB, self.ID_MAX_PROB, g_best)\n","            \n","            for i in range(self.npop):\n","                pop_pos[i, :]=g_best[self.ID_POS]\n","                pop_fit[i]=g_best[self.ID_FIT]\n","\n","            self.loss_train.append(g_best[self.ID_FIT])\n","            self.fs_counts.append(sum(g_best[self.ID_POS]))\n","                \n","            if (it%1==0):\n","                \n","                print(['At iteration'+ str(it+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[self.ID_POS]))])\n","                self.logger.info(['At iteration'+ str(it+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[self.ID_POS]))])\n","        \n","        self.solution = g_best\n","        return g_best[self.ID_POS], g_best[self.ID_FIT], self.loss_train, self.fs_counts\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LcxrzzrwkZC2"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","# from .DYHGSClass import DYHGSClass\n","from numpy.random import uniform, normal, rand\n","\n","    \n","class araha:\n","\n","      def ARAHA(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","    \n","            #Max_iter=1000\n","            #lb=-100\n","            #ub=100\n","            #dim=30  \n","            #SearchAgents_no=5\n","            lb=0\n","            # initialize alpha, beta, and delta_pos\n","            g_best=numpy.zeros(dim)\n","            Alpha_score=float(\"inf\")\n","            XXX=1\n","            verbose = True\n","            # Loop counter\n","            print(\"ARAHA is optimizing  \\\"\"+objf.__name__+\"\\\"\")   \n","            LOGGER_FILE.info(\"ARAHA is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","            s=solution()\n","            timerStart=time.time() \n","            s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            lb = [lb] * dim\n","            ub = [ub] * dim\n","            # print(trainInput.shape)\n","\n","            best_pos1, best_fit1, list_loss1, fs_counts = ARAHAClass(objf, lb, ub, verbose, Max_iter, SearchAgents_no,0.08, 10000,trainInput=trainInput,trainOutput=trainOutput,dim=dim, logger = LOGGER_FILE).train()\n","            # print(md1)\n","            # best_pos1, best_fit1, list_loss1 = md1.train()\n","            \n","            timerEnd=time.time()  \n","            s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            s.executionTime=timerEnd-timerStart\n","            s.bestIndividual=best_pos1\n","            s.convergence1=list_loss1\n","            s.convergence2=fs_counts\n","            \n","            s.optimizer=\"ARAHA\"\n","            s.objfname=objf.__name__\n","            \n","            return s"]},{"cell_type":"markdown","metadata":{"id":"zmvd7o_9XQQk"},"source":["## 19-AQAHA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4TFBIRmtXQQk"},"outputs":[],"source":["class Root:\n","    \"\"\" This is root of all Algorithms \"\"\"\n","\n","    ID_MIN_PROB = 0  # min problem\n","    ID_MAX_PROB = -1  # max problem\n","\n","    ID_POS = 0  # Position\n","    ID_FIT = 1  # Fitness\n","\n","    EPSILON = 10E-10    # Avoid division by 0 \n","\n","    DEFAULT_LB = -1\n","    DEFAULT_UB = 1\n","\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True):\n","        self.verbose = verbose\n","        self.obj_func = obj_func\n","        if lb is None:\n","            print(\"Lower bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.lb = array(lb)\n","        if ub is None:\n","            print(\"Upper bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.ub = array(ub)\n","           \n","        if len(lb) != len(ub):\n","            print(\"Lower bound and Upper bound need to have the same length\")\n","            exit(0)\n","        \n","        self.problem_size = len(lb)\n","        self.solution, self.loss_train, self.fs_counts = None, [], []\n","\n","    def create_solution(self, minmax=0):\n","        \"\"\" Return solution with 2 element: position of solution and fitness of solution\n","        Parameters\n","        ----------\n","        minmax\n","            0 - minimum problem, else - maximum problem\n","        \"\"\"\n","        position = uniform(self.lb, self.ub)\n","        # print(position)\n","        # position=numpy.random.randint(2, size=(self.pop_size,self.dim))\n","        # for i in range(0,self.pop_size):\n","        #   while numpy.sum(position[i,:])==0:   \n","        #           position[i,:]=numpy.random.randint(2, size=(1,self.dim))\n","        fitness, binary_pos = self.get_fitness_position(position=position, minmax=minmax)\n","        return [position, fitness, binary_pos]\n","\n","    def get_fitness_position(self,position=None, minmax=0):\n","        \"\"\"     Assumption that objective function always return the original value\n","        :param position: 1-D numpy array\n","        :param minmax: 0- min problem, 1 - max problem\n","        :return:\n","        \"\"\"\n","        # print(position)\n","        Positions=position;\n","         \n","        \n","        for j in range(0,self.dim):\n","          ss= transfer_functions_benchmark.s1(position[j])\n","          if (random.random()<ss): \n","              Positions[j]=1;\n","          else:\n","              Positions[j]=0;\n","\n","        return self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) if minmax == 0 else 1.0 / (self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) + self.EPSILON), Positions     # Avoid division by 0\n","\n","    def get_fitness_solution(self,solution=None, minmax=0):\n","        return self.get_fitness_position(solution[self.ID_POS],self.trainInput,self.trainOutput,self.dim, minmax)\n","\n","    def get_global_best_global_worst_solution(self, pop=None, id_fit=None, id_best=None):\n","        sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n","        if id_best == self.ID_MIN_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MAX_PROB])\n","        elif id_best == self.ID_MAX_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MIN_PROB])\n","\n","\n","    def update_global_best_global_worst_solution(self, pop=None, id_best=None, id_worst=None, g_best=None):\n","        \"\"\" Sort the copy of population and update the current best position. Return the new current best position \"\"\"\n","        sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n","        current_best = sorted_pop[id_best]\n","        g_best = deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n","        return g_best, sorted_pop[id_worst]\n","\n","\n","    def amend_position(self, position=None):\n","        return clip(position, self.lb, self.ub)\n","\n","    def amend_position_random(self, position=None):\n","        return where(logical_and(self.lb <= position, position <= self.ub), position, uniform(self.lb, self.ub))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jGL9oiFCXQQm"},"outputs":[],"source":["from numpy.linalg import norm\n","from torch import randperm\n","\n","class AQAHAClass(Root):\n","\n","    def __init__(self, obj_func=None, lb=None, ub=None, verbose=True, epoch=750, pop_size=100,alpha=5, miu=0.5, moa_min=0.2, moa_max=0.9,trainInput=None,trainOutput=None,dim=None, logger=None):\n","        # super().__init__({\"obj_func\":obj_func, \"lb\":lb, \"ub\":ub, \"verbose\":verbose})#, \"trainInput\"=trainInput,\"trainOutput\"=trainOutput,\"dim\"=dim})\n","        super().__init__(obj_func,lb, ub, verbose)\n","        \n","        miu=0.5\n","        moa_min=0.2\n","        moa_max=0.9\n","        self.alpha = alpha          # Default: 5, fixed parameter, sensitive exploitation parameter\n","        self.miu = miu              # Default: 0.5, fixed parameter , control parameter to adjust the search process\n","        self.moa_min = moa_min      # Default: 0.2, range min of Math Optimizer Accelerated\n","        self.moa_max = moa_max      # Default: 0.9, range max of Math Optimizer Accelerated\n","\n","        self.epoch = epoch\n","        # self.pop_size = pop_size\n","        self.objf=obj_func\n","        self.npop=pop_size\n","        \n","        self.trainInput=trainInput\n","        self.dim=dim\n","        self.trainOutput=trainOutput\n","        self.logger = logger\n","\n","    def get_simple_levy_step(self):\n","        beta = 1.5\n","        sigma = (gamma(1 + beta) * sin(pi * beta / 2) / (gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)\n","        u = normal(0, 1, self.problem_size) * sigma\n","        v = normal(1, self.problem_size)\n","        step = u / abs(v) ** (1 / beta)\n","        return step\n","\n","    def train(self):\n","        alpha = 0.1\n","        delta = 0.1\n","        \n","        ###################################(add)#############################################\n","        #Initialize population with random positions\n","        pop = [self.create_solution() for _ in range(self.npop)]\n","        # #assign fitness to fprey.\n","        g_best, g_worst = self.get_global_best_global_worst_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","\n","        Sol=numpy.random.randint(2, size=(self.npop,self.dim))     #generating binary individuals\n","        pop_pos=numpy.zeros((self.npop,self.dim))  # Population size, dim\n","        pop_pos=numpy.copy(Sol)\n","        #Evaluate initial random solutions\n","        pop_fit=numpy.zeros(self.npop)\n","        \n","        for i in range(0,self.npop):    \n","            pop_fit[i]=self.objf(pop_pos[i,:],self.trainInput,self.trainOutput,self.dim) #fitness\n","        #####################################################################################\n","\n","        best_f = float('inf')\n","        best_x = []\n","        for i in range(self.npop):\n","            if pop_fit[i] <= best_f:\n","                best_f = pop_fit[i]\n","                best_x = pop_pos[i, :]\n","\n","        \n","        his_best_fit = np.zeros(self.epoch)\n","        visit_table = np.zeros((self.npop, self.npop))\n","        diag_ind = np.diag_indices(self.npop)\n","        visit_table[diag_ind] = float('nan')\n","        \n","        # for epoch in range(self.epoch):\n","        max_it=self.epoch\n","\n","        for it in range(max_it):\n","            g1 = 2 * rand() - 1                 # Eq. 16\n","            g2 = 2 * (1 - it / self.epoch)   # Eq. 17\n","\n","            dim_list = array(list(range(1, self.npop + 1)))\n","            miu = 0.00565\n","            r0 = 10\n","            r = r0 + miu * dim_list\n","            w = 0.005\n","            phi0 = 3 * pi / 2\n","            phi = -w * dim_list + phi0\n","            x = r * sin(phi)           # Eq.(9)\n","            y = r * cos(phi)           # Eq.(10)\n","            QF = (it+1) ** ((2 * rand() - 1) / (1 - self.epoch) ** 2)   # Eq.(15)        Quality function\n","\n","            # Direction\n","            visit_table[diag_ind] = float('-inf')\n","            for i in range(self.npop):\n","                direct_vector = np.zeros((self.npop, self.dim))\n","                r = np.random.rand()\n","                # Diagonal flight\n","                if r < 1 / 3:\n","                    rand_dim = randperm(self.dim)\n","                    if self.dim >= 3:\n","                        rand_num = np.ceil(np.random.rand() * (self.dim - 2))\n","                    else:\n","                        rand_num = np.ceil(np.random.rand() * (self.dim - 1))\n","\n","                    direct_vector[i, rand_dim[:int(rand_num)]] = 1\n","                \n","                # Omnidirectional flight\n","                elif r > 2 / 3:\n","                    direct_vector[i, :] = 1\n","                else:\n","                    # Axial flight\n","                    rand_num = np.ceil(np.random.rand() * (self.dim - 1))\n","                    direct_vector[i, int(rand_num)] = 1\n","                \n","                # Guided foraging\n","                if np.random.rand() < 0.5: # Exploration phase\n","                \n","                    MaxUnvisitedTime = max(visit_table[i, :])\n","                    TargetFoodIndex = visit_table[i, :].argmax()\n","                    MUT_Index = np.where(visit_table[i, :] == MaxUnvisitedTime)\n","                    if len(MUT_Index[0]) > 1:\n","                        Ind = pop_fit[MUT_Index].argmin()\n","                        TargetFoodIndex = MUT_Index[0][Ind]\n","\n","                    newPopPos = pop_pos[TargetFoodIndex, :] + np.random.randn() * direct_vector[i, :] * (pop_pos[i, :] - pop_pos[TargetFoodIndex, :])\n","                    newPopPos = space_bound(newPopPos, self.ub, self.lb)                  \n","\n","                    newPopFit, pos_new_binary = self.get_fitness_position(newPopPos)\n","\n","                    if newPopFit < pop_fit[i]:\n","                        pop_fit[i] = newPopFit\n","                        pop_pos[i, :] = newPopPos\n","\n","                        visit_table[i, :] += 1\n","                        visit_table[i, TargetFoodIndex] = 0\n","                        visit_table[:, i] = np.max(visit_table, axis=1) + 1\n","                        visit_table[i, i] = float('-inf')\n","                    else:\n","                        visit_table[i, :] += 1\n","                        visit_table[i, TargetFoodIndex] = 0\n","\n","                else:     # Exploitation phase\n","\n","                    # Territorial foraging\n","                    # newPopPos = pop_pos[i, :] + np.random.randn() * direct_vector[i, :] * pop_pos[i, :]  \n","\n","                    for i in range(self.npop):\n","                        x_mean = mean(array([item[self.ID_FIT] for item in pop]), axis=0)\n","                        if rand() < 0.5:\n","                            newPopPos = alpha * (pop_pos[i, :] - x_mean) - rand() * (rand() * (self.ub - self.lb) + self.lb) * delta    # Eq. 13\n","                        else:\n","                            newPopPos = QF * pop_pos[i, :] - (g2 * pop[i][self.ID_POS] * rand()) - g2 * self.get_simple_levy_step() + rand() * g1   # Eq. 14\n","                    # newPopPos = space_bound(newPopPos, self.ub, self.lb)                  \n","\n","                    newPopFit, pos_new_binary = self.get_fitness_position(newPopPos)\n","                    \n","                    if newPopFit < pop_fit[i]:\n","                        pop_fit[i] = newPopFit\n","                        pop_pos[i, :] = newPopPos\n","                        visit_table[i, :] += 1\n","                        visit_table[:, i] = np.max(visit_table, axis=1) + 1\n","                        visit_table[i, i] = float('-inf')\n","                    else:\n","                        visit_table[i, :] += 1\n","            \n","            visit_table[diag_ind] = float('nan')\n","            \n","\n","            # Migration foraging\n","            if np.mod(it, 2 * self.npop) == 0:\n","                visit_table[diag_ind] = float('-inf')\n","                MigrationIndex = pop_fit.argmax()\n","                \n","                pop_pos[MigrationIndex, :] = np.random.rand(self.dim) * (np.array(self.ub) - np.array(self.lb)) + np.array(self.lb)\n","                visit_table[MigrationIndex, :] += 1\n","                visit_table[:, MigrationIndex] = np.max(visit_table, axis=1) + 1\n","                visit_table[MigrationIndex, MigrationIndex] = float('-inf')\n","                \n","                # pop_fit[MigrationIndex] = ben_functions(pop_pos[MigrationIndex, :], fun_index)\n","                # print(pop_pos[MigrationIndex,:])\n","                if numpy.sum(pop_pos[MigrationIndex,:])==0:   \n","                      pop_pos[MigrationIndex,:]=numpy.random.randint(2, size=(1,self.dim))\n","                \n","                pop_fit[MigrationIndex]=self.objf(pop_pos[MigrationIndex,:],self.trainInput,self.trainOutput,self.dim) #fitness\n","                visit_table[diag_ind] = float('nan')\n","\n","            for i in range(self.npop):\n","                if pop_fit[i] < best_f:\n","                    best_f = pop_fit[i]\n","                    best_x = pop_pos[i, :]\n","                pop[i] = [best_x, best_f, pos_new_binary]            \n","\n","            his_best_fit[it] = best_f          \n","\n","                #####################################################################################\n","                \n","            # pop[i] = [best_x, best_f, pos_new_binary]\n","            \n","            g_best, g_worst = self.update_global_best_global_worst_solution(pop, self.ID_MIN_PROB, self.ID_MAX_PROB, g_best)\n","            \n","            for i in range(self.npop):\n","                pop_pos[i, :]=g_best[self.ID_POS]\n","                pop_fit[i]=g_best[self.ID_FIT]\n","\n","            self.loss_train.append(g_best[self.ID_FIT])\n","            self.fs_counts.append(sum(g_best[self.ID_POS]))\n","                \n","            if (it%1==0):\n","                \n","                print(['At iteration'+ str(it+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[self.ID_POS]))])\n","                self.logger.info(['At iteration'+ str(it+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[self.ID_POS]))])\n","        \n","        self.solution = g_best\n","        return g_best[self.ID_POS], g_best[self.ID_FIT], self.loss_train, self.fs_counts\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TFET18VnXQQn"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","# from .DYHGSClass import DYHGSClass\n","from numpy.random import uniform, normal, rand\n","\n","    \n","class aqaha:\n","\n","      def AQAHA(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","    \n","            #Max_iter=1000\n","            #lb=-100\n","            #ub=100\n","            #dim=30  \n","            #SearchAgents_no=5\n","            lb=0\n","            # initialize alpha, beta, and delta_pos\n","            g_best=numpy.zeros(dim)\n","            Alpha_score=float(\"inf\")\n","            XXX=1\n","            verbose = True\n","            # Loop counter\n","            print(\"AQAHA is optimizing  \\\"\"+objf.__name__+\"\\\"\")   \n","            LOGGER_FILE.info(\"AQAHA is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","            s=solution()\n","            timerStart=time.time() \n","            s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            lb = [lb] * dim\n","            ub = [ub] * dim\n","            # print(trainInput.shape)\n","\n","            best_pos1, best_fit1, list_loss1, fs_counts = AQAHAClass(objf, lb, ub, verbose, Max_iter, SearchAgents_no,0.08, 10000,trainInput=trainInput,trainOutput=trainOutput,dim=dim, logger = LOGGER_FILE).train()\n","            # print(md1)\n","            # best_pos1, best_fit1, list_loss1 = md1.train()\n","            \n","            timerEnd=time.time()  \n","            s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            s.executionTime=timerEnd-timerStart\n","            s.bestIndividual=best_pos1\n","            s.convergence1=list_loss1\n","            s.convergence2=fs_counts\n","            \n","            s.optimizer=\"AQAHA\"\n","            s.objfname=objf.__name__\n","            \n","            return s"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sReAgGNvwbqZ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"KktsixtKvnS-"},"source":["## 20-SSA  \n","(Sparrow Search Algorithm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GjzT3MDJvnS-"},"outputs":[],"source":["class Root:\n","    \"\"\" This is root of all Algorithms \"\"\"\n","\n","    ID_MIN_PROB = 0  # min problem\n","    ID_MAX_PROB = -1  # max problem\n","\n","    ID_POS = 0  # Position\n","    ID_FIT = 1  # Fitness\n","\n","    EPSILON = 10E-10    # Avoid division by 0 \n","\n","    DEFAULT_LB = -1\n","    DEFAULT_UB = 1\n","\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True):\n","        self.verbose = verbose\n","        self.obj_func = obj_func\n","        if lb is None:\n","            print(\"Lower bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.lb = array(lb)\n","        if ub is None:\n","            print(\"Upper bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.ub = array(ub)\n","           \n","        if len(lb) != len(ub):\n","            print(\"Lower bound and Upper bound need to have the same length\")\n","            exit(0)\n","        \n","        self.problem_size = len(lb)\n","        self.solution, self.loss_train, self.fs_counts = None, [], []\n","\n","    def create_solution(self, minmax=0):\n","        \"\"\" Return solution with 2 element: position of solution and fitness of solution\n","        Parameters\n","        ----------\n","        minmax\n","            0 - minimum problem, else - maximum problem\n","        \"\"\"\n","        position = uniform(self.lb, self.ub)\n","        # print(position)\n","        # position=numpy.random.randint(2, size=(self.pop_size,self.dim))\n","        # for i in range(0,self.pop_size):\n","        #   while numpy.sum(position[i,:])==0:   \n","        #           position[i,:]=numpy.random.randint(2, size=(1,self.dim))\n","        fitness, binary_pos = self.get_fitness_position(position=position, minmax=minmax)\n","        return [position, fitness, binary_pos]\n","\n","    def get_fitness_position(self,position=None, minmax=0):\n","        \"\"\"     Assumption that objective function always return the original value\n","        :param position: 1-D numpy array\n","        :param minmax: 0- min problem, 1 - max problem\n","        :return:\n","        \"\"\"\n","        # print(position)\n","        Positions=position;\n","         \n","        \n","        for j in range(0,self.dim):\n","          ss= transfer_functions_benchmark.s1(position[j])\n","          if (random.random()<ss): \n","              Positions[j]=1;\n","          else:\n","              Positions[j]=0;\n","\n","        return self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) if minmax == 0 else 1.0 / (self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) + self.EPSILON), Positions     # Avoid division by 0\n","\n","    def get_fitness_solution(self,solution=None, minmax=0):\n","        return self.get_fitness_position(solution[self.ID_POS],self.trainInput,self.trainOutput,self.dim, minmax)\n","\n","    def get_global_best_global_worst_solution(self, pop=None, id_fit=None, id_best=None):\n","        sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n","        if id_best == self.ID_MIN_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MAX_PROB])\n","        elif id_best == self.ID_MAX_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MIN_PROB])\n","\n","\n","    def update_global_best_global_worst_solution(self, pop=None, id_best=None, id_worst=None, g_best=None):\n","        \"\"\" Sort the copy of population and update the current best position. Return the new current best position \"\"\"\n","        sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n","        current_best = sorted_pop[id_best]\n","        g_best = deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n","        return g_best, sorted_pop[id_worst]\n","\n","\n","    def amend_position(self, position=None):\n","        return clip(position, self.lb, self.ub)\n","\n","    def amend_position_random(self, position=None):\n","        return where(logical_and(self.lb <= position, position <= self.ub), position, uniform(self.lb, self.ub))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kt1zWTyVvnS_"},"outputs":[],"source":["from numpy.random import normal, rand, choice\n","from numpy import mean, pi, sin, cos, array\n","from math import gamma\n","# from mealpy.optimizer import Root\n","\n","from numpy import exp\n","from numpy.random import uniform, choice, rand\n","from numpy import min as np_min\n","from numpy import max as np_max\n","\n","class SSAClass(Root):\n","    \n","    ID_POS = 0\n","    ID_FIT = 1\n","    ID_DEN = 2  # Density\n","    ID_VOL = 3  # Volume\n","    ID_ACC = 4  # Acceleration\n","        \n","    def __init__(self, obj_func=None, lb=None, ub=None, verbose=True, epoch=750, pop_size=100, c1=2, c2=6, c3=2, c4=0.5,trainInput=None,trainOutput=None,dim=None, logger=None):\n","    # def __init__(self, obj_func=None, lb=None, ub=None, verbose=True, epoch=750, pop_size=100,trainInput=None,trainOutput=None,dim=None, logger=None):\n","        # super().__init__({\"obj_func\":obj_func, \"lb\":lb, \"ub\":ub, \"verbose\":verbose})#, \"trainInput\"=trainInput,\"trainOutput\"=trainOutput,\"dim\"=dim})\n","        super().__init__(obj_func,lb, ub, verbose)\n","        # print(trainInput.shape)\n","        self.epoch = epoch\n","        self.pop_size = pop_size\n","        self.c1 = c1    # Default belongs [1, 2]\n","        self.c2 = c2    # Default belongs [2, 4, 6]\n","        self.c3 = c3    # Default belongs [1, 2]\n","        self.c4 = c4    # Default belongs [0.5, 1]\n","\n","        self.trainInput=trainInput\n","        self.dim=dim\n","        self.trainOutput=trainOutput\n","        self.logger = logger\n","\n","\n","        self.ST = 0.8\n","        self.PD = 0.2\n","        self.SD = 0.1\n","\n","        self.n1 = int(self.PD * self.pop_size)\n","        self.n2 = int(self.SD * self.pop_size)\n","\n","        self.nfe_per_epoch = 2*self.pop_size - self.n2\n","        self.sort_flag = True\n","\n","    def create_solution(self, minmax=0):\n","        pos = uniform(self.lb, self.ub, self.problem_size)\n","        fit, bin_pos = self.get_fitness_position(pos, minmax=minmax)\n","        den = uniform(self.lb, self.ub, self.problem_size)\n","        vol = uniform(self.lb, self.ub, self.problem_size)\n","        acc = self.lb + uniform(self.lb, self.ub, self.problem_size) * (self.ub - self.lb)\n","        return [pos, fit, den, vol, acc]\n","    \n","    def train(self):\n","        pop = [self.create_solution() for _ in range(self.pop_size)]\n","        # g_best = self.get_global_best_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","        g_best, g_worst = self.get_global_best_global_worst_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","\n","        acc_upper = 0.9\n","        acc_lower = 0.1\n","\n","        for epoch in range(0, self.epoch):\n","            r2 = np.random.uniform()  # R2 in [0, 1], the alarm value, random value\n","            pop_new = []\n","            for idx in range(0, self.pop_size):\n","                # Using equation (3) update the sparrows location;\n","                if idx < self.n1:\n","                    if r2 < self.ST:\n","                        x_new = pop[idx][self.ID_POS] * np.exp((idx + 1) / ((np.random.uniform() + self.EPSILON) * self.epoch))\n","                    else:\n","                        x_new = pop[idx][self.ID_POS] + np.random.normal() * np.ones(self.dim)\n","                else:\n","                    # Using equation (4) update the sparrows location;\n","                    # _, x_p, worst = self.get_special_solutions(pop, best=1, worst=1)\n","                    # g_best = x_p[0], g_worst = worst[0]\n","\n","                    if idx > int(self.pop_size / 2):\n","                        x_new = np.random.normal() * np.exp((g_worst[self.ID_POS] - pop[idx][self.ID_POS]) / (idx + 1) ** 2)\n","                    else:\n","                        x_new = g_best[self.ID_POS] + np.abs(pop[idx][self.ID_POS] - g_best[self.ID_POS]) * np.random.normal()\n","\n","                    # r = np.random.uniform()\n","                    # alpha = 2 * r * np.sqrt(np.abs(np.log(r)))\n","                    # if idx == 0:\n","                    #     x_new = pop[idx][self.ID_POS] + r * (g_best[self.ID_POS] - pop[idx][self.ID_POS]) + \\\n","                    #           alpha * (g_best[self.ID_POS] - pop[idx][self.ID_POS])\n","                    # else:\n","                    #     x_new = pop[idx][self.ID_POS] + r * (pop[idx - 1][self.ID_POS] - pop[idx][self.ID_POS]) + \\\n","                    #        alpha * (g_best[self.ID_POS] - pop[idx][self.ID_POS])\n","                \n","                pos_new = self.amend_position_random(x_new)\n","                # pop_new.append([pos_new, None])\n","\n","                # ==================new=================\n","                fit_new, pos_new_binary = self.get_fitness_position(pos_new)\n","\n","                if fit_new < pop[i][self.ID_FIT]:\n","                    pop[i][self.ID_POS] = pos_new_binary#pos_new\n","                    pop[i][self.ID_FIT] = fit_new\n","                # =======================================\n","            ## Update global best and global worst\n","            g_best, g_worst = self.update_global_best_global_worst_solution(pop, self.ID_MIN_PROB, self.ID_MAX_PROB, g_best)\n","\n","            self.loss_train.append(g_best[self.ID_FIT])\n","\n","        #     if self.verbose:\n","        #         print(\">Epoch: {}, Best fit: {}\".format(epoch + 1, g_best[self.ID_FIT]))\n","        # self.solution = g_best\n","        # return g_best[self.ID_POS], g_best[self.ID_FIT], self.loss_train\n","            self.fs_counts.append(sum(g_best[self.ID_POS]))\n","            \n","            if (epoch%1==0):\n","                \n","                print(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[self.ID_POS]))])\n","                self.logger.info(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[self.ID_POS]))])\n","          \n","        self.solution = g_best\n","        return g_best[self.ID_POS], g_best[self.ID_FIT], self.loss_train, self.fs_counts\n","\n","            "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gLkp-sJWvnS_"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","# from .DYHGSClass import DYHGSClass\n","from numpy.random import uniform, normal, rand\n","\n","    \n","class ssa:\n","\n","      def SSA(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","    \n","            #Max_iter=1000\n","            #lb=-100\n","            #ub=100\n","            #dim=30  \n","            #SearchAgents_no=5\n","            lb=0\n","            # initialize alpha, beta, and delta_pos\n","            g_best=numpy.zeros(dim)\n","            Alpha_score=float(\"inf\")\n","            SSA=1\n","            verbose = True\n","            # Loop counter\n","            print(\"SSA is optimizing  \\\"\"+objf.__name__+\"\\\"\")   \n","            LOGGER_FILE.info(\"SSA is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","            s=solution()\n","            timerStart=time.time() \n","            s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            lb = [lb] * dim\n","            ub = [ub] * dim\n","            # print(trainInput.shape)\n","\n","            best_pos1, best_fit1, list_loss1, fs_counts = SSAClass(objf, lb, ub, verbose, Max_iter, SearchAgents_no,0.08, 10000,trainInput=trainInput,trainOutput=trainOutput,dim=dim, logger = LOGGER_FILE).train()\n","            # print(md1)\n","            # best_pos1, best_fit1, list_loss1 = md1.train()\n","            \n","            timerEnd=time.time()  \n","            s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            s.executionTime=timerEnd-timerStart\n","            s.bestIndividual=best_pos1\n","            s.convergence1=list_loss1\n","            s.convergence2=fs_counts\n","            \n","\n","            s.optimizer=\"SSA\"\n","            s.objfname=objf.__name__\n","            \n","            return s"]},{"cell_type":"markdown","metadata":{"id":"Cf_xhsWtktvu"},"source":["## 21-SSMRF \n","Sparrow Search based Manta Ray Foraging (SSMRF) Algorithm Optimization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DykNBQWRktvu"},"outputs":[],"source":["class Root:\n","    \"\"\" This is root of all Algorithms \"\"\"\n","\n","    ID_MIN_PROB = 0  # min problem\n","    ID_MAX_PROB = -1  # max problem\n","\n","    ID_POS = 0  # Position\n","    ID_FIT = 1  # Fitness\n","\n","    EPSILON = 10E-10    # Avoid division by 0 \n","\n","    DEFAULT_LB = -1\n","    DEFAULT_UB = 1\n","\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True):\n","        self.verbose = verbose\n","        self.obj_func = obj_func\n","        if lb is None:\n","            print(\"Lower bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.lb = array(lb)\n","        if ub is None:\n","            print(\"Upper bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.ub = array(ub)\n","           \n","        if len(lb) != len(ub):\n","            print(\"Lower bound and Upper bound need to have the same length\")\n","            exit(0)\n","        \n","        self.problem_size = len(lb)\n","        self.solution, self.loss_train, self.fs_counts = None, [], []\n","\n","    def create_solution(self, minmax=0):\n","        \"\"\" Return solution with 2 element: position of solution and fitness of solution\n","        Parameters\n","        ----------\n","        minmax\n","            0 - minimum problem, else - maximum problem\n","        \"\"\"\n","        position = uniform(self.lb, self.ub)\n","        # print(position)\n","        # position=numpy.random.randint(2, size=(self.pop_size,self.dim))\n","        # for i in range(0,self.pop_size):\n","        #   while numpy.sum(position[i,:])==0:   \n","        #           position[i,:]=numpy.random.randint(2, size=(1,self.dim))\n","        fitness, binary_pos = self.get_fitness_position(position=position, minmax=minmax)\n","        return [position, fitness, binary_pos]\n","\n","    def get_fitness_position(self,position=None, minmax=0):\n","        \"\"\"     Assumption that objective function always return the original value\n","        :param position: 1-D numpy array\n","        :param minmax: 0- min problem, 1 - max problem\n","        :return:\n","        \"\"\"\n","        # print(position)\n","        Positions=position;\n","         \n","        \n","        for j in range(0,self.dim):\n","          ss= transfer_functions_benchmark.s1(position[j])\n","          if (random.random()<ss): \n","              Positions[j]=1;\n","          else:\n","              Positions[j]=0;\n","\n","        return self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) if minmax == 0 else 1.0 / (self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) + self.EPSILON), Positions     # Avoid division by 0\n","\n","    def get_fitness_solution(self,solution=None, minmax=0):\n","        return self.get_fitness_position(solution[self.ID_POS],self.trainInput,self.trainOutput,self.dim, minmax)\n","\n","    def get_global_best_global_worst_solution(self, pop=None, id_fit=None, id_best=None):\n","        sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n","        if id_best == self.ID_MIN_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MAX_PROB])\n","        elif id_best == self.ID_MAX_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MIN_PROB])\n","\n","\n","    def update_global_best_global_worst_solution(self, pop=None, id_best=None, id_worst=None, g_best=None):\n","        \"\"\" Sort the copy of population and update the current best position. Return the new current best position \"\"\"\n","        sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n","        current_best = sorted_pop[id_best]\n","        g_best = deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n","        return g_best, sorted_pop[id_worst]\n","\n","\n","    def amend_position(self, position=None):\n","        return clip(position, self.lb, self.ub)\n","\n","    def amend_position_random(self, position=None):\n","        return where(logical_and(self.lb <= position, position <= self.ub), position, uniform(self.lb, self.ub))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WX2AV8O2ktvv"},"outputs":[],"source":["from numpy.random import normal, rand, choice\n","from numpy import mean, pi, sin, cos, array\n","from math import gamma\n","# from mealpy.optimizer import Root\n","\n","from numpy import exp\n","from numpy.random import uniform, choice, rand\n","from numpy import min as np_min\n","from numpy import max as np_max\n","\n","class SSAMRFClass(Root):\n","    \n","    ID_POS = 0\n","    ID_FIT = 1\n","    ID_DEN = 2  # Density\n","    ID_VOL = 3  # Volume\n","    ID_ACC = 4  # Acceleration\n","        \n","    def __init__(self, obj_func=None, lb=None, ub=None, verbose=True, epoch=750, pop_size=100, c1=2, c2=6, c3=2, c4=0.5,trainInput=None,trainOutput=None,dim=None, logger=None):\n","    # def __init__(self, obj_func=None, lb=None, ub=None, verbose=True, epoch=750, pop_size=100,trainInput=None,trainOutput=None,dim=None, logger=None):\n","        # super().__init__({\"obj_func\":obj_func, \"lb\":lb, \"ub\":ub, \"verbose\":verbose})#, \"trainInput\"=trainInput,\"trainOutput\"=trainOutput,\"dim\"=dim})\n","        super().__init__(obj_func,lb, ub, verbose)\n","        # print(trainInput.shape)\n","        self.epoch = epoch\n","        self.pop_size = pop_size\n","        self.c1 = c1    # Default belongs [1, 2]\n","        self.c2 = c2    # Default belongs [2, 4, 6]\n","        self.c3 = c3    # Default belongs [1, 2]\n","        self.c4 = c4    # Default belongs [0.5, 1]\n","\n","        self.trainInput=trainInput\n","        self.dim=dim\n","        self.trainOutput=trainOutput\n","        self.logger = logger\n","\n","\n","        self.ST = 0.8\n","        self.PD = 0.2\n","        self.SD = 0.1\n","\n","        self.n1 = int(self.PD * self.pop_size)\n","        self.n2 = int(self.SD * self.pop_size)\n","\n","        self.nfe_per_epoch = 2*self.pop_size - self.n2\n","        self.sort_flag = True\n","\n","    def create_solution(self, minmax=0):\n","        pos = uniform(self.lb, self.ub, self.problem_size)\n","        fit, bin_pos = self.get_fitness_position(pos, minmax=minmax)\n","        den = uniform(self.lb, self.ub, self.problem_size)\n","        vol = uniform(self.lb, self.ub, self.problem_size)\n","        acc = self.lb + uniform(self.lb, self.ub, self.problem_size) * (self.ub - self.lb)\n","        return [pos, fit, den, vol, acc]\n","    \n","    def train(self):\n","        pop = [self.create_solution() for _ in range(self.pop_size)]\n","        # g_best = self.get_global_best_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","        g_best, g_worst = self.get_global_best_global_worst_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","\n","        acc_upper = 0.9\n","        acc_lower = 0.1\n","\n","        for epoch in range(0, self.epoch):\n","            r2 = np.random.uniform()  # R2 in [0, 1], the alarm value, random value\n","            pop_new = []\n","            for idx in range(0, self.pop_size):\n","                # Using equation (3) update the sparrows location;\n","                if idx < self.n1:\n","                    if r2 < self.ST:\n","                        x_new = pop[idx][self.ID_POS] * np.exp((idx + 1) / ((np.random.uniform() + self.EPSILON) * self.epoch))\n","                    else:\n","                        x_new = pop[idx][self.ID_POS] + np.random.normal() * np.ones(self.dim)\n","                else:\n","                    # Using equation (4) update the sparrows location;\n","                    # _, x_p, worst = self.get_special_solutions(pop, best=1, worst=1)\n","                    # g_best = x_p[0], g_worst = worst[0]\n","\n","                    # if idx > int(self.pop_size / 2):\n","                    #     x_new = np.random.normal() * np.exp((g_worst[self.ID_POS] - pop[idx][self.ID_POS]) / (idx + 1) ** 2)\n","                    # else:\n","                    #     x_new = g_best[self.ID_POS] + np.abs(pop[idx][self.ID_POS] - g_best[self.ID_POS]) * np.random.normal()\n","\n","                    \n","                    r = np.random.uniform()\n","                    alpha = 2 * r * np.sqrt(np.abs(np.log(r)))\n","                    if idx == 0:\n","                        x_new = pop[idx][self.ID_POS] + r * (g_best[self.ID_POS] - pop[idx][self.ID_POS]) + \\\n","                              alpha * (g_best[self.ID_POS] - pop[idx][self.ID_POS])\n","                    else:\n","                        x_new = pop[idx][self.ID_POS] + r * (pop[idx - 1][self.ID_POS] - pop[idx][self.ID_POS]) + \\\n","                           alpha * (g_best[self.ID_POS] - pop[idx][self.ID_POS])\n","                \n","                pos_new = self.amend_position_random(x_new)\n","                # pop_new.append([pos_new, None])\n","\n","                # ==================new=================\n","                fit_new, pos_new_binary = self.get_fitness_position(pos_new)\n","\n","                if fit_new < pop[i][self.ID_FIT]:\n","                    pop[i][self.ID_POS] = pos_new_binary#pos_new\n","                    pop[i][self.ID_FIT] = fit_new\n","                # =======================================\n","            ## Update global best and global worst\n","            g_best, g_worst = self.update_global_best_global_worst_solution(pop, self.ID_MIN_PROB, self.ID_MAX_PROB, g_best)\n","\n","            self.loss_train.append(g_best[self.ID_FIT])\n","\n","        #     if self.verbose:\n","        #         print(\">Epoch: {}, Best fit: {}\".format(epoch + 1, g_best[self.ID_FIT]))\n","        # self.solution = g_best\n","        # return g_best[self.ID_POS], g_best[self.ID_FIT], self.loss_train\n","            self.fs_counts.append(sum(g_best[self.ID_POS]))\n","            \n","            if (epoch%1==0):\n","                \n","                print(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[self.ID_POS]))])\n","                self.logger.info(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[self.ID_POS]))])\n","          \n","        self.solution = g_best\n","        return g_best[self.ID_POS], g_best[self.ID_FIT], self.loss_train, self.fs_counts\n","\n","            "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UNoyQIeYz4zy"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","# from .DYHGSClass import DYHGSClass\n","from numpy.random import uniform, normal, rand\n","\n","    \n","class ssamrf:\n","\n","      def SSAMRF(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","    \n","            #Max_iter=1000\n","            #lb=-100\n","            #ub=100\n","            #dim=30  \n","            #SearchAgents_no=5\n","            lb=0\n","            # initialize alpha, beta, and delta_pos\n","            g_best=numpy.zeros(dim)\n","            Alpha_score=float(\"inf\")\n","            XXX=1\n","            verbose = True\n","            # Loop counter\n","            print(\"SSAMRF is optimizing  \\\"\"+objf.__name__+\"\\\"\")   \n","            LOGGER_FILE.info(\"SSAMRF is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","            s=solution()\n","            timerStart=time.time() \n","            s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            lb = [lb] * dim\n","            ub = [ub] * dim\n","            # print(trainInput.shape)\n","\n","            best_pos1, best_fit1, list_loss1, fs_counts = SSAMRFClass(objf, lb, ub, verbose, Max_iter, SearchAgents_no,0.08, 10000,trainInput=trainInput,trainOutput=trainOutput,dim=dim, logger = LOGGER_FILE).train()\n","            # print(md1)\n","            # best_pos1, best_fit1, list_loss1 = md1.train()\n","            \n","            timerEnd=time.time()  \n","            s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            s.executionTime=timerEnd-timerStart\n","            s.bestIndividual=best_pos1\n","            s.convergence1=list_loss1\n","            s.convergence2=fs_counts\n","            \n","\n","            s.optimizer=\"SSAMRF\"\n","            s.objfname=objf.__name__\n","            \n","            return s"]},{"cell_type":"markdown","metadata":{"id":"LdYU8SL6pDUu"},"source":["## 22-NMRA \n","Naked Mole-Rat Algorithm "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"57e25itmqI16"},"outputs":[],"source":["class Root:\n","    \"\"\" This is root of all Algorithms \"\"\"\n","\n","    ID_MIN_PROB = 0  # min problem\n","    ID_MAX_PROB = -1  # max problem\n","\n","    ID_POS = 0  # Position\n","    ID_FIT = 1  # Fitness\n","\n","    EPSILON = 10E-10    # Avoid division by 0 \n","\n","    DEFAULT_LB = -1\n","    DEFAULT_UB = 1\n","\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True):\n","        self.verbose = verbose\n","        self.obj_func = obj_func\n","        if lb is None:\n","            print(\"Lower bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.lb = array(lb)\n","        if ub is None:\n","            print(\"Upper bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.ub = array(ub)\n","           \n","        if len(lb) != len(ub):\n","            print(\"Lower bound and Upper bound need to have the same length\")\n","            exit(0)\n","        \n","        self.problem_size = len(lb)\n","        self.solution, self.loss_train, self.fs_counts = None, [], []\n","\n","    def create_solution(self, minmax=0):\n","        \"\"\" Return solution with 2 element: position of solution and fitness of solution\n","        Parameters\n","        ----------\n","        minmax\n","            0 - minimum problem, else - maximum problem\n","        \"\"\"\n","        position = uniform(self.lb, self.ub)\n","        # print(position)\n","        # position=numpy.random.randint(2, size=(self.pop_size,self.dim))\n","        # for i in range(0,self.pop_size):\n","        #   while numpy.sum(position[i,:])==0:   \n","        #           position[i,:]=numpy.random.randint(2, size=(1,self.dim))\n","        fitness, binary_pos = self.get_fitness_position(position=position, minmax=minmax)\n","        return [position, fitness, binary_pos]\n","\n","    def get_fitness_position(self,position=None, minmax=0):\n","        \"\"\"     Assumption that objective function always return the original value\n","        :param position: 1-D numpy array\n","        :param minmax: 0- min problem, 1 - max problem\n","        :return:\n","        \"\"\"\n","        # print(position)\n","        Positions=position;\n","         \n","        \n","        for j in range(0,self.dim):\n","          ss= transfer_functions_benchmark.s1(position[j])\n","          if (random.random()<ss): \n","              Positions[j]=1;\n","          else:\n","              Positions[j]=0;\n","\n","        return self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) if minmax == 0 else 1.0 / (self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) + self.EPSILON), Positions     # Avoid division by 0\n","\n","    def get_fitness_solution(self,solution=None, minmax=0):\n","        return self.get_fitness_position(solution[self.ID_POS],self.trainInput,self.trainOutput,self.dim, minmax)\n","\n","    def get_global_best_global_worst_solution(self, pop=None, id_fit=None, id_best=None):\n","        sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n","        if id_best == self.ID_MIN_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MAX_PROB])\n","        elif id_best == self.ID_MAX_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MIN_PROB])\n","\n","\n","    def update_global_best_global_worst_solution(self, pop=None, id_best=None, id_worst=None, g_best=None):\n","        \"\"\" Sort the copy of population and update the current best position. Return the new current best position \"\"\"\n","        sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n","        current_best = sorted_pop[id_best]\n","        g_best = deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n","        return g_best, sorted_pop[id_worst]\n","\n","\n","    def amend_position(self, position=None):\n","        return clip(position, self.lb, self.ub)\n","\n","    def amend_position_random(self, position=None):\n","        return where(logical_and(self.lb <= position, position <= self.ub), position, uniform(self.lb, self.ub))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8PNrpx-yrZRG"},"outputs":[],"source":["from numpy.random import normal, rand, choice\n","from numpy import mean, pi, sin, cos, array\n","from math import gamma\n","# from mealpy.optimizer import Root\n","from copy import deepcopy\n","\n","class NMRAClass(Root):\n","    \n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True, epoch=1000, pop_size=100, L=0.08, LH=10000,trainInput=None,trainOutput=None,dim=None, logger=None):  \n","\n","            super().__init__(obj_func,lb, ub, verbose)\n","            self.epoch = epoch\n","            self.pop_size = pop_size\n","            self.nfe_per_epoch = self.pop_size\n","            self.sort_flag = True\n","            self.size_b = int(self.pop_size / 5)\n","\n","            self.pb = 0.75#self.validator.check_float(\"pb\", pb, (0, 1.0))\n","\n","            self.trainInput=trainInput\n","            self.dim=dim\n","            self.trainOutput=trainOutput\n","            self.logger = logger\n","\n","\n","    def train(self):\n","\n","        pop_new = []\n","        pop = [self.create_solution() for _ in range(self.pop_size)]\n","        g_best, g_worst = self.get_global_best_global_worst_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","\n","        for idx in range(0, self.pop_size):\n","            pos_new = deepcopy(pop[idx][self.ID_POS])\n","            if idx < self.size_b:  # breeding operators\n","                if np.random.uniform() < self.pb:\n","                    alpha = np.random.uniform()\n","                    pos_new = (1 - alpha) * pop[idx][self.ID_POS] + alpha * (g_best[self.ID_POS] - pop[idx][self.ID_POS])\n","            else:  # working operators\n","                t1, t2 = np.random.choice(range(self.size_b, self.pop_size), 2, replace=False)\n","                pos_new = pop[idx][self.ID_POS] + np.random.uniform() * (pop[t1][self.ID_POS] - pop[t2][self.ID_POS])\n","        #     pos_new = self.amend_position(pos_new, self.problem.lb, self.problem.ub)\n","        #     pop_new.append([pos_new, None])\n","        # pop_new = self.update_target_wrapper_population(pop_new)\n","        # self.pop = self.greedy_selection_population(self.pop, pop_new)\n","\n","\n","            fit_new, pos_new_binary = self.get_fitness_position(pos_new)     \n","            pop[i] = [pos_new, fit_new, pos_new_binary]\n","\n","        ## Update global best and global worst\n","        g_best, g_worst = self.update_global_best_global_worst_solution(pop, self.ID_MIN_PROB, self.ID_MAX_PROB, g_best)\n","\n","        # g_best = self.update_global_best_solution(pop, self.ID_MIN_PROB, g_best)\n","        self.loss_train.append(g_best[self.ID_FIT])\n","\n","        self.fs_counts.append(sum(g_best[2]))\n","        # if self.verbose:\n","        #     print(\"> Epoch: {}, Best fit: {}\".format(epoch + 1, g_best[self.ID_FIT]))\n","            \n","        if (self.epoch%1==0):\n","            \n","            print(['At iteration'+ str(self.epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[2]))])\n","            self.logger.info(['At iteration'+ str(self.epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[2]))])\n","      \n","        self.solution = g_best\n","        return g_best[2], g_best[self.ID_FIT], self.loss_train, self.fs_counts\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RykXEliluRCb"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","# from .DYHGSClass import DYHGSClass\n","from numpy.random import uniform, normal, rand\n","\n","    \n","class nmra:\n","\n","      def NMRA(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","    \n","            #Max_iter=1000\n","            #lb=-100\n","            #ub=100\n","            #dim=30  \n","            #SearchAgents_no=5\n","            lb=0\n","            # initialize alpha, beta, and delta_pos\n","            g_best=numpy.zeros(dim)\n","            Alpha_score=float(\"inf\")\n","            XXX=1\n","            verbose = True\n","            # Loop counter\n","            print(\"NMRA is optimizing  \\\"\"+objf.__name__+\"\\\"\")   \n","            LOGGER_FILE.info(\"NMRA is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","            s=solution()\n","            timerStart=time.time() \n","            s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            lb = [lb] * dim\n","            ub = [ub] * dim\n","            # print(trainInput.shape)\n","\n","            best_pos1, best_fit1, list_loss1, fs_counts = NMRAClass(objf, lb, ub, verbose, Max_iter, SearchAgents_no,0.08, 10000,trainInput=trainInput,trainOutput=trainOutput,dim=dim, logger = LOGGER_FILE).train()\n","            # print(md1)\n","            # best_pos1, best_fit1, list_loss1 = md1.train()\n","            \n","            timerEnd=time.time()  \n","            s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            s.executionTime=timerEnd-timerStart\n","            s.bestIndividual=best_pos1\n","            s.convergence1=list_loss1\n","            s.convergence2=fs_counts\n","            \n","\n","            s.optimizer=\"NMRA\"\n","            s.objfname=objf.__name__\n","            \n","            return s"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u1U4gnTq73q4"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"H2D3Gql475Q6"},"source":["## 23-MuNMRA \n","Naked Mole-Rat Algorithm "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HpfPvqSs75Q7"},"outputs":[],"source":["class Root:\n","    \"\"\" This is root of all Algorithms \"\"\"\n","\n","    ID_MIN_PROB = 0  # min problem\n","    ID_MAX_PROB = -1  # max problem\n","\n","    ID_POS = 0  # Position\n","    ID_FIT = 1  # Fitness\n","\n","    EPSILON = 10E-10    # Avoid division by 0 \n","\n","    DEFAULT_LB = -1\n","    DEFAULT_UB = 1\n","\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True):\n","        self.verbose = verbose\n","        self.obj_func = obj_func\n","        if lb is None:\n","            print(\"Lower bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.lb = array(lb)\n","        if ub is None:\n","            print(\"Upper bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.ub = array(ub)\n","           \n","        if len(lb) != len(ub):\n","            print(\"Lower bound and Upper bound need to have the same length\")\n","            exit(0)\n","        \n","        self.problem_size = len(lb)\n","        self.solution, self.loss_train, self.fs_counts = None, [], []\n","\n","    def create_solution(self, minmax=0):\n","        \"\"\" Return solution with 2 element: position of solution and fitness of solution\n","        Parameters\n","        ----------\n","        minmax\n","            0 - minimum problem, else - maximum problem\n","        \"\"\"\n","        position = uniform(self.lb, self.ub)\n","        # print(position)\n","        # position=numpy.random.randint(2, size=(self.pop_size,self.dim))\n","        # for i in range(0,self.pop_size):\n","        #   while numpy.sum(position[i,:])==0:   \n","        #           position[i,:]=numpy.random.randint(2, size=(1,self.dim))\n","        fitness, binary_pos = self.get_fitness_position(position=position, minmax=minmax)\n","        return [position, fitness, binary_pos]\n","\n","    def get_fitness_position(self,position=None, minmax=0):\n","        \"\"\"     Assumption that objective function always return the original value\n","        :param position: 1-D numpy array\n","        :param minmax: 0- min problem, 1 - max problem\n","        :return:\n","        \"\"\"\n","        # print(position)\n","        Positions=position;\n","         \n","        \n","        for j in range(0,self.dim):\n","          ss= transfer_functions_benchmark.s1(position[j])\n","          if (random.random()<ss): \n","              Positions[j]=1;\n","          else:\n","              Positions[j]=0;\n","\n","        return self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) if minmax == 0 else 1.0 / (self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) + self.EPSILON), Positions     # Avoid division by 0\n","\n","    def get_fitness_solution(self,solution=None, minmax=0):\n","        return self.get_fitness_position(solution[self.ID_POS],self.trainInput,self.trainOutput,self.dim, minmax)\n","\n","    def get_global_best_global_worst_solution(self, pop=None, id_fit=None, id_best=None):\n","        sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n","        if id_best == self.ID_MIN_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MAX_PROB])\n","        elif id_best == self.ID_MAX_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MIN_PROB])\n","\n","\n","    def update_global_best_global_worst_solution(self, pop=None, id_best=None, id_worst=None, g_best=None):\n","        \"\"\" Sort the copy of population and update the current best position. Return the new current best position \"\"\"\n","        sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n","        current_best = sorted_pop[id_best]\n","        g_best = deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n","        return g_best, sorted_pop[id_worst]\n","\n","\n","    def amend_position(self, position=None):\n","        return clip(position, self.lb, self.ub)\n","\n","    def amend_position_random(self, position=None):\n","        return where(logical_and(self.lb <= position, position <= self.ub), position, uniform(self.lb, self.ub))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q3LBfl7X75Q7"},"outputs":[],"source":["from numpy.random import normal, rand, choice\n","from numpy import mean, pi, sin, cos, array\n","from math import gamma\n","# from mealpy.optimizer import Root\n","from copy import deepcopy\n","\n","class MuNMRAClass(Root):\n","  \n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True, epoch=1000, pop_size=100, L=0.08, LH=10000,trainInput=None,trainOutput=None,dim=None, logger=None):  \n","            \"\"\"\n","            Args:\n","                problem (dict): The problem dictionary\n","                epoch (int): maximum number of iterations, default = 10000\n","                pop_size (int): number of population size, default = 100\n","                pb (float): breeding probability, default = 0.75\n","                pm (float): probability of mutation, default = 0.01\n","            \"\"\"\n","            super().__init__(obj_func,lb, ub, verbose)\n","            self.epoch = epoch\n","            self.pop_size = pop_size\n","            self.nfe_per_epoch = self.pop_size\n","            self.sort_flag = True\n","            self.size_b = int(self.pop_size / 5)\n","            self.lb=lb\n","            self.ub=ub\n","\n","            self.pb = 0.75#self.validator.check_float(\"pb\", pb, (0, 1.0))\n","            self.pm = 0.01#self.validator.check_float(\"pm\", pm, (0, 1.0))\n","\n","            self.trainInput=trainInput\n","            self.dim=dim\n","            self.trainOutput=trainOutput\n","            self.logger = logger\n","\n","    def _crossover_random(self, pop, g_best):\n","        start_point = np.random.randint(0, self.dim / 2)\n","        id1 = start_point\n","        id2 = int(start_point + self.dim / 3)\n","        id3 = int(self.dim)\n","\n","        partner = pop[np.random.randint(0, self.pop_size)][self.ID_POS]\n","        new_temp = deepcopy(g_best[self.ID_POS])\n","        new_temp[0:id1] = g_best[self.ID_POS][0:id1]\n","        new_temp[id1:id2] = partner[id1:id2]\n","        new_temp[id2:id3] = g_best[self.ID_POS][id2:id3]\n","        return new_temp\n","\n","    def get_levy_flight_step(self, beta=1.0, multiplier=0.001, case=0):\n","        \"\"\"\n","        Get the Levy-flight step size\n","        Args:\n","            beta (float): Should be in range [0, 2].\n","                * 0-1: small range --> exploit\n","                * 1-2: large range --> explore\n","            multiplier (float): default = 0.001\n","            case (int): Should be one of these value [0, 1, -1].\n","                * 0: return multiplier * s * np.random.uniform()\n","                * 1: return multiplier * s * np.random.normal(0, 1)\n","                * -1: return multiplier * s\n","        Returns:\n","            int: The step size of Levy-flight trajectory\n","        \"\"\"\n","        # u and v are two random variables which follow np.random.normal distribution\n","        # sigma_u : standard deviation of u\n","        sigma_u = np.power(gamma(1 + beta) * np.sin(np.pi * beta / 2) / (gamma((1 + beta) / 2) * beta * np.power(2, (beta - 1) / 2)), 1 / beta)\n","        # sigma_v : standard deviation of v\n","        sigma_v = 1\n","        u = np.random.normal(0, sigma_u ** 2)\n","        v = np.random.normal(0, sigma_v ** 2)\n","        s = u / np.power(abs(v), 1 / beta)\n","        if case == 0:\n","            step = multiplier * s * np.random.uniform()\n","        elif case == 1:\n","            step = multiplier * s * np.random.normal(0, 1)\n","        else:\n","            step = multiplier * s\n","        return step\n","\n","    def train(self):\n","        \"\"\"\n","        The main operations (equations) of algorithm. Inherit from Optimizer class\n","        Args:\n","            epoch (int): The current iteration\n","        \"\"\"\n","        pop_new = []\n","        pop = [self.create_solution() for _ in range(self.pop_size)]\n","        g_best, g_worst = self.get_global_best_global_worst_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","\n","\n","        for idx in range(0, self.pop_size):\n","            # Exploration\n","            if idx < self.size_b:  # breeding operators\n","                if np.random.uniform() < self.pb:\n","                    pos_new = pop[idx][self.ID_POS] + np.random.normal(0, 1, self.dim) * \\\n","                              (g_best[self.ID_POS] - pop[idx][self.ID_POS])\n","                else:\n","                    levy_step = self.get_levy_flight_step(beta=1, multiplier=0.001, case=-1)\n","                    pos_new = pop[idx][self.ID_POS] + 1.0 / np.sqrt(self.epoch + 1) * np.sign(np.random.random() - 0.5) * \\\n","                              levy_step * (pop[idx][self.ID_POS] - g_best[self.ID_POS])\n","            # Exploitation\n","            else:  # working operators\n","                if np.random.uniform() < 0.5:\n","                    t1, t2 = np.random.choice(range(self.size_b, self.pop_size), 2, replace=False)\n","                    pos_new = pop[idx][self.ID_POS] + np.random.normal(0, 1, self.dim) * \\\n","                              (pop[t1][self.ID_POS] - pop[t2][self.ID_POS])\n","                else:\n","                    pos_new = self._crossover_random(pop, g_best)\n","            # Mutation\n","            temp = np.random.uniform(self.lb, self.ub)\n","            pos_new = np.where(np.random.uniform(0, 1, self.dim) < self.pm, temp, pos_new)\n","            \n","            fit_new, pos_new_binary = self.get_fitness_position(pos_new)     \n","            pop[i] = [pos_new, fit_new, pos_new_binary]\n","\n","        ## Update global best and global worst\n","        g_best, g_worst = self.update_global_best_global_worst_solution(pop, self.ID_MIN_PROB, self.ID_MAX_PROB, g_best)\n","\n","        # g_best = self.update_global_best_solution(pop, self.ID_MIN_PROB, g_best)\n","        self.loss_train.append(g_best[self.ID_FIT])\n","\n","        self.fs_counts.append(sum(g_best[2]))\n","        # if self.verbose:\n","        #     print(\"> Epoch: {}, Best fit: {}\".format(epoch + 1, g_best[self.ID_FIT]))\n","            \n","        if (self.epoch%1==0):\n","            \n","            print(['At iteration'+ str(self.epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[2]))])\n","            self.logger.info(['At iteration'+ str(self.epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[2]))])\n","      \n","        self.solution = g_best\n","        return g_best[2], g_best[self.ID_FIT], self.loss_train, self.fs_counts\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vV2RNRG075Q8"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","# from .DYHGSClass import DYHGSClass\n","from numpy.random import uniform, normal, rand\n","\n","    \n","class munmra:\n","\n","      def MuNMRA(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","    \n","            #Max_iter=1000\n","            #lb=-100\n","            #ub=100\n","            #dim=30  \n","            #SearchAgents_no=5\n","            lb=0\n","            # initialize alpha, beta, and delta_pos\n","            g_best=numpy.zeros(dim)\n","            Alpha_score=float(\"inf\")\n","            XXX=1\n","            verbose = True\n","            # Loop counter\n","            print(\"MuNMRA is optimizing  \\\"\"+objf.__name__+\"\\\"\")   \n","            LOGGER_FILE.info(\"MuNMRA is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","            s=solution()\n","            timerStart=time.time() \n","            s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            lb = [lb] * dim\n","            ub = [ub] * dim\n","            # print(trainInput.shape)\n","\n","            best_pos1, best_fit1, list_loss1, fs_counts = MuNMRAClass(objf, lb, ub, verbose, Max_iter, SearchAgents_no,0.08, 10000,trainInput=trainInput,trainOutput=trainOutput,dim=dim, logger = LOGGER_FILE).train()\n","            # print(md1)\n","            # best_pos1, best_fit1, list_loss1 = md1.train()\n","            \n","            timerEnd=time.time()  \n","            s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            s.executionTime=timerEnd-timerStart\n","            s.bestIndividual=best_pos1\n","            s.convergence1=list_loss1\n","            s.convergence2=fs_counts\n","            \n","\n","            s.optimizer=\"MuNMRA\"\n","            s.objfname=objf.__name__\n","            \n","            return s"]},{"cell_type":"markdown","metadata":{"id":"JzEnQq08tfw5"},"source":["## 24- ARO\n","Artificial Rabbits Optimization\n","\n","the code is here: https://www.mathworks.com/matlabcentral/fileexchange/110250-artificial-rabbits-optimization-aro?s_tid=FX_rc3_behav"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CTXgco0Tt51x"},"outputs":[],"source":["class Root:\n","    \"\"\" This is root of all Algorithms \"\"\"\n","\n","    ID_MIN_PROB = 0  # min problem\n","    ID_MAX_PROB = -1  # max problem\n","\n","    ID_POS = 0  # Position\n","    ID_FIT = 1  # Fitness\n","\n","    EPSILON = 10E-10    # Avoid division by 0 \n","\n","    DEFAULT_LB = -1\n","    DEFAULT_UB = 1\n","\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True):\n","        self.verbose = verbose\n","        self.obj_func = obj_func\n","        if lb is None:\n","            print(\"Lower bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.lb = array(lb)\n","        if ub is None:\n","            print(\"Upper bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.ub = array(ub)\n","           \n","        if len(lb) != len(ub):\n","            print(\"Lower bound and Upper bound need to have the same length\")\n","            exit(0)\n","        \n","        self.problem_size = len(lb)\n","        self.solution, self.loss_train, self.fs_counts = None, [], []\n","\n","    def create_solution(self, minmax=0):\n","        \"\"\" Return solution with 2 element: position of solution and fitness of solution\n","        Parameters\n","        ----------\n","        minmax\n","            0 - minimum problem, else - maximum problem\n","        \"\"\"\n","        position = uniform(self.lb, self.ub)\n","        # print(position)\n","        # position=numpy.random.randint(2, size=(self.pop_size,self.dim))\n","        # for i in range(0,self.pop_size):\n","        #   while numpy.sum(position[i,:])==0:   \n","        #           position[i,:]=numpy.random.randint(2, size=(1,self.dim))\n","        fitness, binary_pos = self.get_fitness_position(position=position, minmax=minmax)\n","        return [position, fitness, binary_pos]\n","\n","    def get_fitness_position(self,position=None, minmax=0):\n","        \"\"\"     Assumption that objective function always return the original value\n","        :param position: 1-D numpy array\n","        :param minmax: 0- min problem, 1 - max problem\n","        :return:\n","        \"\"\"\n","        # print(position)\n","        Positions=position;\n","         \n","        \n","        for j in range(0,self.dim):\n","          ss= transfer_functions_benchmark.s1(position[j])\n","          if (random.random()<ss): \n","              Positions[j]=1;\n","          else:\n","              Positions[j]=0;\n","\n","        return self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) if minmax == 0 else 1.0 / (self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) + self.EPSILON), Positions     # Avoid division by 0\n","\n","    def get_fitness_solution(self,solution=None, minmax=0):\n","        return self.get_fitness_position(solution[self.ID_POS],self.trainInput,self.trainOutput,self.dim, minmax)\n","\n","    def get_global_best_global_worst_solution(self, pop=None, id_fit=None, id_best=None):\n","        sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n","        if id_best == self.ID_MIN_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MAX_PROB])\n","        elif id_best == self.ID_MAX_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MIN_PROB])\n","\n","\n","    def update_global_best_global_worst_solution(self, pop=None, id_best=None, id_worst=None, g_best=None):\n","        \"\"\" Sort the copy of population and update the current best position. Return the new current best position \"\"\"\n","        sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n","        current_best = sorted_pop[id_best]\n","        g_best = deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n","        return g_best, sorted_pop[id_worst]\n","\n","\n","    def amend_position(self, position=None):\n","        return clip(position, self.lb, self.ub)\n","\n","    def amend_position_random(self, position=None):\n","        return where(logical_and(self.lb <= position, position <= self.ub), position, uniform(self.lb, self.ub))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cXuSYIUfuEDG"},"outputs":[],"source":["from numpy.random import normal, rand, choice\n","from numpy import mean, pi, sin, cos, array\n","from math import gamma\n","# from mealpy.optimizer import Root\n","from copy import deepcopy\n","from torch import randperm\n","\n","class AROClass(Root):\n","  \n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True, epoch=1000, pop_size=100, L=0.08, LH=10000,trainInput=None,trainOutput=None,dim=None, logger=None):  \n","            \"\"\"\n","            Args:\n","                problem (dict): The problem dictionary\n","                epoch (int): maximum number of iterations, default = 10000\n","                pop_size (int): number of population size, default = 100\n","                pb (float): breeding probability, default = 0.75\n","                pm (float): probability of mutation, default = 0.01\n","            \"\"\"\n","            super().__init__(obj_func,lb, ub, verbose)\n","            self.epoch = epoch\n","            self.pop_size = pop_size\n","            self.nfe_per_epoch = self.pop_size\n","            self.sort_flag = True\n","            self.size_b = int(self.pop_size / 5)\n","            self.lb=lb\n","            self.ub=ub\n","            self.objf=obj_func\n","\n","            self.pb = 0.75#self.validator.check_float(\"pb\", pb, (0, 1.0))\n","            self.pm = 0.01#self.validator.check_float(\"pm\", pm, (0, 1.0))\n","\n","            self.trainInput=trainInput\n","            self.dim=dim\n","            self.trainOutput=trainOutput\n","            self.logger = logger\n","\n","\n","    def train(self):\n","    \n","        pop_new = []\n","        #Initialize population with random positions\n","        pop = [self.create_solution() for _ in range(self.pop_size)]\n","        # #assign fitness to fprey.\n","        g_best, g_worst = self.get_global_best_global_worst_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","\n","        Sol=numpy.random.randint(2, size=(self.pop_size,self.dim))     #generating binary individuals\n","        pop_pos=numpy.zeros((self.pop_size,self.dim))  # Population size, dim\n","        pop_pos=numpy.copy(Sol)\n","        #Evaluate initial random solutions\n","        pop_fit=numpy.zeros(self.pop_size)\n","        \n","        for i in range(0,self.pop_size):    \n","            pop_fit[i]=self.objf(pop_pos[i,:],self.trainInput,self.trainOutput,self.dim) #fitness\n","        \n","        best_f = float('inf')\n","        best_x = []\n","        for i in range(self.pop_size):\n","            if pop_fit[i] <= best_f:\n","                best_f = pop_fit[i]\n","                best_x = pop_pos[i, :]\n","\n","        his_best_fit = np.zeros(self.epoch)\n","\n","        for epoch in range(self.epoch):\n","            \n","            Direct1 = np.zeros((self.pop_size, self.dim))\n","            Direct2 = np.zeros((self.pop_size, self.dim))\n","            theta=2*(1-epoch/self.epoch)\n","\n","            pos_new = []\n","            for i in range(0, self.pop_size):\n","                L=(np.exp(1)-np.exp(((epoch-1)/self.epoch)**2))*(np.sin(2*np.pi*np.random.rand())) #Eq.(3)\n","                rd=np.ceil(np.random.rand()*(self.dim))\n","                Direct1[i,np.random.randint(self.dim, size=int(rd))]=1\n","                c=Direct1[i,:]#Eq.(4)            \n","                # R=L*c \n","                R = np.multiply(L,c)#Eq.(2)\n","                A = 2 * np.log(1 / np.random.rand()) * theta #Eq.(15)           \n","                ## Exploration\n","                A=0\n","                if A>1:\n","                    K = np.array([np.arange(1,i - 1+1),np.arange(i + 1,self.pop_size+1)])\n","                    # RandInd = K(np.random.randint(np.array([1,self.pop_size - 1])))\n","                    RandInd = np.random.randint(np.array([1,self.pop_size - 1]))[1]\n","                    pos_new = pop_pos[RandInd,:]+ np.multiply(R,(pop_pos[i,:] - pop_pos[RandInd,:]))++ np.round(0.5 * (0.05 + np.random.rand())) * np.random.randn()#Eq.(1)\n","\n","                ## Exploitation\n","                else:\n","                    rd2=np.abs(np.ceil(np.random.randn() * self.dim))\n","                    Direct2[i,np.random.randint(self.dim, size=int(rd2))]=1\n","                    gr = Direct2[i,:]\n","                    H = ((self.epoch - epoch + 1) / self.epoch) * np.random.randn()\n","                    b = pop_pos[i,:] + np.multiply(H * gr,pop_pos[i,:])\n","                    pos_new = pop_pos[i,:] + np.multiply(R,(np.random.randn() * b - pop_pos[i,:]))\n","\n","                fit_new, pos_new_binary = self.get_fitness_position(pos_new)\n","                if fit_new < pop[i][self.ID_FIT]:\n","                    pop[i] = [pos_new, fit_new, pos_new_binary]\n","\n","            # for i in range(self.pop_size):\n","            #     if pop[i][self.ID_FIT] < best_f:\n","            #         best_f = pop[i][self.ID_FIT]\n","            #         best_x = pop_pos[i, :]\n","            \n","            for i in range(self.pop_size):\n","                if pop[i][self.ID_FIT] < fit_new:\n","                    fit_new = pop[i][self.ID_FIT]\n","                    pos_new = pop[i][self.ID_POS]\n","                pop[i] = [pos_new, fit_new, pos_new_binary]            \n","\n","            his_best_fit[epoch] = fit_new          \n","\n","            g_best, g_worst = self.update_global_best_global_worst_solution(pop, self.ID_MIN_PROB, self.ID_MAX_PROB, g_best)\n","            \n","            for i in range(self.pop_size):\n","                pop_pos[i, :]=g_best[self.ID_POS]\n","                pop_fit[i]=g_best[self.ID_FIT]\n","\n","            self.loss_train.append(g_best[self.ID_FIT])\n","            self.fs_counts.append(sum(g_best[self.ID_POS]))\n","                \n","            if (epoch%1==0):\n","                \n","                print(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[self.ID_POS]))])\n","                self.logger.info(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[self.ID_POS]))])\n","        \n","        self.solution = g_best\n","        return g_best[self.ID_POS], g_best[self.ID_FIT], self.loss_train, self.fs_counts\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hSq75-2VtkrI"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","# from .DYHGSClass import DYHGSClass\n","from numpy.random import uniform, normal, rand\n","\n","    \n","class aro:\n","\n","      def ARO(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","    \n","            #Max_iter=1000\n","            #lb=-100\n","            #ub=100\n","            #dim=30  \n","            #SearchAgents_no=5\n","            lb=0\n","            # initialize alpha, beta, and delta_pos\n","            g_best=numpy.zeros(dim)\n","            Alpha_score=float(\"inf\")\n","            XXX=1\n","            verbose = True\n","            # Loop counter\n","            print(\"ARO is optimizing  \\\"\"+objf.__name__+\"\\\"\")   \n","            LOGGER_FILE.info(\"ARO is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","            s=solution()\n","            timerStart=time.time() \n","            s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            lb = [lb] * dim\n","            ub = [ub] * dim\n","            # print(trainInput.shape)\n","\n","            best_pos1, best_fit1, list_loss1, fs_counts = AROClass(objf, lb, ub, verbose, Max_iter, SearchAgents_no,0.08, 10000,trainInput=trainInput,trainOutput=trainOutput,dim=dim, logger = LOGGER_FILE).train()\n","            # print(md1)\n","            # best_pos1, best_fit1, list_loss1 = md1.train()\n","            \n","            timerEnd=time.time()  \n","            s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            s.executionTime=timerEnd-timerStart\n","            s.bestIndividual=best_pos1\n","            s.convergence1=list_loss1\n","            s.convergence2=fs_counts\n","            \n","\n","            s.optimizer=\"ARO\"\n","            s.objfname=objf.__name__\n","            \n","            return s"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EVVO3on-gi_c"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"FW7H2D_egkV5"},"source":["## 25- MuARO\n","Artificial Rabbits Optimization\n","\n","the code is here: https://www.mathworks.com/matlabcentral/fileexchange/110250-artificial-rabbits-optimization-aro?s_tid=FX_rc3_behav"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"auNSYicPgkV5"},"outputs":[],"source":["class Root:\n","    \"\"\" This is root of all Algorithms \"\"\"\n","\n","    ID_MIN_PROB = 0  # min problem\n","    ID_MAX_PROB = -1  # max problem\n","\n","    ID_POS = 0  # Position\n","    ID_FIT = 1  # Fitness\n","\n","    EPSILON = 10E-10    # Avoid division by 0 \n","\n","    DEFAULT_LB = -1\n","    DEFAULT_UB = 1\n","\n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True):\n","        self.verbose = verbose\n","        self.obj_func = obj_func\n","        if lb is None:\n","            print(\"Lower bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.lb = array(lb)\n","        if ub is None:\n","            print(\"Upper bound need to be a list.\")\n","            exit(0)\n","        else:\n","            self.ub = array(ub)\n","           \n","        if len(lb) != len(ub):\n","            print(\"Lower bound and Upper bound need to have the same length\")\n","            exit(0)\n","        \n","        self.problem_size = len(lb)\n","        self.solution, self.loss_train, self.fs_counts = None, [], []\n","\n","    def create_solution(self, minmax=0):\n","        \"\"\" Return solution with 2 element: position of solution and fitness of solution\n","        Parameters\n","        ----------\n","        minmax\n","            0 - minimum problem, else - maximum problem\n","        \"\"\"\n","        position = uniform(self.lb, self.ub)\n","        # print(position)\n","        # position=numpy.random.randint(2, size=(self.pop_size,self.dim))\n","        # for i in range(0,self.pop_size):\n","        #   while numpy.sum(position[i,:])==0:   \n","        #           position[i,:]=numpy.random.randint(2, size=(1,self.dim))\n","        fitness, binary_pos = self.get_fitness_position(position=position, minmax=minmax)\n","        return [position, fitness, binary_pos]\n","\n","    def get_fitness_position(self,position=None, minmax=0):\n","        \"\"\"     Assumption that objective function always return the original value\n","        :param position: 1-D numpy array\n","        :param minmax: 0- min problem, 1 - max problem\n","        :return:\n","        \"\"\"\n","        # print(position)\n","        Positions=position;\n","         \n","        \n","        for j in range(0,self.dim):\n","          ss= transfer_functions_benchmark.s1(position[j])\n","          if (random.random()<ss): \n","              Positions[j]=1;\n","          else:\n","              Positions[j]=0;\n","\n","        return self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) if minmax == 0 else 1.0 / (self.obj_func(Positions,self.trainInput,self.trainOutput,self.dim) + self.EPSILON), Positions     # Avoid division by 0\n","\n","    def get_fitness_solution(self,solution=None, minmax=0):\n","        return self.get_fitness_position(solution[self.ID_POS],self.trainInput,self.trainOutput,self.dim, minmax)\n","\n","    def get_global_best_global_worst_solution(self, pop=None, id_fit=None, id_best=None):\n","        sorted_pop = sorted(pop, key=lambda temp: temp[id_fit])\n","        if id_best == self.ID_MIN_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MAX_PROB])\n","        elif id_best == self.ID_MAX_PROB:\n","            return deepcopy(sorted_pop[id_best]), deepcopy(sorted_pop[self.ID_MIN_PROB])\n","\n","\n","    def update_global_best_global_worst_solution(self, pop=None, id_best=None, id_worst=None, g_best=None):\n","        \"\"\" Sort the copy of population and update the current best position. Return the new current best position \"\"\"\n","        sorted_pop = sorted(pop, key=lambda temp: temp[self.ID_FIT])\n","        current_best = sorted_pop[id_best]\n","        g_best = deepcopy(current_best) if current_best[self.ID_FIT] < g_best[self.ID_FIT] else deepcopy(g_best)\n","        return g_best, sorted_pop[id_worst]\n","\n","\n","    def amend_position(self, position=None):\n","        return clip(position, self.lb, self.ub)\n","\n","    def amend_position_random(self, position=None):\n","        return where(logical_and(self.lb <= position, position <= self.ub), position, uniform(self.lb, self.ub))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SB_3ikLogkV6"},"outputs":[],"source":["from numpy.random import normal, rand, choice\n","from numpy import mean, pi, sin, cos, array\n","from math import gamma\n","# from mealpy.optimizer import Root\n","from copy import deepcopy\n","from torch import randperm\n","\n","class MuAROClass(Root):\n","  \n","    def __init__(self, obj_func=None, lb=list, ub=list, verbose=True, epoch=1000, pop_size=100, L=0.08, LH=10000,trainInput=None,trainOutput=None,dim=None, logger=None):  \n","            \"\"\"\n","            Args:\n","                problem (dict): The problem dictionary\n","                epoch (int): maximum number of iterations, default = 10000\n","                pop_size (int): number of population size, default = 100\n","                pb (float): breeding probability, default = 0.75\n","                pm (float): probability of mutation, default = 0.01\n","            \"\"\"\n","            super().__init__(obj_func,lb, ub, verbose)\n","            self.epoch = epoch\n","            self.pop_size = pop_size\n","            self.nfe_per_epoch = self.pop_size\n","            self.sort_flag = True\n","            self.size_b = int(self.pop_size / 5)\n","            self.lb=lb\n","            self.ub=ub\n","            self.objf=obj_func\n","\n","            self.pb = 0.75#self.validator.check_float(\"pb\", pb, (0, 1.0))\n","            self.pm = 0.01#self.validator.check_float(\"pm\", pm, (0, 1.0))\n","\n","            self.trainInput=trainInput\n","            self.dim=dim\n","            self.trainOutput=trainOutput\n","            self.logger = logger\n","\n","    def _crossover_random(self, pop, g_best):\n","        start_point = np.random.randint(0, self.dim / 2)\n","        id1 = start_point\n","        id2 = int(start_point + self.dim / 3)\n","        id3 = int(self.dim)\n","\n","        partner = pop[np.random.randint(0, self.pop_size)][self.ID_POS]\n","        new_temp = deepcopy(g_best[self.ID_POS])\n","        new_temp[0:id1] = g_best[self.ID_POS][0:id1]\n","        new_temp[id1:id2] = partner[id1:id2]\n","        new_temp[id2:id3] = g_best[self.ID_POS][id2:id3]\n","        return new_temp\n","\n","    def get_levy_flight_step(self, beta=1.0, multiplier=0.001, case=0):\n","        # u and v are two random variables which follow np.random.normal distribution\n","        # sigma_u : standard deviation of u\n","        sigma_u = np.power(gamma(1 + beta) * np.sin(np.pi * beta / 2) / (gamma((1 + beta) / 2) * beta * np.power(2, (beta - 1) / 2)), 1 / beta)\n","        # sigma_v : standard deviation of v\n","        sigma_v = 1\n","        u = np.random.normal(0, sigma_u ** 2)\n","        v = np.random.normal(0, sigma_v ** 2)\n","        s = u / np.power(abs(v), 1 / beta)\n","        if case == 0:\n","            step = multiplier * s * np.random.uniform()\n","        elif case == 1:\n","            step = multiplier * s * np.random.normal(0, 1)\n","        else:\n","            step = multiplier * s\n","        return step\n","\n","    def train(self):\n","    \n","        pop_new = []\n","        #Initialize population with random positions\n","        pop = [self.create_solution() for _ in range(self.pop_size)]\n","        # #assign fitness to fprey.\n","        g_best, g_worst = self.get_global_best_global_worst_solution(pop, self.ID_FIT, self.ID_MIN_PROB)\n","\n","        Sol=numpy.random.randint(2, size=(self.pop_size,self.dim))     #generating binary individuals\n","        pop_pos=numpy.zeros((self.pop_size,self.dim))  # Population size, dim\n","        pop_pos=numpy.copy(Sol)\n","        #Evaluate initial random solutions\n","        pop_fit=numpy.zeros(self.pop_size)\n","        \n","        for i in range(0,self.pop_size):    \n","            pop_fit[i]=self.objf(pop_pos[i,:],self.trainInput,self.trainOutput,self.dim) #fitness\n","        \n","        best_f = float('inf')\n","        best_x = []\n","        for i in range(self.pop_size):\n","            if pop_fit[i] <= best_f:\n","                best_f = pop_fit[i]\n","                best_x = pop_pos[i, :]\n","\n","        his_best_fit = np.zeros(self.epoch)\n","\n","        for epoch in range(self.epoch):\n","            \n","            Direct1 = np.zeros((self.pop_size, self.dim))\n","            Direct2 = np.zeros((self.pop_size, self.dim))\n","            theta=2*(1-epoch/self.epoch)\n","\n","            pos_new = []\n","            for i in range(0, self.pop_size):\n","                L=(np.exp(1)-np.exp(((epoch-1)/self.epoch)**2))*(np.sin(2*np.pi*np.random.rand())) #Eq.(3)\n","                rd=np.ceil(np.random.rand()*(self.dim))\n","                Direct1[i,np.random.randint(self.dim, size=int(rd))]=1\n","                c=Direct1[i,:]#Eq.(4)            \n","                # R=L*c \n","                R = np.multiply(L,c)#Eq.(2)\n","                A = 2 * np.log(1 / np.random.rand()) * theta #Eq.(15)           \n","                ## Exploration\n","                A=0\n","                if A>1:\n","                    K = np.array([np.arange(1,i - 1+1),np.arange(i + 1,self.pop_size+1)])\n","                    # RandInd = K(np.random.randint(np.array([1,self.pop_size - 1])))\n","                    RandInd = np.random.randint(np.array([1,self.pop_size - 1]))[1]\n","                    pos_new = pop_pos[RandInd,:]+ np.multiply(R,(pop_pos[i,:] - pop_pos[RandInd,:]))++ np.round(0.5 * (0.05 + np.random.rand())) * np.random.randn()#Eq.(1)\n","\n","                ## Exploitation\n","                else:\n","                    if np.random.uniform() < 0.5:\n","                        rd2=np.abs(np.ceil(np.random.randn() * self.dim))\n","                        Direct2[i,np.random.randint(self.dim, size=int(rd2))]=1\n","                        gr = Direct2[i,:]\n","                        H = ((self.epoch - epoch + 1) / self.epoch) * np.random.randn()\n","                        b = pop_pos[i,:] + np.multiply(H * gr,pop_pos[i,:])\n","                        pos_new = pop_pos[i,:] + np.multiply(R,(np.random.randn() * b - pop_pos[i,:]))\n","                    else:\n","                        pos_new = self._crossover_random(pop, g_best)\n","                # Mutation\n","                temp = np.random.uniform(self.lb, self.ub)\n","                pos_new = np.where(np.random.uniform(0, 1, self.dim) < self.pm, temp, pos_new)\n","                # pos_new = self.amend_position(pos_new, self.lb, self.ub)\n","                # pop_new.append([pos_new, None])\n","                \n","                fit_new, pos_new_binary = self.get_fitness_position(pos_new)\n","                if fit_new < pop[i][self.ID_FIT]:\n","                    pop[i] = [pos_new, fit_new, pos_new_binary]\n","\n","            # for i in range(self.pop_size):\n","            #     if pop[i][self.ID_FIT] < best_f:\n","            #         best_f = pop[i][self.ID_FIT]\n","            #         best_x = pop_pos[i, :]\n","            \n","            for i in range(self.pop_size):\n","                if pop[i][self.ID_FIT] < fit_new:\n","                    fit_new = pop[i][self.ID_FIT]\n","                    pos_new = pop[i][self.ID_POS]\n","                pop[i] = [pos_new, fit_new, pos_new_binary]            \n","\n","            his_best_fit[epoch] = fit_new          \n","\n","            g_best, g_worst = self.update_global_best_global_worst_solution(pop, self.ID_MIN_PROB, self.ID_MAX_PROB, g_best)\n","            \n","            for i in range(self.pop_size):\n","                pop_pos[i, :]=g_best[self.ID_POS]\n","                pop_fit[i]=g_best[self.ID_FIT]\n","\n","            self.loss_train.append(g_best[self.ID_FIT])\n","            self.fs_counts.append(sum(g_best[self.ID_POS]))\n","                \n","            if (epoch%1==0):\n","                \n","                print(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[self.ID_POS]))])\n","                self.logger.info(['At iteration'+ str(epoch+1)+' the best fitness on trainig is:'+ str(g_best[self.ID_FIT])+', the best number of features: '+str(sum(g_best[self.ID_POS]))])\n","        \n","        self.solution = g_best\n","        return g_best[self.ID_POS], g_best[self.ID_FIT], self.loss_train, self.fs_counts\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gZlqG_2NgkV7"},"outputs":[],"source":["import random\n","import numpy\n","import math\n","import time\n","# from utils.solution import solution\n","# import utils.transfer_functions_benchmark as transfer_functions_benchmark\n","# import utils.fitnessFUNs as fitnessFUNs\n","# from .DYHGSClass import DYHGSClass\n","from numpy.random import uniform, normal, rand\n","\n","    \n","class muaro:\n","\n","      def MuARO(objf,lb,ub,dim,SearchAgents_no,Max_iter,trainInput,trainOutput,LOGGER_FILE):\n","    \n","            #Max_iter=1000\n","            #lb=-100\n","            #ub=100\n","            #dim=30  \n","            #SearchAgents_no=5\n","            lb=0\n","            # initialize alpha, beta, and delta_pos\n","            g_best=numpy.zeros(dim)\n","            Alpha_score=float(\"inf\")\n","            XXX=1\n","            verbose = True\n","            # Loop counter\n","            print(\"MuARO is optimizing  \\\"\"+objf.__name__+\"\\\"\")   \n","            LOGGER_FILE.info(\"MuARO is optimizing  \\\"\"+objf.__name__+\"\\\"\")  \n","            s=solution()\n","            timerStart=time.time() \n","            s.startTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            lb = [lb] * dim\n","            ub = [ub] * dim\n","            # print(trainInput.shape)\n","\n","            best_pos1, best_fit1, list_loss1, fs_counts = MuAROClass(objf, lb, ub, verbose, Max_iter, SearchAgents_no,0.08, 10000,trainInput=trainInput,trainOutput=trainOutput,dim=dim, logger = LOGGER_FILE).train()\n","            # print(md1)\n","            # best_pos1, best_fit1, list_loss1 = md1.train()\n","            \n","            timerEnd=time.time()  \n","            s.endTime=time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","            s.executionTime=timerEnd-timerStart\n","            s.bestIndividual=best_pos1\n","            s.convergence1=list_loss1\n","            s.convergence2=fs_counts\n","            \n","\n","            s.optimizer=\"MuARO\"\n","            s.objfname=objf.__name__\n","            \n","            return s"]},{"cell_type":"markdown","metadata":{"id":"F7_MNDTTs7o-"},"source":["# lazyPred_src"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pd4GFwg2s-ND"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from copy import deepcopy\n","import datetime\n","import time\n","import sklearn\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer, MissingIndicator\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.utils import all_estimators\n","from sklearn.base import RegressorMixin\n","from sklearn.base import ClassifierMixin\n","from sklearn.metrics import (\n","    accuracy_score,\n","    balanced_accuracy_score,\n","    roc_auc_score,\n","    f1_score,\n","    r2_score,\n","    precision_score,\n","    recall_score,\n","    mean_squared_error,\n",")\n","from sklearn.metrics import cohen_kappa_score\n","\n","import warnings\n","import xgboost\n","\n","# import catboost\n","import lightgbm\n","from sklearn.utils._mocking import CheckingClassifier\n","\n","warnings.filterwarnings(\"ignore\")\n","pd.set_option(\"display.precision\", 4)\n","pd.set_option(\"display.float_format\", lambda x: \"%.4f\" % x)\n","\n","CLASSIFIERS = [est for est in all_estimators() if issubclass(est[1], ClassifierMixin)]\n","REGRESSORS = [est for est in all_estimators() if issubclass(est[1], RegressorMixin)]\n","\n","removed_classifiers = [\n","    # (\"CheckingClassifier\", sklearn.utils._mocking.CheckingClassifier),\n","    (\"ClassifierChain\", sklearn.multioutput.ClassifierChain),\n","    (\"ComplementNB\", sklearn.naive_bayes.ComplementNB),\n","    (\n","        \"GradientBoostingClassifier\",\n","        sklearn.ensemble.GradientBoostingClassifier,\n","    ),\n","    (\n","        \"GaussianProcessClassifier\",\n","        sklearn.gaussian_process.GaussianProcessClassifier,\n","    ),\n","    (\n","        \"HistGradientBoostingClassifier\",\n","        sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,\n","    ),\n","    # (\"MLPClassifier\", sklearn.neural_network.multilayer_perceptron.MLPClassifier),\n","    (\"LogisticRegressionCV\", sklearn.linear_model.LogisticRegressionCV),\n","    (\"MultiOutputClassifier\", sklearn.multioutput.MultiOutputClassifier),\n","    ('LabelPropagation', sklearn.semi_supervised._label_propagation.LabelPropagation),\n","    ('LabelSpreading', sklearn.semi_supervised._label_propagation.LabelSpreading),\n","    ('NuSVC', sklearn.svm._classes.NuSVC),\n","    ('GaussianNB', sklearn.naive_bayes.GaussianNB),\n","\n","\n","\n","    # (\"MultinomialNB\", sklearn.naive_bayes.MultinomialNB),\n","    # (\"OneVsOneClassifier\", sklearn.multiclass.OneVsOneClassifier),\n","    # (\"OneVsRestClassifier\", sklearn.multiclass.OneVsRestClassifier),\n","    (\"OutputCodeClassifier\", sklearn.multiclass.OutputCodeClassifier),\n","    (\n","        \"RadiusNeighborsClassifier\",\n","        sklearn.neighbors.RadiusNeighborsClassifier,\n","    ),\n","    # (\"VotingClassifier\", sklearn.ensemble.voting.VotingClassifier),\n","]\n","\n","removed_regressors = [\n","    (\"TheilSenRegressor\", sklearn.linear_model.TheilSenRegressor),\n","    (\"ARDRegression\", sklearn.linear_model.ARDRegression),\n","    (\"CCA\", sklearn.cross_decomposition.CCA),\n","    (\"IsotonicRegression\", sklearn.isotonic.IsotonicRegression),\n","    (\"StackingRegressor\",sklearn.ensemble.StackingRegressor),\n","    (\"MultiOutputRegressor\", sklearn.multioutput.MultiOutputRegressor),\n","    (\"MultiTaskElasticNet\", sklearn.linear_model.MultiTaskElasticNet),\n","    (\"MultiTaskElasticNetCV\", sklearn.linear_model.MultiTaskElasticNetCV),\n","    (\"MultiTaskLasso\", sklearn.linear_model.MultiTaskLasso),\n","    (\"MultiTaskLassoCV\", sklearn.linear_model.MultiTaskLassoCV),\n","    (\"PLSCanonical\", sklearn.cross_decomposition.PLSCanonical),\n","    (\"PLSRegression\", sklearn.cross_decomposition.PLSRegression),\n","    (\"RadiusNeighborsRegressor\", sklearn.neighbors.RadiusNeighborsRegressor),\n","    (\"RegressorChain\", sklearn.multioutput.RegressorChain),\n","    (\"VotingRegressor\", sklearn.ensemble.VotingRegressor),\n","    # (\"_SigmoidCalibration\", sklearn.calibration._SigmoidCalibration),\n","]\n","\n","for i in removed_regressors:\n","    REGRESSORS.pop(REGRESSORS.index(i))\n","\n","for i in removed_classifiers:\n","    CLASSIFIERS.pop(CLASSIFIERS.index(i))\n","\n","REGRESSORS.append((\"XGBRegressor\", xgboost.XGBRegressor))\n","REGRESSORS.append((\"LGBMRegressor\", lightgbm.LGBMRegressor))\n","# REGRESSORS.append(('CatBoostRegressor',catboost.CatBoostRegressor))\n","\n","CLASSIFIERS.append((\"XGBClassifier\", xgboost.XGBClassifier))\n","CLASSIFIERS.append((\"LGBMClassifier\", lightgbm.LGBMClassifier))\n","# CLASSIFIERS.append(('CatBoostClassifier',catboost.CatBoostClassifier))\n","\n","numeric_transformer = Pipeline(\n","    steps=[(\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())]\n",")\n","\n","categorical_transformer_low = Pipeline(\n","    steps=[\n","        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n","        (\"encoding\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n","    ]\n",")\n","\n","categorical_transformer_high = Pipeline(\n","    steps=[\n","        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n","        # 'OrdianlEncoder' Raise a ValueError when encounters an unknown value. Check https://github.com/scikit-learn/scikit-learn/pull/13423\n","        (\"encoding\", OrdinalEncoder()),\n","    ]\n",")\n","\n","\n","# Helper function\n","\n","\n","def get_card_split(df, cols, n=11):\n","    \"\"\"\n","    Splits categorical columns into 2 lists based on cardinality (i.e # of unique values)\n","    Parameters\n","    ----------\n","    df : Pandas DataFrame\n","        DataFrame from which the cardinality of the columns is calculated.\n","    cols : list-like\n","        Categorical columns to list\n","    n : int, optional (default=11)\n","        The value of 'n' will be used to split columns.\n","    Returns\n","    -------\n","    card_low : list-like\n","        Columns with cardinality < n\n","    card_high : list-like\n","        Columns with cardinality >= n\n","    \"\"\"\n","    cond = df[cols].nunique() > n\n","    card_high = cols[cond]\n","    card_low = cols[~cond]\n","    return card_low, card_high\n","\n","\n","# Helper class for performing classification\n","\n","class LazyClassifier:\n","    \"\"\"\n","    This module helps in fitting to all the classification algorithms that are available in Scikit-learn\n","    Parameters\n","    ----------\n","    verbose : int, optional (default=0)\n","        For the liblinear and lbfgs solvers set verbose to any positive\n","        number for verbosity.\n","    ignore_warnings : bool, optional (default=True)\n","        When set to True, the warning related to algorigms that are not able to run are ignored.\n","    custom_metric : function, optional (default=None)\n","        When function is provided, models are evaluated based on the custom evaluation metric provided.\n","    prediction : bool, optional (default=False)\n","        When set to True, the predictions of all the models models are returned as dataframe.\n","    classifiers : list, optional (default=\"all\")\n","        When function is provided, trains the chosen classifier(s).\n","    Examples\n","    --------\n","    >>> from lazypredict.Supervised import LazyClassifier\n","    >>> from sklearn.datasets import load_breast_cancer\n","    >>> from sklearn.model_selection import train_test_split\n","    >>> data = load_breast_cancer()\n","    >>> X = data.data\n","    >>> y= data.target\n","    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.5,random_state =123)\n","    >>> clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n","    >>> models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n","    >>> model_dictionary = clf.provide_models(X_train,X_test,y_train,y_test)\n","    >>> models\n","    | Model                          |   Accuracy |   Balanced Accuracy |   ROC AUC |   F1 Score |   Time Taken |\n","    |:-------------------------------|-----------:|--------------------:|----------:|-----------:|-------------:|\n","    | LinearSVC                      |   0.989474 |            0.987544 |  0.987544 |   0.989462 |    0.0150008 |\n","    | SGDClassifier                  |   0.989474 |            0.987544 |  0.987544 |   0.989462 |    0.0109992 |\n","    | MLPClassifier                  |   0.985965 |            0.986904 |  0.986904 |   0.985994 |    0.426     |\n","    | Perceptron                     |   0.985965 |            0.984797 |  0.984797 |   0.985965 |    0.0120046 |\n","    | LogisticRegression             |   0.985965 |            0.98269  |  0.98269  |   0.985934 |    0.0200036 |\n","    | LogisticRegressionCV           |   0.985965 |            0.98269  |  0.98269  |   0.985934 |    0.262997  |\n","    | SVC                            |   0.982456 |            0.979942 |  0.979942 |   0.982437 |    0.0140011 |\n","    | CalibratedClassifierCV         |   0.982456 |            0.975728 |  0.975728 |   0.982357 |    0.0350015 |\n","    | PassiveAggressiveClassifier    |   0.975439 |            0.974448 |  0.974448 |   0.975464 |    0.0130005 |\n","    | LabelPropagation               |   0.975439 |            0.974448 |  0.974448 |   0.975464 |    0.0429988 |\n","    | LabelSpreading                 |   0.975439 |            0.974448 |  0.974448 |   0.975464 |    0.0310006 |\n","    | RandomForestClassifier         |   0.97193  |            0.969594 |  0.969594 |   0.97193  |    0.033     |\n","    | GradientBoostingClassifier     |   0.97193  |            0.967486 |  0.967486 |   0.971869 |    0.166998  |\n","    | QuadraticDiscriminantAnalysis  |   0.964912 |            0.966206 |  0.966206 |   0.965052 |    0.0119994 |\n","    | HistGradientBoostingClassifier |   0.968421 |            0.964739 |  0.964739 |   0.968387 |    0.682003  |\n","    | RidgeClassifierCV              |   0.97193  |            0.963272 |  0.963272 |   0.971736 |    0.0130029 |\n","    | RidgeClassifier                |   0.968421 |            0.960525 |  0.960525 |   0.968242 |    0.0119977 |\n","    | AdaBoostClassifier             |   0.961404 |            0.959245 |  0.959245 |   0.961444 |    0.204998  |\n","    | ExtraTreesClassifier           |   0.961404 |            0.957138 |  0.957138 |   0.961362 |    0.0270066 |\n","    | KNeighborsClassifier           |   0.961404 |            0.95503  |  0.95503  |   0.961276 |    0.0560005 |\n","    | BaggingClassifier              |   0.947368 |            0.954577 |  0.954577 |   0.947882 |    0.0559971 |\n","    | BernoulliNB                    |   0.950877 |            0.951003 |  0.951003 |   0.951072 |    0.0169988 |\n","    | LinearDiscriminantAnalysis     |   0.961404 |            0.950816 |  0.950816 |   0.961089 |    0.0199995 |\n","    | GaussianNB                     |   0.954386 |            0.949536 |  0.949536 |   0.954337 |    0.0139935 |\n","    | NuSVC                          |   0.954386 |            0.943215 |  0.943215 |   0.954014 |    0.019989  |\n","    | DecisionTreeClassifier         |   0.936842 |            0.933693 |  0.933693 |   0.936971 |    0.0170023 |\n","    | NearestCentroid                |   0.947368 |            0.933506 |  0.933506 |   0.946801 |    0.0160074 |\n","    | ExtraTreeClassifier            |   0.922807 |            0.912168 |  0.912168 |   0.922462 |    0.0109999 |\n","    | CheckingClassifier             |   0.361404 |            0.5      |  0.5      |   0.191879 |    0.0170043 |\n","    | DummyClassifier                |   0.512281 |            0.489598 |  0.489598 |   0.518924 |    0.0119965 |\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        verbose=0,\n","        ignore_warnings=True,\n","        custom_metric=None,\n","        predictions=False,\n","        random_state=42,\n","        classifiers = \"all\"\n","    ):\n","        self.verbose = verbose\n","        self.ignore_warnings = ignore_warnings\n","        self.custom_metric = custom_metric\n","        self.predictions = predictions\n","        self.models = {}\n","        self.random_state = random_state\n","        self.classifiers = classifiers\n","\n","    def fit(self, X_train, X_test, y_train, y_test):\n","        \"\"\"Fit Classification algorithms to X_train and y_train, predict and score on X_test, y_test.\n","        Parameters\n","        ----------\n","        X_train : array-like,\n","            Training vectors, where rows is the number of samples\n","            and columns is the number of features.\n","        X_test : array-like,\n","            Testing vectors, where rows is the number of samples\n","            and columns is the number of features.\n","        y_train : array-like,\n","            Training vectors, where rows is the number of samples\n","            and columns is the number of features.\n","        y_test : array-like,\n","            Testing vectors, where rows is the number of samples\n","            and columns is the number of features.\n","        Returns\n","        -------\n","        scores : Pandas DataFrame\n","            Returns metrics of all the models in a Pandas DataFrame.\n","        predictions : Pandas DataFrame\n","            Returns predictions of all the models in a Pandas DataFrame.\n","        \"\"\"\n","        \n","\n","        if isinstance(X_train, np.ndarray):\n","            X_train = pd.DataFrame(X_train)\n","            X_test = pd.DataFrame(X_test)\n","\n","        numeric_features = X_train.select_dtypes(include=[np.number]).columns\n","        categorical_features = X_train.select_dtypes(include=[\"object\"]).columns\n","\n","        transformers = []\n","\n","        if len(numeric_features) > 0:\n","            transformers.append((\"numeric\", numeric_transformer, numeric_features))\n","        if len(categorical_features) > 0:\n","            categorical_low, categorical_high = get_card_split(\n","                X_train, categorical_features\n","            )\n","\n","            if len(categorical_low) > 0:\n","                transformers.append((\"categorical_low\", categorical_transformer_low, categorical_low))\n","            if len(categorical_high) > 0:\n","                transformers.append((\"categorical_high\", categorical_transformer_high, categorical_high))\n","        \n","        preprocessor = ColumnTransformer(transformers=transformers)\n","\n","        if self.classifiers == \"all\":\n","            # self.classifiers = deepcopy(CLASSIFIERS)\n","            self.classifiers = CLASSIFIERS\n","\n","        else:\n","            try:\n","                temp_list = []\n","                for classifier in self.classifiers:\n","                    full_name = (classifier.__class__.__name__, classifier.__class__)\n","                    temp_list.append(full_name)\n","                self.classifiers = temp_list\n","            except Exception as exception:\n","                print(exception)\n","                print(\"Invalid Classifier(s)\")\n","\n","\n","        Accuracy = []\n","        B_Accuracy = []\n","        ROC_AUC = []\n","        F1 = []\n","        L_recall = []\n","        L_precision = []\n","        names = []\n","        TIME = []\n","        predictions = {}\n","        CUSTOM_METRIC = []\n","        Kappa =[]\n","\n","        for name, model in tqdm(self.classifiers):\n","            print(name)\n","            start = time.time()\n","            try:\n","                if \"random_state\" in model().get_params().keys():\n","                    pipe = Pipeline(\n","                        steps=[\n","                            (\"preprocessor\", preprocessor),\n","                            (\"classifier\", model(random_state=self.random_state)),\n","                        ]\n","                    )\n","                else:\n","                    pipe = Pipeline(\n","                        steps=[(\"preprocessor\", preprocessor), (\"classifier\", model())]\n","                    )\n","\n","                pipe.fit(X_train, y_train)\n","                self.models[name] = pipe\n","                y_pred = pipe.predict(X_test)\n","                accuracy = accuracy_score(y_test, y_pred, normalize=True)\n","                b_accuracy = balanced_accuracy_score(y_test, y_pred)\n","                kappa = cohen_kappa_score(y_test, y_pred)\n","                f1 = f1_score(y_test, y_pred, average=\"weighted\")\n","                precision = precision_score(y_test, y_pred, average=\"weighted\")\n","                recall = recall_score(y_test, y_pred, average=\"weighted\")\n","                \n","                try:\n","                    roc_auc = roc_auc_score(y_test, y_pred)\n","                except Exception as exception:\n","                    roc_auc = None\n","                    if self.ignore_warnings is False:\n","                        print(\"ROC AUC couldn't be calculated for \" + name)\n","                        print(exception)\n","\n","                names.append(name)\n","                Accuracy.append(accuracy)\n","                B_Accuracy.append(b_accuracy)\n","                ROC_AUC.append(roc_auc)\n","                F1.append(f1)\n","                L_recall.append(recall)\n","                L_precision.append(precision)\n","                Kappa.append(kappa)\n","\n","                TIME.append(time.time() - start)\n","                if self.custom_metric is not None:\n","                    custom_metric = self.custom_metric(y_test, y_pred)\n","                    CUSTOM_METRIC.append(custom_metric)\n","                if self.verbose > 0:\n","                    if self.custom_metric is not None:\n","                        print(\n","                            {\n","                                \"Model\": name,\n","                                \"Accuracy\": accuracy,\n","                                \"Balanced Accuracy\": b_accuracy,\n","                                \"ROC AUC\": roc_auc,\n","                                \"F1 Score\": f1,\n","                                \"Recall\": recall,\n","                                \"Precision\": precision,\n","                                \"Kappa\": kappa,\n","                                self.custom_metric.__name__: custom_metric,\n","                                \"Time taken\": time.time() - start,\n","                            }\n","                        )\n","                    else:\n","                        print(\n","                            {\n","                                \"Model\": name,\n","                                \"Accuracy\": accuracy,\n","                                \"Balanced Accuracy\": b_accuracy,\n","                                \"ROC AUC\": roc_auc,\n","                                \"F1 Score\": f1,\n","                                \"Recall\": recall,\n","                                \"Precision\": precision,\n","                                \"Kappa\": kappa,\n","                                \"Time taken\": time.time() - start,\n","                            }\n","                        )\n","                if self.predictions:\n","                    predictions[name] = y_pred\n","            except Exception as exception:\n","                if self.ignore_warnings is False:\n","                    print(name + \" model failed to execute\")\n","                    print(exception)\n","        if self.custom_metric is None:\n","            scores = pd.DataFrame(\n","                {\n","                    \"Model\": names,\n","                    \"Accuracy\": Accuracy,\n","                    \"Balanced Accuracy\": B_Accuracy,\n","                    \"ROC AUC\": ROC_AUC,\n","                    \"F1 Score\": F1,\n","                    \"Recall\": L_recall,\n","                    \"Precision\": L_precision,\n","                    \"Kappa\": kappa,\n","                    \"Time Taken\": TIME,\n","                }\n","            )\n","        else:\n","            scores = pd.DataFrame(\n","                {\n","                    \"Model\": names,\n","                    \"Accuracy\": Accuracy,\n","                    \"Balanced Accuracy\": B_Accuracy,\n","                    \"ROC AUC\": ROC_AUC,\n","                    \"F1 Score\": F1,\n","                    \"Recall\": L_recall,\n","                    \"Precision\": L_precision,\n","                    \"Kappa\": kappa,\n","                    self.custom_metric.__name__: CUSTOM_METRIC,\n","                    \"Time Taken\": TIME,\n","                }\n","            )\n","        scores = scores.sort_values(by=\"Accuracy\", ascending=False).set_index(\n","            \"Model\"\n","        )\n","\n","        if self.predictions:\n","            predictions_df = pd.DataFrame.from_dict(predictions)\n","        return scores, predictions_df if self.predictions is True else scores\n","\n","    def provide_models(self, X_train, X_test, y_train, y_test):\n","        \"\"\"\n","        This function returns all the model objects trained in fit function.\n","        If fit is not called already, then we call fit and then return the models.\n","        Parameters\n","        ----------\n","        X_train : array-like,\n","            Training vectors, where rows is the number of samples\n","            and columns is the number of features.\n","        X_test : array-like,\n","            Testing vectors, where rows is the number of samples\n","            and columns is the number of features.\n","        y_train : array-like,\n","            Training vectors, where rows is the number of samples\n","            and columns is the number of features.\n","        y_test : array-like,\n","            Testing vectors, where rows is the number of samples\n","            and columns is the number of features.\n","        Returns\n","        -------\n","        models: dict-object,\n","            Returns a dictionary with each model pipeline as value\n","            with key as name of models.\n","        \"\"\"\n","        if len(self.models.keys()) == 0:\n","            self.fit(X_train,X_test,y_train,y_test)\n","\n","        return self.models\n","\n","\n","def adjusted_rsquared(r2, n, p):\n","    return 1 - (1-r2) * ((n-1) / (n-p-1))\n","\n","\n","# Helper class for performing classification\n","\n","\n","class LazyRegressor:\n","    \"\"\"\n","    This module helps in fitting regression models that are available in Scikit-learn\n","    Parameters\n","    ----------\n","    verbose : int, optional (default=0)\n","        For the liblinear and lbfgs solvers set verbose to any positive\n","        number for verbosity.\n","    ignore_warnings : bool, optional (default=True)\n","        When set to True, the warning related to algorigms that are not able to run are ignored.\n","    custom_metric : function, optional (default=None)\n","        When function is provided, models are evaluated based on the custom evaluation metric provided.\n","    prediction : bool, optional (default=False)\n","        When set to True, the predictions of all the models models are returned as dataframe.\n","    regressors : list, optional (default=\"all\")\n","        When function is provided, trains the chosen regressor(s).\n","    Examples\n","    --------\n","    >>> from lazypredict.Supervised import LazyRegressor\n","    >>> from sklearn import datasets\n","    >>> from sklearn.utils import shuffle\n","    >>> import numpy as np\n","    >>> boston = datasets.load_boston()\n","    >>> X, y = shuffle(boston.data, boston.target, random_state=13)\n","    >>> X = X.astype(np.float32)\n","    >>> offset = int(X.shape[0] * 0.9)\n","    >>> X_train, y_train = X[:offset], y[:offset]\n","    >>> X_test, y_test = X[offset:], y[offset:]\n","    >>> reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\n","    >>> models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n","    >>> model_dictionary = reg.provide_models(X_train, X_test, y_train, y_test)\n","    >>> models\n","    | Model                         | Adjusted R-Squared | R-Squared |  RMSE | Time Taken |\n","    |:------------------------------|-------------------:|----------:|------:|-----------:|\n","    | SVR                           |               0.83 |      0.88 |  2.62 |       0.01 |\n","    | BaggingRegressor              |               0.83 |      0.88 |  2.63 |       0.03 |\n","    | NuSVR                         |               0.82 |      0.86 |  2.76 |       0.03 |\n","    | RandomForestRegressor         |               0.81 |      0.86 |  2.78 |       0.21 |\n","    | XGBRegressor                  |               0.81 |      0.86 |  2.79 |       0.06 |\n","    | GradientBoostingRegressor     |               0.81 |      0.86 |  2.84 |       0.11 |\n","    | ExtraTreesRegressor           |               0.79 |      0.84 |  2.98 |       0.12 |\n","    | AdaBoostRegressor             |               0.78 |      0.83 |  3.04 |       0.07 |\n","    | HistGradientBoostingRegressor |               0.77 |      0.83 |  3.06 |       0.17 |\n","    | PoissonRegressor              |               0.77 |      0.83 |  3.11 |       0.01 |\n","    | LGBMRegressor                 |               0.77 |      0.83 |  3.11 |       0.07 |\n","    | KNeighborsRegressor           |               0.77 |      0.83 |  3.12 |       0.01 |\n","    | DecisionTreeRegressor         |               0.65 |      0.74 |  3.79 |       0.01 |\n","    | MLPRegressor                  |               0.65 |      0.74 |  3.80 |       1.63 |\n","    | HuberRegressor                |               0.64 |      0.74 |  3.84 |       0.01 |\n","    | GammaRegressor                |               0.64 |      0.73 |  3.88 |       0.01 |\n","    | LinearSVR                     |               0.62 |      0.72 |  3.96 |       0.01 |\n","    | RidgeCV                       |               0.62 |      0.72 |  3.97 |       0.01 |\n","    | BayesianRidge                 |               0.62 |      0.72 |  3.97 |       0.01 |\n","    | Ridge                         |               0.62 |      0.72 |  3.97 |       0.01 |\n","    | TransformedTargetRegressor    |               0.62 |      0.72 |  3.97 |       0.01 |\n","    | LinearRegression              |               0.62 |      0.72 |  3.97 |       0.01 |\n","    | ElasticNetCV                  |               0.62 |      0.72 |  3.98 |       0.04 |\n","    | LassoCV                       |               0.62 |      0.72 |  3.98 |       0.06 |\n","    | LassoLarsIC                   |               0.62 |      0.72 |  3.98 |       0.01 |\n","    | LassoLarsCV                   |               0.62 |      0.72 |  3.98 |       0.02 |\n","    | Lars                          |               0.61 |      0.72 |  3.99 |       0.01 |\n","    | LarsCV                        |               0.61 |      0.71 |  4.02 |       0.04 |\n","    | SGDRegressor                  |               0.60 |      0.70 |  4.07 |       0.01 |\n","    | TweedieRegressor              |               0.59 |      0.70 |  4.12 |       0.01 |\n","    | GeneralizedLinearRegressor    |               0.59 |      0.70 |  4.12 |       0.01 |\n","    | ElasticNet                    |               0.58 |      0.69 |  4.16 |       0.01 |\n","    | Lasso                         |               0.54 |      0.66 |  4.35 |       0.02 |\n","    | RANSACRegressor               |               0.53 |      0.65 |  4.41 |       0.04 |\n","    | OrthogonalMatchingPursuitCV   |               0.45 |      0.59 |  4.78 |       0.02 |\n","    | PassiveAggressiveRegressor    |               0.37 |      0.54 |  5.09 |       0.01 |\n","    | GaussianProcessRegressor      |               0.23 |      0.43 |  5.65 |       0.03 |\n","    | OrthogonalMatchingPursuit     |               0.16 |      0.38 |  5.89 |       0.01 |\n","    | ExtraTreeRegressor            |               0.08 |      0.32 |  6.17 |       0.01 |\n","    | DummyRegressor                |              -0.38 |     -0.02 |  7.56 |       0.01 |\n","    | LassoLars                     |              -0.38 |     -0.02 |  7.56 |       0.01 |\n","    | KernelRidge                   |             -11.50 |     -8.25 | 22.74 |       0.01 |\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        verbose=0,\n","        ignore_warnings=True,\n","        custom_metric=None,\n","        predictions=False,\n","        random_state=42,\n","        regressors=\"all\",\n","    ):\n","        self.verbose = verbose\n","        self.ignore_warnings = ignore_warnings\n","        self.custom_metric = custom_metric\n","        self.predictions = predictions\n","        self.models = {}\n","        self.random_state = random_state\n","        self.regressors = regressors\n","\n","    def fit(self, X_train, X_test, y_train, y_test):\n","        \"\"\"Fit Regression algorithms to X_train and y_train, predict and score on X_test, y_test.\n","        Parameters\n","        ----------\n","        X_train : array-like,\n","            Training vectors, where rows is the number of samples\n","            and columns is the number of features.\n","        X_test : array-like,\n","            Testing vectors, where rows is the number of samples\n","            and columns is the number of features.\n","        y_train : array-like,\n","            Training vectors, where rows is the number of samples\n","            and columns is the number of features.\n","        y_test : array-like,\n","            Testing vectors, where rows is the number of samples\n","            and columns is the number of features.\n","        Returns\n","        -------\n","        scores : Pandas DataFrame\n","            Returns metrics of all the models in a Pandas DataFrame.\n","        predictions : Pandas DataFrame\n","            Returns predictions of all the models in a Pandas DataFrame.\n","        \"\"\"\n","        R2 = []\n","        ADJR2 = []\n","        RMSE = []\n","        # WIN = []\n","        names = []\n","        TIME = []\n","        predictions = {}\n","        CUSTOM_METRIC = []\n","\n","        if isinstance(X_train, np.ndarray):\n","            X_train = pd.DataFrame(X_train)\n","            X_test = pd.DataFrame(X_test)\n","\n","        numeric_features = X_train.select_dtypes(include=[np.number]).columns\n","        categorical_features = X_train.select_dtypes(include=[\"object\"]).columns\n","\n","        transformers = []\n","\n","        if len(numeric_features) > 0:\n","            transformers.append((\"numeric\", numeric_transformer, numeric_features))\n","        if len(categorical_features) > 0:\n","            categorical_low, categorical_high = get_card_split(\n","                X_train, categorical_features\n","            )\n","\n","            if len(categorical_low) > 0:\n","                transformers.append((\"categorical_low\", categorical_transformer_low, categorical_low))\n","            if len(categorical_high) > 0:\n","                transformers.append((\"categorical_high\", categorical_transformer_high, categorical_high))\n","\n","        preprocessor = ColumnTransformer(transformers=transformers)\n","\n","        if self.regressors == \"all\":\n","            self.regressors = REGRESSORS\n","        else:\n","            try:\n","                temp_list = []\n","                for regressor in self.regressors:\n","                    full_name = (regressor.__class__.__name__, regressor)\n","                    temp_list.append(full_name)\n","                self.regressors = temp_list\n","            except Exception as exception:\n","                print(exception)\n","                print(\"Invalid Regressor(s)\")\n","\n","        for name, model in tqdm(self.regressors):\n","            start = time.time()\n","            try:\n","                if \"random_state\" in model().get_params().keys():\n","                    pipe = Pipeline(\n","                        steps=[\n","                            (\"preprocessor\", preprocessor),\n","                            (\"regressor\", model(random_state=self.random_state)),\n","                        ]\n","                    )\n","                else:\n","                    pipe = Pipeline(\n","                        steps=[(\"preprocessor\", preprocessor), (\"regressor\", model())]\n","                    )\n","\n","                pipe.fit(X_train, y_train)\n","                self.models[name] = pipe\n","                y_pred = pipe.predict(X_test)\n","\n","                r_squared = r2_score(y_test, y_pred)\n","                adj_rsquared = adjusted_rsquared(r_squared, X_test.shape[0], X_test.shape[1])\n","                rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","\n","                names.append(name)\n","                R2.append(r_squared)\n","                ADJR2.append(adj_rsquared)\n","                RMSE.append(rmse)\n","                TIME.append(time.time() - start)\n","\n","                if self.custom_metric:\n","                    custom_metric = self.custom_metric(y_test, y_pred)\n","                    CUSTOM_METRIC.append(custom_metric)\n","\n","                if self.verbose > 0:\n","                    scores_verbose = {\n","                        \"Model\": name,\n","                        \"R-Squared\": r_squared,\n","                        \"Adjusted R-Squared\": adj_rsquared,\n","                        \"RMSE\": rmse,\n","                        \"Time taken\": time.time() - start,\n","                    }\n","\n","                    if self.custom_metric:\n","                        scores_verbose[self.custom_metric.__name__] = custom_metric\n","\n","                    print(scores_verbose)\n","                if self.predictions:\n","                    predictions[name] = y_pred\n","            except Exception as exception:\n","                if self.ignore_warnings is False:\n","                    print(name + \" model failed to execute\")\n","                    print(exception)\n","\n","        scores = {\n","            \"Model\": names,\n","            \"Adjusted R-Squared\": ADJR2,\n","            \"R-Squared\": R2,\n","            \"RMSE\": RMSE,\n","            \"Time Taken\": TIME\n","        }\n","\n","        if self.custom_metric:\n","            scores[self.custom_metric.__name__] = CUSTOM_METRIC\n","\n","        scores = pd.DataFrame(scores)\n","        scores = scores.sort_values(by=\"Adjusted R-Squared\", ascending=False).set_index(\"Model\")\n","\n","        if self.predictions:\n","            predictions_df = pd.DataFrame.from_dict(predictions)\n","        return scores, predictions_df if self.predictions is True else scores\n","\n","    def provide_models(self, X_train, X_test, y_train, y_test):\n","        \"\"\"\n","        This function returns all the model objects trained in fit function.\n","        If fit is not called already, then we call fit and then return the models.\n","        Parameters\n","        ----------\n","        X_train : array-like,\n","            Training vectors, where rows is the number of samples\n","            and columns is the number of features.\n","        X_test : array-like,\n","            Testing vectors, where rows is the number of samples\n","            and columns is the number of features.\n","        y_train : array-like,\n","            Training vectors, where rows is the number of samples\n","            and columns is the number of features.\n","        y_test : array-like,\n","            Testing vectors, where rows is the number of samples\n","            and columns is the number of features.\n","        Returns\n","        -------\n","        models: dict-object,\n","            Returns a dictionary with each model pipeline as value\n","            with key as name of models.\n","        \"\"\"\n","        if len(self.models.keys()) == 0:\n","            self.fit(X_train,X_test,y_train,y_test)\n","\n","        return self.models\n","\n","\n","Regression = LazyRegressor\n","Classification = LazyClassifier"]},{"cell_type":"markdown","metadata":{"id":"oaeejqGViakC"},"source":["# Selector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6vHBG1ZoR1u"},"outputs":[],"source":["# !pip install --upgrade tpot\n","# from tpot import TPOTClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bcxeGSsVicPz"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score, f1_score, recall_score, precision_score\n","import time\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from xgboost import XGBClassifier\n","import numpy as np\n","from keras.layers import Dense, LSTM, Bidirectional, Conv1D, GlobalMaxPooling1D, Dropout\n","from keras.optimizers import Adam\n","import xgboost as xgb\n","\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding\n","import sys, os\n","from sklearn.pipeline import make_pipeline\n","# import swarms.PSO as pso\n","# import swarms.GWO as gwo\n","# import swarms.MVO as mvo\n","# import swarms.MFO as mfo\n","# import swarms.BAT as bat\n","# import swarms.WOA as woa\n","# import swarms.FFA as ffa\n","# import swarms.HGS as hgs\n","# import swarms.DYHGS as dyhgs\n","import csv\n","import numpy\n","import numpy as np\n","import time\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import pandas as pd\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","# import utils.fitnessFUNs as fitnessFUNs\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","# from utils.solution import solution\n","# from utils.lazyPred_src import LazyClassifier\n","from sklearn.svm import SVC\n","from sklearn import svm\n","from pycm import *\n","import seaborn as sns\n","\n","from sklearn.metrics import auc\n","import matplotlib.pyplot as plt\n","\n","\n","def export_conf_mat(normalized_data, labels, exp_path, algo, flag = None):\n","\n","    #df_pct =  data.div(data.sum(axis=1), axis=0).round(3)\n","\n","    colormap = sns.color_palette(\"Blues\", as_cmap=True)\n","    plt.tight_layout()\n","    plt.rcParams['font.family'] = 'Arial'\n","    plt.rcParams['font.size'] = 20\n","\n","    fig, ax = plt.subplots(figsize=(12,12))\n","    sns.heatmap(np.around(normalized_data, decimals=3), \n","                cmap=colormap, #\n","                annot=True, \n","                square=True,\n","                cbar=False,\n","                # annot_kws={'size':10},\n","                fmt='g',\n","                xticklabels=list(labels), \n","                yticklabels=list(labels))\n","    # ax.set_title('UniMib_SHAR Dataset Confusion Matrix (Best Acc)')\n","    # plt.yticks(rotation=0)\n","    # ax.xaxis.tick_top() # x axis on top\n","    # ax.xaxis.set_label_position('top')\n","    # plt.xticks(rotation=45)\n","\n","    plt.xlabel('Predicted Class')\n","    plt.ylabel('Actual Class')\n","\n","    save_path = os.path.join(exp_path, flag + \"_best_individual_conf_mat_\"+ algo + \".pdf\")\n","    fig.savefig(save_path, bbox_inches='tight', dpi=400)\n","\n","def plot_roc_curve(clf, X_test, Y_test, labels, img_name = 'curve'):\n","    \n","    pred_prob = clf.predict_proba(X_test)\n","\n","    # roc curve for classes\n","    fpr = {}\n","    tpr = {}\n","    thresh ={}\n","    roc_auc = {}\n","\n","    n_class = len(set(Y_test))\n","    for i in range(n_class):    \n","        fpr[i], tpr[i], thresh[i] = roc_curve(Y_test, pred_prob[:,i], pos_label=i) \n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","        \n","        \n","    # roc for each class\n","    fig, ax = plt.subplots(figsize=(10, 10))\n","    ax.plot([0, 1], [0, 1], 'k--')\n","    ax.set_xlim([0.0, 1.0])\n","    ax.set_ylim([0.0, 1.05])\n","    ax.set_xlabel('False Positive Rate')\n","    ax.set_ylabel('True Positive Rate')\n","    ax.set_title('Receiver operating characteristic')\n","    for i in range(n_class):\n","        ax.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) for %s' % (roc_auc[i], str(labels[i])))\n","    ax.legend(loc=\"best\")\n","    ax.grid(alpha=.4)\n","    plt.savefig(img_name,dpi=300);  \n","    \n","    \n","\n","\n","class slctr:\n","\n","    def selector(algo,func_details,popSize,Iter,completeData_train, completeData_test,samples_ratio, path_f, exp_folder, LOGGER_FILE, class_ = 'multiclass'):\n","        function_name=func_details[0]\n","        lb=func_details[1]\n","        ub=func_details[2]\n","        use_predifined_best_indiv = False\n","      \n","        \n","        DatasetSplitRatio=0.15   #Training 66%, Testing 34%\n","        \n","        DataFile_train=path_f+completeData_train\n","        DataFile_test=path_f+completeData_test\n","          \n","        data_set_train=numpy.loadtxt(open(DataFile_train,\"rb\"),delimiter=\",\",skiprows=0)\n","        data_set_test=numpy.loadtxt(open(DataFile_test,\"rb\"),delimiter=\",\",skiprows=0)\n","        \n","        LOGGER_FILE.info(f'Loaded Train shape {data_set_train.shape} - Test shape {data_set_test.shape}')\n","        \n","        \n","        numRowsData_train=numpy.shape(data_set_train)[0]    # number of instances in the  dataset\n","        numFeaturesData_train=numpy.shape(data_set_train)[1]-1 #number of features in the  dataset\n","\n","        numRowsData_test=numpy.shape(data_set_test)[0]    # number of instances in the  dataset\n","        numFeaturesData_test=numpy.shape(data_set_test)[1]-1 #number of features in the  dataset\n","        \n","        trainInput=data_set_train[0:numRowsData_train,0:-1]\n","        testInput=data_set_test[0:numRowsData_test,0:-1]\n","\n","        #print(trainInput[0])\n","        # Data scaling\n","        #s = MinMaxScaler()\n","        s= StandardScaler()\n","        #apply to training, test \n","        trainInput = s.fit_transform(trainInput)\n","        testInput = s.transform(testInput)\n","\n","\n","        trainOutput=np.array(data_set_train[0:numRowsData_train,-1]).astype(int)\n","        testOutput=np.array(data_set_test[0:numRowsData_test,-1]).astype(int)\n","\n","\n","        if 'PH2' in completeData_train and class_ != 'binary':\n","          targets_names= ['Atypical-Nevus', 'Common-Nevus', 'Melanoma']\n","        elif 'ISIC_2016' in completeData_train and class_ == 'binary':\n","          targets_names= ['Benign', 'Malignant']\n","\n","        print(targets_names)\n","        \n","\n","        # if 'BoT-IoT' in completeData_train and class_ != 'binary':\n","        #   targets_names = ['DDoS', 'DoS', 'Normal', 'Reconnaissance', 'Theft']\n","        # elif 'NSL-KDD' in completeData_train and class_ != 'binary':\n","        #   targets_names = ['Benign', 'DoS', 'Probe', 'r2l', 'u2r']\n","        # elif 'kdd99' in completeData_train and class_ != 'binary':\n","        #   targets_names = ['Normal', 'Probe', 'DoS', 'u2r', 'r2l']\n","        # elif 'CICIDS2017' in completeData_train and class_ != 'binary':\n","        #   targets_names = ['Benign','DDoS','FTP-Patator', 'PortScan','SSH-Patator',\n","        #                   'Web Attack Brute Force', 'Web Attack Sql Injection', 'Web Attack XSS']\n","\n","\n","        # if class_ == 'binary': \n","        #     targets_names = ['Benign','Attack']\n","        #     # array(['DDoS', 'DoS', 'Normal', 'Reconnaissance', 'Theft'], dtype=object)\n","        #     if 'BoT-IoT' in completeData_train or 'BoT-IoT' in completeData_test: \n","        #         transdict = {0:1, 1:1, 2:0, 3:1, 4:1}\n","        #         trainOutput = np.array([transdict[x] for x in trainOutput])\n","        #         testOutput = np.array([transdict[x] for x in testOutput])\n","        #     else:\n","        #         trainOutput[trainOutput > 0] = 1\n","        #         testOutput[testOutput > 0] = 1\n","\n","\n","      # dataInput=[trainInput,testInput]\n","        #dataInput=np.concatenate((trainInput, testInput), axis=0)\n","      # dataTarget=np.concatenate((trainOutput, testOutput), axis=0)\n","        #dataTarget=[trainOutput,testOutput]\n","        if numFeaturesData_train != numFeaturesData_test:\n","            print('Mismatch in number of feature of Train {numFeaturesData_train} and Test {numFeaturesData_test}')\n","            sys.exit()   \n","        \n","        #trainInput, testInput, trainOutput, testOutput = train_test_split(dataInput, dataTarget, test_size=DatasetSplitRatio, random_state=1) \n","    #\n","      \n","    #    numRowsTrain=numpy.shape(trainInput)[0]    # number of instances in the train dataset\n","    #    numFeaturesTrain=numpy.shape(trainInput)[1]-1 #number of features in the train dataset\n","    #\n","    #    numRowsTest=numpy.shape(testInput)[0]    # number of instances in the test dataset\n","    #    numFeaturesTest=numpy.shape(testInput)[1]-1 #number of features in the test dataset\n","    # \n","        dim=numFeaturesData_train\n","        x = solution()\n","        \n","        if samples_ratio == None:\n","\n","            trainInput_red = trainInput.copy()\n","            trainOutput_red = trainOutput.copy()\n","\n","\n","        else:\n","\n","            df_train = pd.DataFrame(data_set_train)\n","            data = df_train.loc[df_train[df_train.columns[-1]].isin(set(df_train[df_train.columns[-1]].values))].groupby(df_train.columns[-1]).head(samples_ratio)\n","            trainInput_red = data.iloc[:,:-1].values\n","            trainOutput_red = np.array(data[df_train.columns[-1]].values).astype(int)\n","            \n","        # if class_ == 'binary': \n","        #     # array(['DDoS', 'DoS', 'Normal', 'Reconnaissance', 'Theft'], dtype=object)\n","        #     if 'BoT-IoT' in completeData_train or 'BoT-IoT' in completeData_test: \n","        #         transdict = {0:1, 1:1, 2:0, 3:1, 4:1}\n","        #         trainOutput_red = np.array([transdict[x] for x in trainOutput_red])\n","\n","        #     else:\n","        #         trainOutput_red[trainOutput_red > 0] = 1\n","\n","        \n","        if(algo==0):\n","            if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1]\n","            else:\n","              x=pso.PSO(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","    \n","            algo = 'PSO'\n","        if(algo==1):\n","            if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1]\n","            else:\n","              x=mvo.MVO(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","\n","            algo = 'MVO'\n","        if(algo==2):\n","            if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1]\n","            else:\n","              x=gwo.GWO(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","\n","            algo = 'GWO'\n","        if(algo==3):\n","            if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1]\n","            else:\n","              x=mfo.MFO(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","\n","            algo = 'MFO'\n","\n","        if(algo==4):\n","            if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1]\n","            else:\n","              x=woa.WOA(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","\n","            algo = 'WOA'\n","        if(algo==5):\n","            if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1]\n","            else:\n","              x=ffa.FFA(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","\n","            algo = 'FFA'\n","        if(algo==6):\n","            if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                                  1, 1, 1, 1, 1, 1, 1, 1, 1]\n","            else:\n","              x=bat.BAT(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","        \n","            algo = 'BAT'\n","\n","        if(algo==7):\n","          if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","          else:\n","              x=hgs.HGS(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","          algo = 'HGS'\n","\n","        if(algo==8):\n","          if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","          else:\n","              x=dyhgs.DYHGS(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","          algo = 'DYHGS'\n","            \n","        if(algo==9):\n","          if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","          else:\n","              x=cgo.CGO(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","          algo = 'CGO'\n","          \n","        if(algo==10):\n","          if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","          else:\n","              x=aoa.AOA(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","          algo = 'AOA'\n","            \n","        if(algo==11):\n","          if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","          else:\n","              x=ao.AO(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","          algo = 'AO'\n","\n","        if(algo==12):\n","          if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","          else:\n","              x=archoa.ArchOA(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","          algo = 'ArchOA'\n","\n","        if(algo==13):\n","          if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","          else:\n","              x=lvaoa.LVAOA(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","          algo = 'LVAOA'\n","            \n","        if(algo==14):\n","          if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","          else:\n","              x=dolao.DOLAO(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","          algo = 'DOLAO'\n","            \n","        if(algo==15):\n","          if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","          else:\n","              x=hba.HBA(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","          algo = 'HBA'\n","\n","        if(algo==16):\n","          if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","          else:\n","              x=lvhba.LVHBA(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","          algo = 'LVHBA'\n","\n","        if(algo==17):\n","          if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","          else:\n","              x=aha.AHA(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","          algo = 'AHA'\n","\n","        if(algo==18):\n","          if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","          else:\n","              x=araha.ARAHA(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","          algo = 'ARAHA'\n","\n","        if(algo==19):\n","          if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","          else:\n","              x=aqaha.AQAHA(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","          algo = 'AQAHA'        \n","\n","        if(algo==20):\n","          if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","          else:\n","              x=ssa.SSA(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","          algo = 'SSA'\n","\n","        if(algo==21):\n","          if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","          else:\n","              x=ssamrf.SSAMRF(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","          algo = 'SSAMRF'\n","\n","        if(algo==22):\n","          if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","          else:\n","              x=nmra.NMRA(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","          algo = 'NMRA'\n","\n","        if(algo==23):\n","          if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","          else:\n","              x=munmra.MuNMRA(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","          algo = 'MuNMRA'\n","\n","        if(algo==24):\n","          if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","          else:\n","              x=aro.ARO(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","          algo = 'ARO'\n","\n","\n","        if(algo==25):\n","          if use_predifined_best_indiv:\n","              x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","          else:\n","              x=muaro.MuARO(getattr(fitnessFUNs, function_name),lb,ub,dim,popSize,Iter,trainInput_red,trainOutput_red,LOGGER_FILE)\n","          algo = 'MuARO'\n","\n","        # Evaluate MLP classification model based on the training set\n","    #    trainClassification_results=evalNet.evaluateNetClassifier(x,trainInput,trainOutput,net)\n","    #   x.trainAcc=trainClassification_results[0]\n","      #  x.trainTP=trainClassification_results[1]\n","      # x.trainFN=trainClassification_results[2]\n","        #x.trainFP=trainClassification_results[3]\n","        #x.trainTN=trainClassification_results[4]\n","      \n","        # Evaluate MLP classification model based on the testing set   \n","        #testClassification_results=evalNet.evaluateNetClassifier(x,testInput,testOutput,net)\n","                \n","\n","        '''\n","        x.bestIndividual = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n","                            1, 1, 1, 1, 1, 1, 1, 1, 1]\n","        '''\n","\n","        reducedfeatures=[]\n","        for index in range(0,dim):\n","            if (x.bestIndividual[index]==1):\n","                reducedfeatures.append(index)\n","        reduced_data_train_global=trainInput[:,reducedfeatures]\n","        reduced_data_test_global=testInput[:,reducedfeatures]\n","        print(f'Reduced to {len(reducedfeatures)} features.')\n","        LOGGER_FILE.info(f'Reduced to {len(reducedfeatures)} features.')\n","\n","        from xgboost import XGBClassifier\n","\n","        knn = make_pipeline(StandardScaler(), XGBClassifier())\n","        # knn= make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True))\n","        # knn = KNeighborsClassifier(n_neighbors=5)\n","        knn.fit(reduced_data_train_global,trainOutput)\n","\n","        # Compute the accuracy of the prediction\n","        target_pred_train = knn.predict(reduced_data_train_global)\n","\n","        cm_train = ConfusionMatrix(actual_vector=trainOutput, predict_vector=target_pred_train)\n","        LOGGER_FILE.info('Train \\n')\n","        LOGGER_FILE.info(cm_train)\n","        \n","\n","        LOGGER_FILE.info(cm_train.print_matrix())\n","        LOGGER_FILE.info(cm_train.print_normalized_matrix())\n","        # export_conf_mat(cm_train.to_array(normalized=True), targets_names, exp_folder, algo, flag = 'train')\n","        export_conf_mat(cm_train.to_array(normalized=False), targets_names, exp_folder, algo, flag = 'train')\n","        \n","        acc_train = float(accuracy_score(trainOutput, target_pred_train))\n","        x.trainAcc=acc_train\n","        \n","        rec_train = float(recall_score(trainOutput, target_pred_train, average='weighted'))\n","        x.rec_train=rec_train\n","        \n","        prec_train = float(precision_score(trainOutput, target_pred_train, average='weighted'))\n","        x.prec_train=prec_train\n","        \n","        f1_train = float(f1_score(trainOutput, target_pred_train, average='weighted'))\n","        x.f1_train=f1_train\n","        \n","        roc_auc_train = float(roc_auc_score(trainOutput, target_pred_train, average='weighted'))\n","        x.roc_auc_train=roc_auc_train      \n","        \n","        target_pred_test = knn.predict(reduced_data_test_global)\n","        acc_test = float(accuracy_score(testOutput, target_pred_test))\n","        x.testAcc=acc_test\n","\n","        cm_test = ConfusionMatrix(actual_vector=testOutput, predict_vector=target_pred_test)\n","        LOGGER_FILE.info('Test \\n')\n","        LOGGER_FILE.info(cm_test.classes)\n","        LOGGER_FILE.info(cm_test)\n","        LOGGER_FILE.info(cm_test.print_matrix())\n","        LOGGER_FILE.info(cm_test.print_normalized_matrix())\n","        export_conf_mat(cm_test.to_array(normalized=False), targets_names, exp_folder, algo, flag = 'test')\n","\n","        \n","        print(f'{knn.__class__.__name__}: test set accuracy: {(acc_test * 100)}')\n","        LOGGER_FILE.info(f'{knn.__class__.__name__}: test set accuracy: {(acc_test * 100)}')\n","\n","        rec_test = float(recall_score(testOutput, target_pred_test, average='weighted'))\n","        x.rec_test=rec_test\n","        \n","        prec_test = float(precision_score(testOutput, target_pred_test, average='weighted'))\n","        x.prec_test=prec_test\n","        \n","        f1_test = float(f1_score(testOutput, target_pred_test, average='weighted'))\n","        x.f1_test=f1_test\n","        \n","        # lazy_clf = LazyClassifier(predictions=False, verbose=0,ignore_warnings=True, custom_metric=None)\n","        # models,predictions = lazy_clf.fit(reduced_data_train_global, reduced_data_test_global, trainOutput, testOutput)\n","        # print(models.head(5))\n","        # LOGGER_FILE.info(models)\n","\n","##########################################################################\n","\n","        models = ['DCNN', 'XGBM', 'BiLSTM', 'FCN']\n","        models = ['DCNN', 'XGBM', 'FCN']\n","        \n","        scores = [accuracy_score, balanced_accuracy_score, roc_auc_score, f1_score, recall_score, precision_score, cohen_kappa_score]\n","        results = []\n","\n","        for model_name in models:\n","            score_list = []\n","            start_time = time.time()\n","            trainInput=reduced_data_train_global\n","            testInput=reduced_data_test_global\n","\n","            if model_name == 'XGBM':\n","            # XGBM model code\n","                # Preprocess the data using StandardScaler\n","                # scaler = StandardScaler()\n","                # trainInput = scaler.fit_transform(trainInput)\n","                # testInput = scaler.transform(testInput)\n","\n","                params = {\n","                    'max_depth': 3,\n","                    'eta': 0.1,\n","                    'objective': 'binary:logistic',\n","                    'eval_metric': 'error'\n","                }\n","\n","                # Convert the data into DMatrix format\n","                dtrain = xgb.DMatrix(trainInput, label=trainOutput)\n","                dtest = xgb.DMatrix(testInput)\n","                num_rounds = 10\n","                model_xgbm = xgb.train(params, dtrain, num_rounds)\n","                y_pred_prob = model_xgbm.predict(dtest)\n","                y_pred = (y_pred_prob > 0.5).astype(int)\n","\n","            if model_name == 'DCNN':\n","                # DCNN model code\n","                model_dcnn = Sequential()\n","                model_dcnn.add(Dense(64, input_shape=(trainInput.shape[1],), activation='relu'))\n","                model_dcnn.add(Dense(1, activation='sigmoid'))\n","                model_dcnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","                model_dcnn.fit(trainInput, trainOutput, epochs=10, batch_size=32, verbose=0)\n","                y_pred_prob = model_dcnn.predict(testInput).flatten()\n","                y_pred = np.where(y_pred_prob >= 0.5, 1, 0)\n","\n","            # elif model_name == 'XGBM':\n","            #     # XGBM model code\n","            #     tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, template='Selector-Transformer-XGBClassifier')\n","            #     tpot.fit(trainInput, trainOutput)\n","            #     y_pred = tpot.predict(testInput)\n","            #     model = XGBClassifier()\n","            #     model.fit(trainInput, trainOutput)\n","            #     y_pred = model.predict(testInput)\n","            \n","            \n","            if model_name == 'BiLSTM':\n","                # BiLSTM model code\n","                model_Bilstm = Sequential()\n","                model_Bilstm.add(Bidirectional(LSTM(64, activation='relu', return_sequences=True), input_shape=(trainInput.shape[1], 1)))\n","                model_Bilstm.add(Bidirectional(LSTM(64, activation='relu')))\n","                model_Bilstm.add(Dense(1, activation='sigmoid'))\n","                model_Bilstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","                # X_train_reshape = np.reshape(trainInput, (trainInput.shape[0], trainInput.shape[1], 1))\n","                # X_test_reshape = np.reshape(testInput, (testInput.shape[0], testInput.shape[1], 1))\n","                # model_Bilstm.fit(X_train_reshape, trainOutput, epochs=10, batch_size=32, verbose=0)\n","                model_Bilstm.fit(trainInput, trainOutput, epochs=10, batch_size=32, verbose=0)\n","                # y_pred_prob = model_Bilstm.predict(X_test_reshape).flatten()\n","                y_pred_prob = model_Bilstm.predict(testInput).flatten()\n","                y_pred = np.where(y_pred_prob >= 0.5, 1, 0)          \n","                               \n","                \n","            if model_name == 'FCN':\n","                # FCN model code\n","                model_fcn = Sequential()\n","                model_fcn.add(Conv1D(64, 3, activation='relu', input_shape=(trainInput.shape[1], 1)))\n","                model_fcn.add(Conv1D(64, 3, activation='relu'))\n","                model_fcn.add(GlobalMaxPooling1D())\n","                model_fcn.add(Dense(64, activation='relu'))\n","                model_fcn.add(Dropout(0.5))\n","                model_fcn.add(Dense(1, activation='sigmoid'))\n","                model_fcn.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n","                X_train_reshape = np.reshape(trainInput, (trainInput.shape[0], trainInput.shape[1], 1))\n","                X_test_reshape = np.reshape(testInput, (testInput.shape[0], testInput.shape[1], 1))\n","                model_fcn.fit(X_train_reshape, trainOutput, epochs=10, batch_size=32, verbose=0)\n","                y_pred_prob = model_fcn.predict(X_test_reshape).flatten()\n","                y_pred = np.where(y_pred_prob >= 0.5, 1, 0)\n","\n","            score_list.append(model_name)\n","            score_list.append(accuracy_score(testOutput, y_pred))\n","            score_list.append(balanced_accuracy_score(testOutput, y_pred))\n","            score_list.append(roc_auc_score(testOutput, y_pred))\n","            score_list.append(f1_score(testOutput, y_pred, average='weighted'))\n","            score_list.append(recall_score(testOutput, y_pred, average='weighted'))\n","            score_list.append(precision_score(testOutput, y_pred, average='weighted'))\n","            end_time = time.time()\n","            score_list.append(end_time - start_time)\n","            score_list.append(cohen_kappa_score(testOutput, y_pred))\n","            results.append(score_list)\n","\n","        # print('{:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}'.format('Model', 'Accuracy', 'Balanced Accuracy', 'ROC AUC', 'F1 Score', 'Recall', 'Precision', 'Time Taken'))\n","        # for i in range(len(models)):\n","        #     print('{:<10} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}'.format(models[i], results[i][0], results[i][1], results[i][2], results[i][3], results[i][4], results[i][5], results[i][6]))\n","        \n","        columns = ['Model','Accuracy', 'Balanced Accuracy', 'ROC AUC', 'F1 Score', 'Recall', 'Precision', 'Time Taken', 'Cohen Kappa']\n","        df_results = pd.DataFrame(results, columns=columns)\n","        print(df_results)\n","\n","        # print(df_results)\n","        LOGGER_FILE.info(df_results)\n","########################################################################\n","\n","\n","        #we will also receive predictions of all the models for each and every observation\n","        #print(predictions)\n","\n","        plot_roc_curve(knn, reduced_data_test_global, testOutput, targets_names, img_name = exp_folder+'/roc_curve_'+str(algo)+'_end_'+str(x.endTime))\n","\n","        roc_auc_test = float(roc_auc_score(testOutput, target_pred_test, average='weighted'))\n","        x.roc_auc_test=roc_auc_test\n","\n","        #x.testTP=testClassification_results[1]\n","        #x.testFN=testClassification_results[2]\n","        #x.testFP=testClassification_results[3]\n","        #x.testTN=testClassification_results[4] \n","        \n","        x.metrics = { 'rec_train': x.rec_train,\n","                    'prec_train': x.prec_train,\n","                    'f1_train': x.f1_train,\n","                     'roc_auc_train': x.roc_auc_train,\n","                    'rec_test': x.rec_test,\n","                    'prec_test': x.prec_test,\n","                    'f1_test': x.f1_test,\n","                     'roc_auc_test': x.roc_auc_test,\n","                     \n","                    }\n","        \n","        \n","        return x\n","        \n","    #####################################################################    \n"]},{"cell_type":"markdown","metadata":{"id":"YYMVjzsviAUy"},"source":["# Logger"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LfQwzQUqiB4m"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","from __future__ import division\n","from __future__ import print_function\n","\n","import logging\n","import logging.config\n","import os, sys\n","from datetime import datetime\n","\n","def getLogger(folder, File_name, mode = 'root'):\n","    LOGFINENAME = '{0}.log'.format(File_name)\n","    LOGFILE = os.path.join(folder, LOGFINENAME)\n","\n","    DEFAULT_LOGGING = {\n","        'version': 1,\n","        'formatters': {\n","            'deep_info': {\n","                'format': '%(asctime)s %(module)s(%(lineno)d) - %(levelname)s: %(message)s',\n","                'datefmt': '%Y-%m-%d %H:%M:%S',\n","            },\n","            'standard': {\n","                'format': '%(asctime)s %(levelname)s(%(lineno)d): %(message)s',\n","                'datefmt': '%Y-%m-%d %H:%M:%S',\n","            },\n","            'simple': {\n","                'format': '%(message)s',\n","            },\n","        },\n","        \n","        'handlers': {\n","            'console': {\n","                'class': 'logging.StreamHandler',\n","                'formatter': 'standard',\n","                'level': 'INFO',\n","                'stream': sys.stdout,\n","            },\n","            'file': {\n","                'class': 'logging.FileHandler',\n","                'formatter': 'deep_info', #simple\n","                'level': 'INFO',\n","                'filename': LOGFILE,\n","                'mode': 'w',  #overwrite\n","            },\n","        },\n","        \n","        'loggers': {\n","            'print_all': {\n","                'level': 'INFO',\n","                'handlers': ['console', 'file'],\n","                'propagate': False,\n","            },\n","            'root': {\n","                'level': 'INFO',\n","                'handlers': ['file'],\n","                'propagate': False,\n","            },\n","            __name__: {\n","                'level': 'INFO',\n","                'handlers': ['console', 'file'],\n","                'propagate': False,\n","            },\n","        }\n","    }\n","    \n","    logging.config.dictConfig(DEFAULT_LOGGING)\n","    logger_console = logging.getLogger(mode)\n","    logger_file = logging.getLogger('root')\n","\n","    return logger_console, logger_file"]},{"cell_type":"markdown","metadata":{"id":"rMun5UYejyXG"},"source":["# Main.py\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3FhFYa5KA_gZ"},"outputs":[],"source":["\n","import os\n","import csv\n","import numpy\n","import time\n","# import utils.selector as slctr\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","# import utils.fitnessFUNs as fitnessFUNs\n","# from utils.logger import getLogger\n","\n","pd.set_option(\"display.precision\", 4)\n","pd.set_option(\"display.float_format\", lambda x: \"%.4f\" % x)\n","PROJECT_PATH = os.getcwd() \n","os.chdir(PROJECT_PATH)\n","\n","\n","PSO= False\n","MVO= False\n","GWO = True\n","MFO= False\n","WOA= False\n","FFA=False\n","BAT=False\n","HGS=True\n","AO=False\n","CGO=False\n","AOA=True\n","HBA=False\n","AHA=False\n","ArchOA=False\n","SSA=True\n","SSAMRF=False\n","NMRA=False\n","MuNMRA=False\n","ARO=False\n","MuARO=True\n","\n","# Select optimizers\n","ARAHA=False\n","AQAHA=False\n","\n","LVHBA=False\n","LVAOA=False\n","DOLAO=False\n","DYHGS=False\n","\n","\n","optimizer=[PSO, MVO, GWO, MFO, WOA,FFA,BAT,HGS,DYHGS,CGO, AOA, AO, ArchOA, LVAOA, DOLAO, HBA, LVHBA, AHA, ARAHA, AQAHA,SSA, SSAMRF, NMRA, MuNMRA, ARO, MuARO]\n","\n","# datasets folder\n","p=\"/content/drive/MyDrive/Group/Colab/Medical Image Processing/ISIC/Feature Selection/\"\n","# datasets folder\n","path_f = p+\"datasets/\"\n","\n","# dataset features Train/test files\n","\n","# datasets=[[\"Train_ft_FC1_distilbert_C36_Rep_1_Seed_320_1623630509.2547212.pt_96.8080\",\n","#             \"Test_ft_FC1_distilbert_C36_Rep_1_Seed_320_1623630509.2547212.pt_96.8080\"]]\n","\n","# datasets=[[\"Train_ft_FC1_InceptionV3_ISIC_2016_Rep_8_Seed_745_1619407682.037227.pt_87.3351\",\n","#           \"Test_ft_FC1_InceptionV3_ISIC_2016_Rep_8_Seed_745_1619407682.037227.pt_87.3351\"]]\n","\n","datasets=[[\"Train_ft_FC1_MobileNetV3_ISIC_2016_Rep_11_Seed_296_1619717789.4316587.pt_87.0712\",\n","          \"Test_ft_FC1_MobileNetV3_ISIC_2016_Rep_11_Seed_296_1619717789.4316587.pt_87.0712\"]]\n","\n","\n","class_ = 'binary'#'multiclass' # binary / multiclass\n","samples_ratio = None # None: no reduce / other numbers such as 100: getting 1000 samples from each class\n","\n","\n","#benchmarkfunc=[Fs1,Fs2,Fs3,Fs4,Fs5,Fs6,Fs7,Fs8,Fs9,Fs10] \n","        \n","# Select number of repetitions for each experiment. \n","# To obtain meaningful statistical results, usually 30 independent runs \n","# are executed for each algorithm.\n","NumOfRuns=2#10\n","\n","# Select general parameters for all optimizers (population size, number of iterations)\n","PopulationSize = 50\n","Iterations= 10#30\n","\n","#Export results ?\n","Export=True\n","\n","\n","# Check if it works at least once\n","Flag=False\n","\n","# CSV Header for for the cinvergence \n","CnvgHeader1=[]\n","CnvgHeader2=[]\n","\n","\n","for l in range(0,Iterations):\n","\tCnvgHeader1.append(\"Iter\"+str(l+1))\n","\n","for l in range(0,Iterations):\n","\tCnvgHeader2.append(\"Iter\"+str(l+1))\n","\n","\n","for j in range (0, len(datasets)):        # specfiy the number of the datasets\n","\n","    #Automaticly generated file name by date and time\n","    File_name = f'{\"_\".join(datasets[j][0].split(\"_\")[3:6])}_{class_}_{time.strftime(\"%Y-%m-%d-%H-%M-%S\")}'\n","    #File_name = 'experiment'+'_'+class_+'_'+'_'.join(datasets[0].split('_')[3:5])+'_'+time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n","    exp_folder = os.path.join(p+\"results/\", File_name)\n","    if not os.path.isdir(exp_folder): \n","        os.makedirs(exp_folder)\n","                \n","    ExportToFile= os.path.join(exp_folder, File_name + \".csv\") \n","    # Loggers\n","    Console_LOGGER, LOGGER_FILE  = getLogger(exp_folder, File_name, mode = 'print_all')\n","\n","    for i in range (0, len(optimizer)):\n","    \n","        if((optimizer[i]==True)): # start experiment if an optimizer and an objective function is selected\n","            for k in range (0,NumOfRuns):\n","                \n","                #func_details=[\"costNN\",-1,1]\n","                func_details=fitnessFUNs.getFunctionDetails(0)\n","                completeData_train=datasets[j][0]+\".csv\"\n","                completeData_test=datasets[j][1]+\".csv\"\n","                x=slctr.selector(i,func_details,PopulationSize,Iterations,completeData_train, completeData_test,samples_ratio, path_f, exp_folder, LOGGER_FILE, class_)\n","                  \n","                if(Export==True):\n","                    with open(ExportToFile, 'a',newline='\\n') as out:\n","                        writer = csv.writer(out,delimiter=',')\n","                        if (Flag==False): # just one time to write the header of the CSV file\n","                            header= numpy.concatenate([[\"Optimizer\",\n","                                                        \"Dataset\",\n","                                                        \"objfname\",\n","                                                        \"Experiment\",\n","                                                        \"startTime\",\n","                                                        \"EndTime\",\n","                                                        \"ExecutionTime\",\n","                                                        \"trainAcc\",\n","                                                        \"testAcc\",\n","                                                        \"rec_train\",\n","                                                        \"prec_train\",\n","                                                        \"f1_train\",\n","                                                        \"rec_test\",\n","                                                        \"prec_test\",\n","                                                        \"f1_test\",\n","                                                        \"roc_auc_train\",\n","                                                        \"roc_auc_test\",\n","                                                        \"bestIndividual\"],\n","                                                       CnvgHeader1,\n","                                                       CnvgHeader1])\n","                            writer.writerow(header)\n","                        a=numpy.concatenate([[x.optimizer,\n","                                              datasets[j],\n","                                              x.objfname,\n","                                              k+1,\n","                                              x.startTime,\n","                                              x.endTime,\n","                                              x.executionTime,\n","                                              x.trainAcc,\n","                                              x.testAcc,\n","                                              x.rec_train,\n","                                              x.prec_train,\n","                                              x.f1_train,\n","                                              x.rec_test,\n","                                              x.prec_test,\n","                                              x.f1_test,\n","                                              x.roc_auc_train,\n","                                              x.roc_auc_test,\n","                                              x.bestIndividual],\n","                                             x.convergence1,\n","                                             x.convergence2])\n","                        writer.writerow(a)\n","                    out.close()\n","                Flag=True # at least one experiment\n","                \n","if (Flag==False): # Faild to run at least one experiment\n","    print(\"No Optimizer or Cost function is selected. Check lists of available optimizers and cost functions\") \n","              \n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1ZmgBNBbe0ytb9v73kSAsDMlREyoiWZsQ","timestamp":1624847959466}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}